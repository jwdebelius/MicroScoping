---
title: "MicrobiomeScopingReview_DataPrep"
author: "JW Debelius; Y Chen; Y Sun; Z Li"
date: "2025-09-01"
output:
 html_document:
   toc: true
   toc_depth: 3
   toc_float: true
   number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r, warning=FALSE, message=FALSE, echo=FALSE}
library(tidyverse)
library(stringr)
library(tableone)
library(readxl)
library(ComplexUpset)
library(ggplot2)
library(patchwork)
library(reactable)
```

# Reading in the data

We'll start by reading in the data and a manually prepared summary table.

```{r}
data <- read.csv('~/Repositories/MicroScoping/data/LiChenScoping_data.tsv', sep='\t', 
                 colClasses = 'character') %>%
   tibble() %>%
  mutate_all(., ~replace(., . %in% c('', 'NA'), NA)) %>%
  mutate(., doi = tolower(doi))
dim(data)
```

We'll also pull in the citations.

```{r, warning=FALSE, message=FALSE}
citations <- read_excel('~/Repositories/MicroScoping/data/LiChenScopingCitations.xlsx') %>%
  select(., -(c(DOI...13))) %>%
  filter(., `Covidence #` %in% data$covidence_number) %>%
  rename(all_of(c(doi='DOI...1', covidence_number='Covidence #')))
```

We're going to go through and check each question to see how the data review
and retrieval worked and to make sure there is nothing missing that could
be mis-coded. This way, we'll have a clean, reviewed, and documented analytical
data set. Becuase we need something cleaner?

```{r, echo=FALSE}
cleaned = list()
```

# Citation

This is technically *not* a question from the survey, but its still something
I'd like to work through the citation description so we can highlight the
articles in a reasonable way?

```{r}
cleaned[['citations']] <- 
  citations %>%
  mutate(., doi = tolower(doi)
          , meta_in_title = (grepl('meta-ana', tolower(Title)) | grepl('meta ana', tolower(Title)))
          , meta_in_abstr = (grepl('meta-ana', tolower(Abstract)) | grepl('meta ana', tolower(Abstract)))
          , meta_in_title_or_abs = (meta_in_title | meta_in_abstr) * 1
          , author_list = case_when(doi == '10.1038/s41598-018-32221-8' ~ 'Aguirre de Carcer, D.'
                                   ,.default=Authors)
          , author_count = str_count(author_list, ';') + 1
          , author_id = case_when(author_count == 1 ~ str_match(author_list, '^.*?(?=,)')
                                 ,author_count == 2 ~ paste(str_match(str_split_i(author_list, ';', 1), '^.*?(?=,)'),
                                                            str_match(str_split_i(author_list, ';', 2), '^.*?(?=,)'),
                                                            sep=' and ')
                                 ,.default = paste(str_match(author_list, '^.*?(?=,)'), 'et al', sep=' ')
                                 )
          , short_cite = paste(author_id, `Published Year`, sep=', ')
          ) %>%
  select(c(doi, Study, short_cite, meta_in_title_or_abs)) %>%
  rename_with(., .fn=tolower, .cols=everything())
```

# Purpose 

The original question was 

* What was the primary purpose of the combined analysis? (Why did they combine data)

The original answer were

| Column | Response Text |
|:--- |:---|
| `meta_purpose[replication_validation]` | Replication or validation of targeted findings from a single cohort |
| `meta_purpose[core_microbiome]` | Core microbiome analysis (looking for a set of features common across all the samples) |
| `meta_purpose[population_synthesis]` | To compare synthesize across multiple studies to describe a population |
| `meta_purpose[not_meta]` | The study described didn't actually involve combining multiple data sets from multiple prior publications or sequencing data sets |
| `meta_purpose[not_described]` | Not described |
| `meta_purpose[other]` | Other |

The "Other" repsonse triggered a free text description.

# Any Missing

So, we'll start by making sure that all the records were harmonized correctly.

```{r, echo=FALSE}
data %>% 
  column_to_rownames('doi') %>%
  select(starts_with('meta_purpose.')) %>%
  mutate_at(., .vars=vars(starts_with('meta_purpose.')), .fun=is.na) %>%
  rename_with(., ~str_remove(., 'meta_purpose.')) %>%
  rename_with(., ~str_remove(., '\\.')) %>%
  mutate(., missing = rowSums(.)) %>%
  filter(., missing > 0) %>%
  mutate_all(., ~(. * 1)) %>%
  rownames_to_column('doi') %>%
  pivot_longer(cols = -c(doi)) %>%
  pivot_wider(id_cols = name, names_from = doi, values_from = value) %>%
  reactable()
```

## Article Checks

So, we need to check for a core microbiome in article `10.3389/fcimb.2021.645951`.

### 10.3389/fcimb.2021.645951

According to the paper, 

> Here, we collected 16S rRNA gene sequence data of stool samples (n=439) from 
six studies (Saulnier et al., 2011; Pozuelo et al., 2015; Labus et al., 2017;
Zhuang et al., 2018; Lo Presti et al., 2019; Zhu et al., 2019). A unified 
pipeline was used to process raw sequencing data to investigate whether 
biomarkers describing bacterial communities or community-specific microbiota 
profiles could more accurately identify IBS and healthy controls. The results 
of our study showed that alterations in bacterial communities are indeed 
associated with IBS and that a subset of the bacterial profiles may be 
considered as potential biomarkers for identifying the presence of IBS.

I dont think that reflects a "core microbiome" so I'm going to classify as "no".

# Other answers

We'll also look at answers for "other purpose" to see if there's something specific.

```{r, echo=FALSE}
data %>% 
  column_to_rownames('doi') %>%
  select(starts_with('meta_purpose')) %>%
  filter(., (`meta_purpose.other.` %in% c(1, '1')) | !is.na(meta_purpose_other)) %>%
  select(contains('other')) %>%
  reactable()
```
So, we have a set of other purposes that make sense and then two articles where
we have 0s for the "other purpose". So, I'll clean those up as well.

# Tidy Data

I'd also like to code a single column for purpose with a short and long
description.

```{r}
cleaned[['meta_purpose']] <- 
  data %>% 
  select(c(doi, starts_with('meta_purpose'))) %>%
  mutate(., `meta_purpose.core_microbiome.` = ifelse(doi == '10.3389/fcimb.2021.645951', '0',
                                                     `meta_purpose.core_microbiome.`)
          , meta_purpose_other = ifelse(meta_purpose_other == 0, NA, meta_purpose_other)
          , meta_purpose_other = case_when(meta_purpose_other == 'To check the perfromance of a new technique' ~ 'Evaluate a method'
                                          ,.default = meta_purpose_other)
          ) %>%
  mutate_at(., .vars=vars(contains('meta_purpose.')), .funs=as.numeric) %>%
  mutate(., purpose_code = (`meta_purpose.population_synthesis.` * 2 + 
                            `meta_purpose.replication_validation.` * 4 + 
                            `meta_purpose.core_microbiome.` * 8 +
                            `meta_purpose.other.` * 16
                            )
          , purpose_short = case_when(purpose_code == 2 ~ 'Population syntheseis'
                                     ,purpose_code == 4 ~ ''
                                     )
          )
```

# Goal

The original question was, "What was the primary goal of this meta-analysis (what was the hypothesis they were testing)?"

It had possible answers

| Column | Response Text |
|:--- |:--- |
| `meta_goal[microbiome_outcome]` | To characterize a microbiome-outcome association |
| `meta_goal[microbiome_exposure]` | To characterize a microbiome-exposure association |
| `meta_goal[assembly]` | To characterize assembly, development, or temporal variation in the microbial community |
| `meta_goal[unqiue_microbiome]` | To characterize a unique microbiome (microbiome environment, population) |
| `meta_goal[other]` | Other |

## Any Missing

Let's start by looking for any missing harmonized responses.

```{r, echo=FALSE}
data %>% 
  column_to_rownames('doi') %>%
  select(starts_with('meta_goal.')) %>%
  mutate_at(., .vars=vars(starts_with('meta_goal.')), .fun=is.na) %>%
  rename_with(., ~str_remove(., 'meta_goal.')) %>%
  rename_with(., ~str_remove(., '\\.')) %>%
  mutate(., missing = rowSums(.)) %>%
  filter(., missing > 0) %>%
  mutate_all(., ~(. * 1)) %>%
  reactable()
```

There's no issue with the checks on meta_goal, so let's move forward.

## Other answers

We'll also look at answers for "other purpose" to see if there's something specific.

```{r, echo=FALSE}
data %>% 
  column_to_rownames('doi') %>%
  select(starts_with('meta_goal')) %>%
  filter(., (`meta_goal.other.` %in% c(1, '1')) | !is.na(meta_goal_other)) %>%
  select(contains('other')) %>%
  reactable()
```
So, we have a set of other purposes that make sense and we dont need to modify anything.

## Tidy Data

```{r}
cleaned[['meta_goal']] <- 
data %>% 
  select(c(doi, starts_with('meta_goal'))) %>%
  mutate_at(., .vars=vars(contains('meta_goal.')), .funs=as.numeric)
```

# Systematic Approach

The question of "Was there a clear systematic approach for gathering reference data sets?" is asociated with
three trigger questions: 

* Was the search strategy for this meta-analysis described? [`search_described`]
* Was a PRISMA diagram describing how papers were selected included in the paper? [`prisma_included`]
* Was there a clear set of inclusion/exclusion criteria for the final studies? [`study_inclusion`]
 
So, I want to make sure the data an be analyzed easily.

```{r, echo=FALSE}
data %>% 
  column_to_rownames('doi') %>%
  select(systematic_approach, search_described, prisma_included, study_inclusion) %>%
  mutate_at(., .vars=vars(everything()), .funs=~(is.na(.) | (tolower(.) %in% c('yes', 'no')))) %>%
  mutate(., missing = rowSums(.)) %>%
  filter(., missing < 4) %>%
  mutate_all(., ~(. * 1)) %>%
  rownames_to_column('doi') %>%
  pivot_longer(cols=-c(doi)) %>%
  pivot_wider(id_cols = name, names_from = doi, values_from = value) %>%
  reactable()
```
So, we need to look at the data for `doi` because its not convertable.

```{r, echo=FALSE}
data %>% filter(., doi == '10.3389/fmicb.2020.00476') %>%
  select(doi, systematic_approach, search_described, prisma_included, study_inclusion) %>%
  mutate_at(., .vars=vars(everything()), .funs=~ifelse(is.na(.), 'NA', .)) %>%
  reactable()
```

So, the value is listed as a "?" but not values were provided for the three 
answers. This could be because they weren't triggered or because we didn't 
check them when they came in. 

### 10.3389/fmicb.2020.00476

The methods start with a "data availability" section:

> Data Availability
>
> Raw data and metadata for the DiGiulio et al. (2015) cohort were downloaded 
from ImmPort (Bhattacharya et al., 2014), under Study SDY465 (Immport, 2011) 
in May 2016. Raw data and metadata for Romero et al. (2014b) cohort were 
downloaded from the Sequence Read Archive (Leinonen et al., 2011) under 
BioProject PRJNA242473 (NCBI Sequence Read Archive, 2014)1 in May 2016. Raw 
data for Hyman et al. (2014) cohort were received from the authors of the study
(raw sequences and weeks of collection), metadata was downloaded from 
Supplementary Figure S1 of the paper. Raw data and metadata for the Callahan 
et al. (2017) cohort were downloaded from the Sequence Read Archive under 
BioProject PRJNA393472 (NCBI Sequence Read Archive, 2014)2 in January 2018. 
Raw data and metadata for the Stout et al. (2017) cohort were downloaded from 
the Sequence Read Archive under BioProject PRJNA294119 (NCBI Sequence Read 
Archive, 2014)3 in January 2018. The processed data was uploaded to ImmPort 
(Bhattacharya et al., 2014) under study SDY1162 (Immport, 2019) and to 
figshare (Kosti et al., 2019), together with relevant metadata. The DiGiulio 
cohort were collected from various locations within the United States. 
DiGiulio cohort was collected at Stanford University (California), the Callahan
cohort was collected at Stanford University (California) and UAB (Alabama), 
the Romero cohort was collected at Wayne State University (Michigan), the Hyman
cohort was collected at UCSF (California) and the Stout cohort was collected at
Washington University in St. Louis (Missouri).

This doesn't explain anything about how the five data sets were selected.

The results describes the data sets in the first section:

> Study Cohorts
> 
> We searched the literature for raw 16S data from publicly available microbiome
studies from vaginal samples of pregnant women who went on to deliver either 
preterm or at term. We excluded studies with only processed data or studies 
lacking metadata and outcome information. Five studies met our criteria: Hyman 
et al. (2014), Romero et al. (2014b), DiGiulio et al. (2015), Callahan et al. 
(2017), and Stout et al. (2017)(Table 1). To our knowledge there were no other
publicly available PTB related microbiome studies that meet the above criteria
at the time of study design. The prevalence of PTB in all five cohorts (ranging
from 12.5 to 37%) is higher than in the general population, likely reflecting 
study design and clinical settings (Ferré, 2016). The experimental design and 
sampling strategy was different for each cohort (as listed in Table 1) yielding
a different number of overall samples. For all three cohorts, the most samples
were collected in the second and third trimesters (defined as 14–25 and 25–37 
weeks of gestation, respectively). The five cohorts are different from one
another not only in their sampling study design and number of patients but also
in the racial composition of the cohorts (Table 1 and Supplementary Figure S1).
In the Hyman et al. (2014) and DiGiulio et al. (2015) cohorts, most samples 
are from white patients, while in the Romero et al. (2014b), Callahan et al. 
(2017), and Stout et al. (2017) cohorts, most samples are from black patients
(Supplementary Figure S1). By combining these groups together in a meta-analysis
(see section “Materials and Methods”), we are able to capture variability 
across a more diverse population of patients. The combined data set contains 
vaginal 16S sequences and metadata for 3,201 samples from 415 pregnant women, 
mostly split between black and white individuals (44 and 34% accordingly).

Again, we don't have any description in the main text. I also can't find a PRISMA figure, although there's a workflow overview (Figure 1), so I'm going to assume that we dont have one.

I could not find anything about a search stragedy or PRISMA diagram in the 
supplement. So, I'm going to assume that there was not a systematic search
and the missingness is correct, and code that as a "no".

## Masking

The second question is whether the missing questions are masked appropriately.
R is weird about missing, so we'll code the missing as 0s and make sure the
analysis filters correctly.

```{r, echo=FALSE}
data %>%
  column_to_rownames('doi') %>%
  select(systematic_approach, search_described, prisma_included, study_inclusion) %>%
  mutate(., systematic_approach = ifelse(!(systematic_approach %in% c('Yes', 'No')), 'No', systematic_approach)) %>%
  mutate_all(., ~case_when(. == 'Yes' ~ 1
                          ,. == 'No' ~ 0
                          ,is.na(.) ~ NA
                          ,.default = -1)) %>%
  mutate(., miss_all = (is.na(search_described) & is.na(prisma_included) & is.na(study_inclusion)) * 1
          , miss_any = (is.na(search_described) | is.na(prisma_included) | is.na(study_inclusion)) * 1
          ) %>%
  filter(., (miss_all != miss_any) | ((systematic_approach == 1) & (miss_any == 1))) %>%
  reactable()
```

So, I think our masking is also reasonable.

## Tidy Data

For the data cleaning, we're going to clean up the systematic appraoch for 
`10.3389/fmicb.2020.00476`, coding it as "No". We'll then convert the values
to numeric (yes/no) and fill in the places we dont have a systematic appraoch
as 0.

```{r}
cleaned[['systematic_approach']] <- 
  data %>% 
  select(doi, systematic_approach, search_described, prisma_included, study_inclusion) %>%
  mutate(., systematic_approach = ifelse(doi == '10.3389/fmicb.2020.00476', 'No', systematic_approach)) %>%
  mutate_at(., .vars=vars(-doi), .funs=~case_when(. == 'Yes' ~ 1
                                                 ,. == 'No' ~ 0
                                                 ,(systematic_approach %in% c('No', 0)) ~ 0
                                                 ,is.na(.) ~ -9)
            )
```

# Number of Studies and samples

We asked two questions about the number of samples and studies:

* How many studies were analyzed in the primary meta analysis, after the data was processed? (If this is not listed, leave it blank) [`num_studies_1o`]
* How many samples were analyzed in the primary meta-analysis, after the data was processed? (If not listed, leave blank) [`num_studies_1o`]

We need to check and make sure the data can be converted to a number, because
numeric values were not allowed, which means the data gets cleaned up.

```{r, warning=FALSE, echo=FALSE}
data %>%
  column_to_rownames('doi') %>%
  select(c(num_studies_1o, num_samples_1o)) %>%
  mutate(., pass_study = is.na(num_studies_1o) | !is.na(as.numeric(num_studies_1o))
          , pass_sample = is.na(num_samples_1o) | !is.na(as.numeric(num_samples_1o))
          ) %>%
  filter(., !(pass_study & pass_sample)) %>%
  select(-starts_with('pass')) %>%
  reactable()
```
It looks like we can clean up the last one by just pulling off the total?

```{r}
cleaned[['num_samples_primary']] <- 
  data %>% 
  select(doi, num_studies_1o, num_samples_1o) %>%
  mutate(., num_samples_1o = str_split_i(num_samples_1o, '=', -1)) %>%
  mutate_at(., .vars=vars(-doi), .funs=as.numeric)
```

# Data Sources

We wanted to know where data was coming from; the question was a select all
defined as, 

"What are the sources for sequencing and metadata for data included in the primary meta-analysis?"

The options were

| Column | Response Text |
| :--- | :--- |
| `sources_1o[consortium_authors]` | The authors of the meta-analysis or consortium |
| `sources_1o[SRA]` | SRA/Genbank |
| `sources_1o[ena]` | ENA |
| `sources_1o[request_authors]` | Request to authors of the original data set (not co-authors of meta analysis) |
| `sources_1o[mg_rast]` | MG-RAST |
| `sources_1o[qitta]` | QIIME DB or Qiita |
| `sources_1o[other]` | Other |
| `sources_1o[not_described]` | Not described |
| `sources_1o_other` | *the other text field* |

## Missing data

We'll start by checking to see if we have any columns which do not have complete
information
```{r, echo=FALSE}
data %>% 
  column_to_rownames('doi') %>%
  select(starts_with('sources_1o')) %>%
  mutate_at(., .vars=vars(starts_with('sources_1o.')), .funs=~(!(. %in% c('0', '1')))) %>%
  select(starts_with('sources_1o.')) %>%
  summarise(across(everything(), sum)) %>%
  rename_with(., .fn=~str_remove(str_remove(., 'sources_1o.'), '\\.')) %>%
  pivot_longer(everything()) %>%
  reactable()
```
And then let's get the list of papers where we have problems or disagreements.

```{r, echo=FALSE}
data %>% 
  select(doi, starts_with('sources_1o')) %>%
  rowwise() %>%
  mutate(., miss_any = (!(`sources_1o.consortium_authors.` %in% c('0', '1')) | 
                        !(`sources_1o.SRA.` %in% c('0', '1')) | 
                        !(`sources_1o.request_authors.` %in% c('0', '1')) | 
                        !(`sources_1o.not_described.` %in% c('0', '1')))
         ) %>%
  ungroup() %>%
  filter(., miss_any) %>%
  reactable()
```



So, we have four papers we need to look at.

### 10.1186/s13059-019-1908-8

We're missing a flag for data not described. If we look at the methods section,
we find:

> A literature search was conducted to identify relevant studies with data accessible from public databases. The majority of sample sequences were downloaded from the Sequence Read Archive (SRA) of the National Center for Biotechnology Information (NCBI), the European Nucleotide Archive (ENA), MG-RAST, and Qiita repositories. The combined dataset is made available here (Additional file 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, and 12), and accession numbers and DOIs for all published studies are indicated in the mapping (metadata) files described in Additional file 1.

So, we don't have any data sets that are not described. Additionally, the data
is coded as data from "other", where "other" is NCBI. Except, that as stated
here, NCBI is SRA. So, we may also want to correct that and say there aren't
"other" sources.

### 10.1371/journal.pcbi.1004468

This article is missing a flag for "consortium authors", but looking 
at Figure 1 from the paper, samples are listed as "in house samples" which
indicates that samples come from co-authors on the paper. 

### 10.1093/braincomms/fcab113

We're missing three values on `10.1093/braincomms/fcab113`: consortium authorship,
request to author, and not described.

Going through Table S1, we can see that samples came from SRA and request to authors. 
So, no consortium authors, yes request to authors, no to not described, yes SRA.

### 10.3389/fcimb.2020.00434

We have an issue with the way other is described, and I'm seeing "EBI" in the
"not described" column. So, we'll fix this by recoganizing that "EBI" is a
another name for "ENA" (the European Nucleotide Archive [ENA] is part of the
European Bioinformatics Insitute [EBI]) in the same way SRA is part of NCBI,
so, we can flag ENA. And then I want to check the "not describe" column. 

## Other data

I would also like to check and potentially clean up the "other" categories to
make the analysis easier.

```{r, echo=FALSE}
data %>% 
  select(doi, starts_with('sources_1o')) %>%
  filter(., (`sources_1o.other.` %in% c('1', 1)) | !is.na(sources_1o_other)) %>%
  reactable(defaultPageSize = 15)
```

For our analysis, we're going to code NCBI as SRA (since they're the same thing)
and we're going to dropt he place where data was uploaded to ImmPort, since that's
about *their* data sharing, not about the data sources.

## Tidy up

As we tidy, I'm going to create a combined SRA/ENA column, since the two are
mirrored and then classify data in SRA/ENA, MG-RAST, Qiita as "open" and 
consortium and request data as closed.

```{r}
cleaned[['sources_primary']] <- 
  data %>%
  select(c(doi, starts_with('sources_1o'))) %>%
  mutate(., `sources_1o.consortium_authors.` = case_when(doi == '10.1371/journal.pcbi.1004468' ~ '1'
                                                        ,doi == '10.1093/braincomms/fcab113' ~ '0'
                                                        ,.default=`sources_1o.consortium_authors.`)
          , `sources_1o.SRA.` = case_when(doi == '10.1093/braincomms/fcab113' ~ '1'
                                          ,grepl('NCBI', sources_1o_other) ~ '1'
                                          ,.default=`sources_1o.SRA.`)
          , `sources_1o.request_authors.` = case_when(doi == '10.1093/braincomms/fcab113' ~ '1'
                                                      ,.default=`sources_1o.request_authors.`)
          , `sources_1o.mg_rast.` = case_when(grepl('mgrast', tolower(sources_1o_other)) ~ '1'
                                            ,grepl('mg-rast', tolower(sources_1o_other)) ~ '1'
                                            ,.default=`sources_1o.mg_rast.`)
          , `sources_1o.ena.` = case_when(grepl('EBI', `sources_1o.not_described.`) ~ '1'
                                          ,grepl('EBI', sources_1o_other) ~ '1'
                                          ,.default=`sources_1o.ena.`)
          , `sources_1o.not_described.` = case_when(doi == '10.3389/fcimb.2020.00434' ~ '0'
                                                    ,doi == '10.1186/s13059-019-1908-8' ~ '0'
                                                    ,doi == '10.1093/braincomms/fcab113' ~ '0'
                                                    ,.default=`sources_1o.not_described.`)
          , `sources_1o.other.` = case_when(sources_1o_other == 'NCBI' ~ '0'
                                           ,.default= `sources_1o.other.`)
          , sources_1o_other = case_when(sources_1o_other == 'NCBI' ~ NA
                                         ,grepl('mg-rast', tolower(sources_1o_other)) ~ str_remove(sources_1o_other, 
                                                                                                   ', MG-RAST')
                                         ,sources_1o_other %in% c('0', 0) ~ NA
                                         ,.default = sources_1o_other)
          ) %>%
  mutate_at(., .vars=vars(starts_with('sources_1o.')), .funs=as.numeric) %>%
  rowwise() %>%
  mutate(., `sources_1o.insdc.` = max(`sources_1o.SRA.`, `sources_1o.ena.`)
          , `sources_1o.any_open.` = max(`sources_1o.insdc.`, 
                                         `sources_1o.mg_rast.`,
                                         `sources_1o.qiita.`)
          , `sources_1o.any_closed.` = max( `sources_1o.request_authors.`, 
                                            `sources_1o.consortium_authors.`)
          , `sources_1o.all_open.` = ((`sources_1o.any_open.` == 1) & 
                                     (`sources_1o.any_closed.` == 0) &
                                     (`sources_1o.not_described.` == 0) & 
                                     (`sources_1o.other.` == 0)) * 1
          ) %>%
   ungroup()
```


# Validation

We have three peices in the question about validation. We asked whether the 
data went through any validation; and if it did, we need to look at the
number of studies and samples included.

The questions were

* Did the study include any independent validation cohorts? (not included in the original analysis) [`validation`]
* How many studies were analyzed for validation (final number after processing)? (If not known, leave blank) [`num_studies_valid`]
* How many samples were analyzed for validation (final number after processing)? (if not listed, leave blank) [`num_sample_validation`]

Additionally, the number of validation studies and number of validation samples
are triggered by the validation question. So, a number should only be
provided for studies with a validation component.

```{r, echo=FALSE}
data %>%
  column_to_rownames('doi') %>%
  select(validation, num_studies_valid, num_sample_validation) %>%
  group_by(validation) %>% count() %>%
  reactable()
```

So, we need to start by cleaning up the validation data; I'd like to convert
it to numberic.

We also need to check the number of samples for places where there was
validation.

```{r, echo=FALSE}
data %>%
  column_to_rownames('doi') %>%
  select(validation, num_studies_valid, num_sample_validation) %>%
  filter(., (tolower(validation) %in% c('yes')) | 
           (!is.na(num_studies_valid)) | 
           (!is.na(num_sample_validation))) %>%
  reactable()
```
I want to check the study where we have a sample size but not a number of 
studies.



### 10.1038/s41396-020-0727-y

Reviewing `0.1038/s41396-020-0727-y`, there were no reserved cohorts in the 
meta analysis. However, a small cohort (28 samples) was included as an overall
validation cohort. However, this was profiled with metagenomics, 
metatranscriptomics, and metabolomics. I'm not sure I'd describe this as a
validation cohort for the 16S studies, but if it's described that was, there's
one cohort.

So, then, we'll clean up the data?

```{r}
cleaned[['num_samples_valid']] <- 
  data %>% 
  select(c(doi, validation, num_studies_valid, num_sample_validation)) %>%
  mutate(., validation = (tolower(validation) == 'yes') * 1
          , num_studies_valid = ifelse(doi == '10.1038/s41396-020-0727-y', 1, num_studies_valid)
          ) %>%
  mutate_at(., .vars=vars(contains('num'))
            , .funs=~ifelse(validation == 0, NA, .)) %>%
  mutate_at(., .vars=vars(-doi), .funs=as.numeric)
```

# Validation Sources

For studies with validation, we also had a triggered question about where the
sources for the validationd data:

"What were the sources for sequences and metadata for the validation analysis?"

This was a select all that apply question with options

| Column | Response Text |
| :--- | :--- |
| `valid_source[consortium_authors]` | The authors of the meta-analysis or consortium |
| `valid_source[SRA]` | SRA/Genbank |
| `valid_source[ena]` | ENA |
| `valid_source[request_authors]` | Request to authors of the original data set (not co-authors of meta analysis) |
| `valid_source[mg_rast]` | MG-RAST |
| `valid_source[qitta]` | QIIME DB or Qiita |
| `valid_source[other]` | Other |
| `valid_source[not_described]` | Not described |
| `valid_source_other` | *The text field for the "other" column |

```{r, echo=FALSE}
data %>% 
  select(doi, validation, starts_with('valid_source')) %>%
  filter(., validation %in% 'Yes') %>%
  rename_with(., .cols=starts_with('valid_source.')
               , .fn=~str_remove(str_remove(., 'valid_source.'), '\\.')) %>%
  reactable()
```
The validation sources look reasonable for cases where we have validation.

When we clean up the data, we're going to drop a validation description from
anyone who doesn't have validation.

```{r}
cleaned[['valid_sources']] <- 
  data %>%
  select(c(doi, validation, starts_with('valid_sour'))) %>%
  mutate_at(., .vars=vars(contains('valid_source.'))
             , .funs=~ifelse(!(validation == "Yes"), '0', .)
             ) %>%
  mutate_at(., .vars=vars(c(contains('_other')))
             , .funs=~ifelse(!(validation == "Yes"), NA, .)
             ) %>%
  mutate_at(., .vars=vars(contains('valid_source.'))
             , .funs=as.numeric
            ) %>%
  select(c(doi, starts_with('valid_source')))
```

# Envioment and body site

Enviroment was described through a series of questions. The first was
the defination of the enviroments included:

"Which environments were included?"

The question was a select all with possible options

| Column  | Response Text  | Triggered Questions |
|:--- | :--- | :--- |
| `envo[built_envo]` | Built environment (i.e. house walls, office floor, etc) |
| `envo[host]`  | host | `species`; `species_compare` |
| `envo[human]` | Human-associated | `envo_bodysite` |
| `envo[other]` | Other  |
| `envo_other` | *Free text for the other column* |

For the non-human host comparison, we have

* What non-human animal species were included? [`species`]
* If non-human animals were included, was this analysis a comparison between host species? [`species_compare`]

And then, theres the body site question:

"If human or non-human animals were included, which body sites were analyzed?"

with possible select all responses of

| Column | Response Text |
| :--- | :--- |
| `envo_bodysite[gut]` | Gut (Feces, rectal swab, biopsy) |
| `envo_bodysite[oral]` | Oral |
| `envo_bodysite[skin]` | Urogenital |
| `envo_bodysite[urogenital]` | Skin |
| `envo_bodysite[airway]` | Airway |
| `envo_bodysite[other]` | Other |
| `envo_bodysite_other` | *Other text* |

## Missing data

```{r, echo=FALSE}
data %>% 
  select(c(doi, starts_with('envo'))) %>%
  column_to_rownames('doi') %>%
  mutate_at(., .vars=vars(c(starts_with('envo.'), starts_with('envo_bodysite.')))
             , .funs=~(!(. %in% c('0', '1', 0, 1)))
             ) %>%
  rowwise() %>%
  mutate(., miss_envo = any(select(., starts_with('envo.')))
          , miss_bodysite = any(select(., starts_with('envo_bodysite.')))
          ) %>%
  ungroup() %>%
  summarise(., across(starts_with('miss'), sum)) %>%
  reactable()
```
So, all our columns are appropriately defined. that makes this *a lot* easier.

## Enviroment

Next, I want to check that we have all human samples and look at where we see
other environments.

```{r, echo=FALSE}
data %>%
  select(starts_with('envo.')) %>%
  mutate_at(., .vars=vars(everything()), .funs=as.numeric) %>%
  summarise(., across(everything(), mean)) %>%
  mutate_all(., .funs=~round(., 3)) %>%
  reactable()
```
```{r, echo=FALSE}
data %>%
  column_to_rownames('doi') %>%
  select(starts_with('envo.'), envo_other, species, species_compare) %>%
  mutate_at(., .vars=vars(starts_with('envo.'))
             , .funs=as.numeric) %>%
  mutate(., multi_envo = rowSums(select(., starts_with('envo.')))) %>%
  filter(., multi_envo > 1) %>%
  select(-c(multi_envo, `envo.human.`)) %>%
  reactable()
```


It looks like we have 3 cases where we have only host and human 
(`10.1186/s13059-019-1908-8`, `10.3389/fmicb.2016.00660`, and 
`10.1371/journal.pone.0062578`) and other case where we have pan environmental 
surveys that humans are one part of (`10.1038/ismej.2013.54`, 
`10.1371/journal.pcbi.1004468`, `10.1073/pnas.1000080107`). I think we 
should treat the pan enviromental surveys as their own enviromental class.

### 10.1186/s13059-019-1908-8

I want to double check `10.1186/s13059-019-1908-8`, which lists the other
environment as plants. We'll colde the multi species () as pan enviromental
and keep the other two as is. (I think `10.3389/fmicb.2016.00660` references the
non-humna primate survey from [Muegge  et al, 2011](https://pmc.ncbi.nlm.nih.gov/articles/PMC3303602/)
which focused on non-human primates (divided by gut fermentation type). So,
I think its maybe easier to describe these and non-human primates?

So, that handles environment. Let's look at body sites. We don't have any
missing data, so let's start by looking at the "other" body sites.

```{r, echo=FALSE}
data %>%
  column_to_rownames('doi') %>%
  select(starts_with('envo')) %>%
  mutate_at(., .vars=vars(-contains('_other'))
             , .funs=as.numeric) %>%
  mutate(., envo = case_when((`envo.other.` == 1) ~ 'pan enviromental'
                            ,(`envo.host.` == 1) ~ 'human and host'
                            ,.default = 'human only')
          ) %>%
  select(-c(starts_with('envo.'), envo_other)) %>%
  filter(., (`envo_bodysite.other.` == 1) | !(is.na(envo_bodysite_other))) %>%
  reactable()
```
I think we'll ignore body site in pan environmental comparisons and then I think
we can accept the sinonasal as upper airway. So, I think we're all good
on the "other" body sites.

So, from there, I think we also want to build a reference of the number of 
bodysites and what kind of overlaps we see, and I think we can work past that
for the final data sets?

```{r}
cleaned[['envo']] <- data %>%
  select(c(doi, starts_with('envo'), species, species_compare)) %>%
  select(-c(`envo_bodysite.other.`, envo_bodysite_other)) %>%
  mutate_at(., .vars=vars(starts_with('envo.'), starts_with('envo_bodysite.'))
             , .funs=as.numeric) %>%
  mutate(., envo = case_when((`envo.other.` == 1) ~ 'pan environmental'
                            ,(`envo.host.` == 1) ~ 'human and host'
                            ,.default = 'human only')
          , species = case_when(envo == 'human only' ~ NA
                               ,envo == 'pan environmental' ~ NA
                               ,doi == '10.1186/s13059-019-1908-8' ~ 'Non-human primates'
                               ,.default = species)
        ) %>%
  mutate(., bs_code = ((`envo_bodysite.gut.` * 2) + 
                       (`envo_bodysite.oral.` * 4) + 
                       (`envo_bodysite.skin.` * 8) + 
                       (`envo_bodysite.urogenital.` * 16) + 
                       (`envo_bodysite.airway.` * 32))
          , envo_bodysite_count = rowSums(select(., starts_with('envo_bodysite.')))
          , envo_bodysite_tidy = case_when(envo == 'pan environmental' ~ envo
                                          ,envo_bodysite_count >= 3 ~ 'three or more sites'
                                          ,bs_code == 2 ~ 'gut'
                                          ,bs_code == 4 ~ 'oral'
                                          ,bs_code == 8 ~ 'skin'
                                          ,bs_code == 16 ~ 'urogenital'
                                          ,bs_code == 32 ~ 'airway'
                                          ,bs_code %in% c(6, 10, 18, 34) ~ 'gut and one other site'
                                          ,bs_code == 36 ~ 'oral and airway')
        ) %>%
  select(c(doi, envo, species, species_compare, starts_with('envo_bodysite')))
```

# Data Described

We asked how much information was described in the study, with the question

* Which of the following are reported for the studies that were combined?

The question was select all with possible choices

| Column | Response Text |
| :--- | :--- |
| `design_info[population]` | Population description |
| `design_info[experimental_design]` | Experimental design (randomization, variable matching) |
| `design_info[sampling_method]` | Sampling method(s)/collection kit(s) |
| `design_info[collection_kit]` | Population description |
| `design_info[extraction_kit]` | Extraction kit(s) used |
| `design_info[hypervariable_region]` | Hypervariable region(s) |
| `design_info[sequencer]` | Sequencing platform |

We'll start by looking for missing values.

## Missing

```{r, echo=FALSE}
data %>% 
  column_to_rownames('doi') %>%
  select(starts_with('design_info')) %>%
  mutate_at(., .vars=vars(starts_with('design_info.'))
             , .funs=~(!(. %in% c('0', '1')))
             ) %>%
  summarise(across(everything(), sum)) %>%
  rename_with(., .cols=starts_with('design_info.')
               , .fn = ~str_remove(str_remove(., 'design_info.'), '\\.')) %>%
  reactable()
```

## Tidy Data
So, all the data is defined, and we can move forward.

```{r}
cleaned[['design_info']] <- 
  data %>% 
  select(doi, starts_with('design_info')) %>%
  mutate_at(., .vars=vars(starts_with('design_info.'))
             , .funs=as.numeric)
```

# Sequence Platform

If the sequencing platforms were described, we identified the sequencing
platforms that were used.

* "Which sequencing platforms were used?"

This was a select all question, with possible responses

| Column | Response Text |
| :--- | :--- | 
| `seq_platforms[454]` | 454 pyrosequencing |
| `seq_platforms[illumina]` | Illumina (MiSeq, HiSeq, NovoSeq) |
| `seq_platforms[ion_torrent]` | 454 pyrosequencing |
| `seq_platforms[other]` | Other |
| `seq_platforms_other` | *other free text* |

## Missingd data

```{r, echo=FALSE}
data %>%
  column_to_rownames('doi') %>%
  select(c(`design_info.sequencer.`, starts_with('seq_platforms'))) %>%
  mutate_at(., .vars=vars(starts_with('seq_platforms.'))
             , .funs=~!(((`design_info.sequencer.` == '0') & is.na(.)) | (. %in% c('0', '1')))
            ) %>%
  select(starts_with('seq_platforms.')) %>%
  summarise(across(everything(), sum))
```

So, it looks like all of our data is coded in a reasonable way and we don't 
have to evaluate missing values.

## Other 

Let's check out the "other" platform.

```{r, echo=FALSE}
data %>% 
  column_to_rownames('doi') %>%
  select(c(`design_info.sequencer.`, starts_with('seq_platforms'))) %>%
  mutate_at(., .vars=vars(c(`design_info.sequencer.`, starts_with('seq_platforms.')))
             , .funs=as.numeric) %>%
  filter(., `design_info.sequencer.` == 1) %>%
  filter(., (`seq_platforms.other.` == 1) | !(is.na(`seq_platforms_other`))) %>%
  select(-c(`design_info.sequencer.`)) %>%
  rename_with(., .fn=~str_remove(str_remove(., 'seq_platforms.'), '\\.')
               , .cols=starts_with('seq_platforms.')) %>%
  reactable()
```



So, let's check the "other" sequencer models and see where they fit?

### 10.3389/fimmu.2021.722206

Looking up the data, PGM is the same as the Ion torrent system, so we can
flag that consistently. And then, the MiniSeq is an illumina model. So, the 
data is coded correctly, but we're goig to drop the "other" flag here.

### 10.1093/cid/ciz258

Roche 454 is the same as 454? So, we're going to just keep the flag for 454
sequencing.

### 10.1186/s40168-018-0479-3

This article is a genetic meta-analysis and all of these are genetics platforms.
So, we're going to not treat these as "other" platforms.

### 10.1101/gr.151803.112

And then we need to clean up the "other" category so that we don't have an
empty flag. 

## Tidy data

So, we'll clean up the data and finish here.

```{r}
cleaned[['sequencer']] <-
  data %>%
  select(c(doi, `design_info.sequencer.`, starts_with('seq_platforms'))) %>%
  mutate_at(., .vars=vars(c(`design_info.sequencer.`, starts_with('seq_platforms.')))
             , .funs=as.numeric) %>%
  mutate_at(., .vars=vars(starts_with('seq_platforms.'))
             , .funs=~ifelse(`design_info.sequencer.` == 0, 0, .)
             ) %>%
  select(-c(contains('other'), `design_info.sequencer.`))
```

# Hypervariable Region

This is another case where we have a triggered question, where we're checking
which hypervariable regions were included in the original analysis.

The triggered questions was

* "Which hypervariable regions were included"

And this had multiple possible responses, select all:

| Column | Response Text |
| :--- | :--- |
| `hypervar_regions[V12]` | V12 |
| `hypervar_regions[V13]` | V13 |
| `hypervar_regions[V2]` | V2 |
| `hypervar_regions[V23]` | V23 |
| `hypervar_regions[V3]` | V3 |
| `hypervar_regions[V34]` | V34 |
| `hypervar_regions[V35]` | V35 |
| `hypervar_regions[V4]` | V4 |
| `hypervar_regions[V45]` | V45 |
| `hypervar_regions[V46]` | V46 |
| `hypervar_regions[V68]` | V68 |
| `hypervar_regions[Other]` | Other |
| `hypervar_regions_other` | *other hypervariable region* |


## Missing data

```{r, echo=FALSE}
data %>% 
  column_to_rownames('doi') %>%
  select(c(`design_info.hypervariable_region.`, starts_with('hypervar_regions'))) %>%
  mutate_at(., .vars=vars(starts_with('hypervar_regions.'))
             , .funs=~!(((`design_info.hypervariable_region.` == '0') & (is.na(.) | (. == '0'))) |
                       (. %in% c('0', '1')))
            ) %>%
  select(starts_with('hypervar_regions.')) %>%
  summarise(across(everything(), sum)) %>%
  rename_with(., .cols=starts_with('hypervar_regions.')
               , .fn=~str_remove(str_remove(., 'hypervar_regions.'), '\\.')
              ) %>%
  pivot_longer(everything()) %>%
  reactable(., defaultPageSize = 12)
```
So, we have one column where we need to check the hypervariable regions that 
were included/missing. So, let's find that?

```{r, echo=FALSE}
data %>% 
  column_to_rownames('doi') %>%
  select(c(`design_info.hypervariable_region.`, starts_with('hypervar_regions.'))) %>%
  filter(., ((`design_info.hypervariable_region.` == '0') & !(is.na(`hypervar_regions.V3.`) | (`hypervar_regions.V3.` == '0'))) | 
             (`design_info.hypervariable_region.` == '1') & !(`hypervar_regions.V3.` %in% c('0', '1'))
          ) %>%
  select(-c(`design_info.hypervariable_region.`)) %>%
  rename_with(., .fn=~str_remove(str_remove(., 'hypervar_regions.'), '\\.')) %>%
  rownames_to_column('doi') %>%
  pivot_longer(cols=-c(doi)) %>%
  pivot_wider(., id_cols = name, names_from = doi, values_from = value) %>%
   reactable(., defaultPageSize = 12)
```
So, we need to look at one paper.



### 10.1007/s12275-022-1526-0

Looking at Table 1, which I'm not reproducing below, we don't see a V3 independent primer.

| Study (year) | Healthy (n) | Non-healhty  (n) | Details of non-healthy group | Sequencing Platform | 16S hyper-varaible region | Ethnicity | Reference |
| :----------- | ----------: | ---------------: | :--------------------------- | ------------------- | ------------------------- | --------- | --------- | 
| CKGMP (2018–2020) | 587 | 524 | ... | Illumina MiSeq  | V3-V4 Korea | This study |
| CMJHP (2018) | 42  | 46 | ... |  Illumina MiSeq | V3-V4 |  Korea | This study |
| CSMCP (2016–2017) | 150  | 314 | ... |  Illumina MiSeq | V3-V4 | Korea | This study |
| Smits (2017) | 350 | - | - | Illumina MiSeq | V4 | Africa |  PMID 28839072 |
| Oki (2016) | 516 | - | - | Illumina MiSeq | V1-V2 | Japan | PMID 27894251 |
| Baxter (2016) | 382 | 615 | ... | Illumina MiSeq | V4 | North America | PMID 27842559 |
| Bressa (2016) | 40  | - | - | Illumina MiSeq | V3-V4 |  Europe | PMID 28187199 |
| Zeller (2014) |  98 | 127 | ... |  Illumina MiSeq | V4 | Europe | PMID 25432777 |
| Zhang (2014) | 320 | - |  - | 454 | V1-V3 | Mongolia | PMID 24833488 |
| Gevers (2014) | 28 | 256  | ... |  Illumina MiSeq | V4 | North America | PMID 24629344 |
| Yatsunenko (2012) | 528 | - |  - | Illumina MiSeq |  V4 | South America, North America, Africa |  PMID 22699611 |

So, we're going to code that as a '0', corresponding to "No".

## Other category

```{r, echo=FALSE}
data %>%
  column_to_rownames('doi') %>%
  select(c(`design_info.hypervariable_region.`, starts_with('hypervar_regions'))) %>%
  filter(., `design_info.hypervariable_region.` == '1') %>%
  filter(., (`hypervar_regions.Other.` == '1') | !is.na(`hypervar_regions_other`)) %>%
  select(-c(design_info.hypervariable_region.)) %>%
  rename_with(., .fn=~str_remove(str_remove(., 'hypervar_regions.'), '\\.')
               , .cols=starts_with('hypervar_regions.')) %>%
   reactable(., defaultPageSize = 12)
```

It looks like the V5-V6 hypervariable region is a common primer pair, so I want
to check that? 

And then we'll keep the remainer, and clean up the missing values. I sort of
want to check the V14 primers, since that feels like a weird primer pair, but
maybe we assume the original article coded it correctly.

## Tidy Data

```{r, echo=FALSE}
cleaned[['hypervariable']] <-
  data %>% 
  select(c(doi, `design_info.hypervariable_region.`, starts_with('hypervar_regions'))) %>%
  mutate(., `hypervar_regions.V3.` = ifelse(doi == '10.1007/s12275-022-1526-0', '0', 
                                            `hypervar_regions.V3.`)
          ) %>%
  mutate_at(., .vars=vars(starts_with('hypervar_regions.'))
             , .funs=~ifelse(`design_info.hypervariable_region.` == '0', '0', .)
             ) %>%
  select(-c(`design_info.hypervariable_region.`)) %>%
  mutate_at(., .vars=vars(starts_with('hypervar_regions.'))
             , .funs=as.numeric
             ) %>%
  mutate(., `hypervar_regions.V56.` = grepl('V56', hypervar_regions_other) * 1
          , `hypervar_regions_other` = str_remove(hypervar_regions_other, 'V56')
          , `hypervar_regions_other` = str_remove(hypervar_regions_other, ', ')
          , `hypervar_regions.Other.` = ifelse(
                (`hypervar_regions.V56.` == 1) & (hypervar_regions_other == ''), 
                0, `hypervar_regions.Other.`)
          , hypervar_regions_other = ifelse(`hypervar_regions.Other.` == 0, NA,
                                            hypervar_regions_other)
                                                 , 
          ) %>%
  mutate(., other_count = ifelse(`hypervar_regions.Other.` == 1, str_count(hypervar_regions_other, ',') + 1, 0)
          , main_count = rowSums(select(., contains('.V')))
          , hypervar_regions_count = main_count + other_count
          , `hypervar_regions.other.` = `hypervar_regions.Other.`
          ) %>%
  select(c(doi, `hypervar_regions.V12.`:`hypervar_regions.V46.`, 
           `hypervar_regions.V56.`, `hypervar_regions.V68.`, 
           `hypervar_regions.other.`, hypervar_regions_other, 
           hypervar_regions_count))
```

# Table construction

Our table construction incorporates several ideas, and we likely need to 
clean all the data together.

The first, major question is whether the data was analyzed using a common 
processing pipeline (if not, we dont ask any more questions about the pipeline.)

* Were all the data processed using a similar pipeline? [`common_pipeline`]

If the data was analyzed with a common pipeline, we asked if the authors did
a taxonomic analysis or if they predicted function, or both

 * Did the authors perform taxonomic profiling and analyze the data? (i.e. denoise to ASVs, OTU clustering)? [`taxa_profile`]
 * Did the authors perform functional profiling? (i.e. tools like PICRUSt, Tax4Fun, mention of KEGGs in results) [`fun_profile`]

We then go into how the taxonomic feature table was constructed. So, did the
authors denoise to get ASVs, did they cluster the data into OTUs, or did they
perform some kind of quality filtering without OTUs and ASVs.

* Were sequences denoised? (This might be indicated by the used of DADA2, Deblur, Unoise3, or the description of features as ASVs)? [`asvs`]
 * Were the sequences clustered into operational taxonomic units (OTUs)? (This might be
indicated by the use of algorithms like mothur, usearch, or vsearch or the mention of
clustering)[`otus`]
 * If the data was not denoised or clustered into OTUs, was the data collapsed to a taxonomic level without denoising or clustering? [`qual_filter`]
 
Specifically of note, `qual_filter` is only triggered when data was not denoised
to produce ASVs or clustered into OTUs.

And then, if the dataset was clustered into OTUs, we need to identify the 
type. For this, we asked

* What type of OTU clustering was performed and used for primary analysis (if multiple types of clustering were performed, or validation clustering was perform)?

This was as select all question with possible responses

| Column | Response Text |
| :--- | :--- |
| `otu_type[de_novo]` | de novo clustering |
| `otu_type[open_ref]` | open reference clustering |
| `otu_type[closed_ref]` | closed reference clustering |
| `otu_type[unclear]` | Not described/unclear |
| `otu_type[not_described]` | Not described |

And then, after all this processing, I want to end up with a column that 
described the type of feature table construction (ASVs, OTU type, or quality 
filtered without denoising or clustering, unknown, or no common pipeline).

We're going to break this down into individual steps, because of the 
complexity of the question, but acknoweldging that it's interconnected.


## Missing pipeline

```{r, echo=FALSE}
data %>% 
  column_to_rownames('doi') %>%
  select(c(common_pipeline, taxa_profile, fun_profile)) %>%
  mutate_at(., .vars=vars(everything())
             , .funs=~((is.na(.) | !(. %in% c('No', 'Yes', 'Not described')))) * 1
            ) %>%
  summarise(across(everything(), sum)) %>%
  reactable()
```

So, we are missing descriptions in multiple places, and we need to check the 
missing values for the common taxa profile and functional profile.

```{r, echo=FALSE}
data %>% 
  column_to_rownames('doi') %>%
  select(c(common_pipeline, taxa_profile, fun_profile)) %>%
  mutate(., miss_pipeline = (is.na(common_pipeline) | 
                             !(common_pipeline %in% c('No', 'Yes', 'Not described')))
          , miss_taxa = (is.na(taxa_profile) | 
                         !(taxa_profile %in% c('No', 'Yes', 'Not described')))
          , miss_fun = (is.na(fun_profile) | 
                        !(fun_profile %in% c('No', 'Yes', 'Not described')))
          , miss_any = (miss_pipeline | miss_taxa | miss_fun)
          ) %>%
  filter(., miss_any) %>% 
  select(-c(starts_with('miss'))) %>%
  mutate_at(., .vars=vars(everything())
             , .funs=~ifelse(is.na(.), 'NA', as.character(.))) %>%
  reactable()
```

So, I want to check `10.1038/s41598-020-69537-3` where we have a 
"not described"flag and `10.1111/bjd.20626`, where we're not sure if there 
was a taxa profile.

#### 10.1038/s41598-020-69537-3

Their "meta-analysis" section is as follows:

> **Meta-analysis**
> 
> We performed a systematic literature search of PubMed databases up to 
> March 31, 2020 using the following terms: “Psoriasis” and “gut microbiota”
> or “gut microbiome”. The study inclusion criteria were: Case–control studies
> with publicly available raw 16S data and metadata, indicating case/control
> status for each sample. Studies including patients with other clinical forms
> different from plaque psoriasis and patients under systemic treatment 
> (DMARDS and biologics) were excluded.

Which very much means that whether or not their was a common pipeline is not
described, and their supplement is two figures. So, this is accurate coding
and it should be NA.

#### 10.1111/bjd.20626

Looking at the methods,

> **Sequencing data processing and statistical analysis**
> ...
> 
> To perform a meta‐analysis to compare bacterial communities from our study 
> against the cohort of patients with metastatic melanoma from Matson et al.,
> 16S‐targeted metagenomics raw data were firstly retrieved from the Sequence
> Read Archive (accession SRP116709). Given that in Matson et al. the sole
> V4 hypervariable region was targeted, we firstly had to make the two 
> datasets comparable by selecting the portion corresponding to the V4 
> hypervariable region from our data. To do so, we used Cutadapt to trim and 
> discard the portion of our reads before the forward primer used in Matson et 
> al. (i.e. primer 515f; in Cutadapt we used the ‐g option for the primer 
> sequence and the option ‐discard untrimmed). After trimming, the two 
> datasets were joined and then processed in MICCA for OTU picking using the
> ‘greedy denovo’ algorithm with a similarity threshold of 97%, and for 
> taxonomy assignment. We preferred the use of the similarity‐based OTU 
> picking algorithm to avoid any assumptions or considerations connected to 
> the more recent zOTU picking method (UNOISE3) that we used for our data.

So, they did taxonomic analysis in this paper. 

## OTU clustering and ASV construction

```{r, echo=FALSE}
data %>%
  select(c(doi, common_pipeline, taxa_profile, asvs, otus, qual_filter)) %>%
  mutate(., common_pipeline = ifelse(tolower(common_pipeline) == 'yes', 1, 0)
          , taxa_profile = case_when(doi == '10.1111/bjd.20626' ~ 1
                                    ,common_pipeline == 0 ~ 0
                                    ,taxa_profile == 'Yes' ~ 1
                                    ,taxa_profile == "No" ~ 0
                                    ,.default=NA)
          ) %>%
  filter(., taxa_profile == 1) %>%
  column_to_rownames('doi') %>%
  select(c(asvs, otus)) %>%
  mutate_at(., .vars=vars(everything())
             , .funs=~!(!is.na(.) & (. %in% c('No', 'Yes', 'Unclear/Not described')))
             ) %>%
  summarise(across(everything(), sum)) %>%
  reactable()
```

So, we need to check the OTUs flag

```{r, echo=FALSE}
data %>%
  select(c(doi, common_pipeline, taxa_profile, otus)) %>%
  mutate(., common_pipeline = ifelse(tolower(common_pipeline) == 'yes', 1, 0)
          , taxa_profile = case_when(doi == '10.1111/bjd.20626' ~ 1
                                    ,common_pipeline == 0 ~ 0
                                    ,taxa_profile == 'Yes' ~ 1
                                    ,taxa_profile == "No" ~ 0
                                    ,.default=NA)
          ) %>%
  filter(., (taxa_profile == 1)) %>%
  filter(., is.na(otus) | !(otus %in% c('No', 'Yes', 'Unclear/Not described'))) %>%
  select(c(doi, otus)) %>%
  mutate_at(., .vars=vars(-c(doi))
             , .funs=~ifelse(is.na(.), 'NA', as.character(.))) %>%
  reactable()
```
So, we need to review the OTUs. Going back to the earlier description, we know
that they used OTUs for combining the data.

## Expanded table construction

```{r, echo=FALSE}
data %>%
  select(c(doi, common_pipeline, taxa_profile, asvs, otus, starts_with('otu_t'))) %>%
  mutate(., common_pipeline = ifelse(tolower(common_pipeline) == 'yes', 1, 0)
          , taxa_profile = case_when(doi == '10.1111/bjd.20626' ~ 1
                                    ,common_pipeline == 0 ~ 0
                                    ,taxa_profile == 'Yes' ~ 1
                                    ,taxa_profile == "No" ~ 0
                                    ,.default=NA)
          , asvs = case_when(taxa_profile == 0 ~ 0
                            ,asvs == 'Yes' ~ 1
                            ,asvs == 'No' ~ 0
                            ,asvs == 'Unclear/Not described' ~ 2)
          , otus = case_when(taxa_profile == 0 ~ 0
                            ,doi == '10.1111/bjd.20626' ~ 1
                            ,otus == 'Yes' ~ 1
                            ,otus == 'No' ~ 0
                            ,otus == 'Unclear/Not described' ~ 2)
          ) %>%
  mutate_at(., .vars=vars(starts_with('otu_type.'))
             , .funs=as.numeric) %>%
  mutate_at(., .vars=vars(starts_with('otu_type.'))
             , .funs=~case_when(common_pipeline == 0 ~ 0
                               ,otus == 0 ~ 0
                               ,.default = .)
            ) %>%
  select(starts_with('otu_t')) %>%
  mutate_at(., .vars=vars(everything()), .funs=~is.na(.)) %>%
  summarise(across(everything(), sum)) %>%
  reactable()
```
We need to check the OTU type for at least one manuscript.

```{r, echo=FALSE}
data %>%
  select(c(doi, common_pipeline, taxa_profile, asvs, otus, starts_with('otu_t'))) %>%
  mutate(., common_pipeline = ifelse(tolower(common_pipeline) == 'yes', 1, 0)
          , taxa_profile = case_when(doi == '10.1111/bjd.20626' ~ 1
                                    ,common_pipeline == 0 ~ 0
                                    ,taxa_profile == 'Yes' ~ 1
                                    ,taxa_profile == "No" ~ 0
                                    ,.default=NA)
          , asvs = case_when(taxa_profile == 0 ~ 0
                            ,asvs == 'Yes' ~ 1
                            ,asvs == 'No' ~ 0
                            ,asvs == 'Unclear/Not described' ~ 2)
          , otus = case_when(taxa_profile == 0 ~ 0
                            ,doi == '10.1111/bjd.20626' ~ 1
                            ,otus == 'Yes' ~ 1
                            ,otus == 'No' ~ 0
                            ,otus == 'Unclear/Not described' ~ 2)
          ) %>%
  mutate_at(., .vars=vars(starts_with('otu_type.'))
             , .funs=as.numeric) %>%
  mutate_at(., .vars=vars(starts_with('otu_type.'))
             , .funs=~case_when(common_pipeline == 0 ~ 0
                               ,otus == 0 ~ 0
                               ,.default = .)
            ) %>%
  filter(., is.na(`otu_type.de_novo.`) | is.na(`otu_type.closed_ref.`)) %>%
  select(c(doi, otus, starts_with('otu_t'))) %>%
  rename_with(., .cols=starts_with('otu_type.')
               , .fn=~str_remove(str_remove(., 'out_type.'), '\\.')
              ) %>%
  reactable()
```

Looking at the description, its based on "greedy denovo" clustering, which I'd
say is not closed reference clustering, but is de novo.

## Table comparison

```{r, echo=FALSE}
data %>%
  select(c(doi, common_pipeline, taxa_profile, asvs, otus, starts_with('otu_t'), qual_filter)) %>%
  mutate(., common_pipeline = ifelse(tolower(common_pipeline) == 'yes', 1, 0)
          , taxa_profile = case_when(doi == '10.1111/bjd.20626' ~ 1
                                    ,common_pipeline == 0 ~ 0
                                    ,taxa_profile == 'Yes' ~ 1
                                    ,taxa_profile == "No" ~ 0
                                    ,.default=NA)
          , asvs = case_when(taxa_profile == 0 ~ 0
                            ,asvs == 'Yes' ~ 1
                            ,asvs == 'No' ~ 0
                            ,asvs == 'Unclear/Not described' ~ 2)
          , otus = case_when(taxa_profile == 0 ~ 0
                            ,doi == '10.1111/bjd.20626' ~ 1
                            ,otus == 'Yes' ~ 1
                            ,otus == 'No' ~ 0
                            ,otus == 'Unclear/Not described' ~ 2)
          ) %>%
  mutate_at(., .vars=vars(starts_with('otu_type.'))
             , .funs=as.numeric) %>%
  mutate_at(., .vars=vars(starts_with('otu_type.'))
             , .funs=~case_when(common_pipeline == 0 ~ 0
                               ,otus == 0 ~ 0
                               ,.default = .)
            ) %>%
  mutate(., `otu_type.closed_ref.` = ifelse(doi == '10.1111/bjd.20626', 0, `otu_type.closed_ref.`)
          , `otu_type.de_novo.` = ifelse(doi == '10.1111/bjd.20626', 1, `otu_type.de_novo.`)
          ) %>%
  mutate(., otu_type = ((`otu_type.de_novo.` * 2) + 
                        (`otu_type.open_ref.` * 4) + 
                        (`otu_type.closed_ref.` * 8) + 
                        (`otu_type.unclear.` * 16) + 
                        (`otu_type.not_described.` * 32))
        ) %>%
  filter(., !otu_type %in% c(0, 2, 4, 6, 8, 10, 16, 32)) %>%
  select(doi, starts_with('otu_t')) %>%
  reactable()
```

So, let's look at the two data sets?

#### 10.1186/s40168-018-0479-3

So, this is a case where the study tried multiple annotation appraoches. From
the methods,

> Microbiome data processing
> 
> The microbiome data included in our consortium was mainly generated using an
Illumina sequencing platform (MiSeq or HiSeq). The most frequently sequenced
hyper-variable region of the 16S rRNA gene was V4 (eight cohorts, n=8472),
although five cohorts sequenced the V3-V4 region (n=5719), and another four
sequenced the V1-V2 region (n=4774). We assessed the compatibility of the
datasets obtained from sequencing different regions by comparing technical
replicates of ten samples (three replicates each) generated from different
hyper-variable regions. This analysis showed that the influence of technical
differences in microbiome profiles is less than the inter-individual
differences (Additional file 1). Nevertheless, including different
hyper-variable regions requires compatible methods of 16S rRNA gene-amplicon
data processing, and it is no longer feasible to use “open” (de novo)
operational taxonomic units (OTU) picking protocols. Further analysis of 
technical replicates using closed-reference OTU picking showed that the
clustering results also have large technical artifacts (Additional file 1).
In contrast, the between-replicate similarity on genera- and higher taxonomic
levels showed reasonable concordance (Additional file 1). As a result, we
implemented the 16S data processing pipeline, which comprised a naive 
Bayesian classifier from the Ribosomal Database Project [16], and the most
recent, full, SILVA database (release 128): we only analyzed taxonomical
results using genus- and higher taxonomic levels.

So, based on this, they decided against the OTU clustering technique and 
decided on a collapsed approach. From the supplemental material, we get that
they decided on a *de novo* appraoch. So, we'll code this as de novo OTUs and
not closed reference.

#### 10.1371/journal.pcbi.1002863

Looking at their results, they did both de novo and closed reference clustering:

> Effect of OTU-Picking Method
> 
> We compared enterotype clustering using two methods for OTU picking: 
(1) de novo sequence clustering into OTUs, in which sequences are clustered
based on similarity to one another, and (2) a reference based approach, in
which sequences are clustered based on similarity to sequences in a reference
database [27]. We found that for the HMP dataset, the two OTU picking
approaches yielded consistent results for the majority of body sites
(Fig. 5). However, for the attached keratinized gingiva, posterior fornix
and tongue dorsum, the reference-based approach provided moderate support for
enterotypes, whereas the de novo approach did not support clustering. One
important difference between the two OTU-picking approaches is that the
reference-based method can yield fewer OTUs, particularly at fine taxonomic
resolution, because any sequence that fails to find a match in the database
is discarded. In contrast, the de novo approach retains all sequences and has
the potential to yield higher OTU counts. Fewer OTUs would have the effect of
increasing the relative abundances of the dominant genera, and may therefore
strengthen the gradient effect frequently observed (see below). Thus, the
reference-based OTU picking approach may result in over-confidence in
enterotype discovery.

Which suggests they did both and we can accept this coding?

## Quality filtering 

And then let's look at the quality filtering question, which is only triggered
when OTUs and ASVs are not true. So, I want to check the coding here.

```{r, echo=FALSE}
data %>%
  select(c(doi, common_pipeline, taxa_profile, asvs, otus, qual_filter)) %>%
  mutate(., common_pipeline = ifelse(tolower(common_pipeline) == 'yes', 1, 0)
          , taxa_profile = case_when(doi == '10.1111/bjd.20626' ~ 1
                                    ,common_pipeline == 0 ~ 0
                                    ,taxa_profile == 'Yes' ~ 1
                                    ,taxa_profile == "No" ~ 0
                                    ,.default=NA)
          , asvs = case_when(taxa_profile == 0 ~ 0
                            ,asvs == 'Yes' ~ 1
                            ,asvs == 'No' ~ 0
                            ,asvs == 'Unclear/Not described' ~ 2)
          , otus = case_when(taxa_profile == 0 ~ 0
                            ,doi == '10.1111/bjd.20626' ~ 1
                            ,otus == 'Yes' ~ 1
                            ,otus == 'No' ~ 0
                            ,otus == 'Unclear/Not described' ~ 2)
          ) %>%
  filter(., ((taxa_profile == 1) & !((otus == 1) | (asvs == 1))) | !is.na(qual_filter)) %>%
  reactable()
```


The only place I see a major problem is `10.1371/journal.pcbi.1010066`, where
we have OTUs flagged and the quality filtering is not described. 

#### 10.1371/journal.pcbi.1010066

Looking at the methods section,

> We additionally analysed 4,026 16S rRNA samples coming from 30 publicly
available case-control studies (S1 Table and Fig 2A). We considered the
same set of gut samples considered in [13] with metadata information in
terms of disease status as follows: autism spectrum disorder (ASD),
Clostridioides difficile infection (CDI), CRC, enteric diarrheal disease
(EDD), human immunodeficiency virus (HIV), IBD, liver cirrhosis (CIRR),
minimal hepatic encephalopathy (MHE), non-alcoholic steatohepatitis (NASH),
obesity (OB), Parkinson disease, psoriatic arthritis (PSA), rheumatoid
arthritis (RA), and T1D. 16S rRNA samples were pre-processed following the
same procedure adopted in [13]. More specifically, we discarded samples with
fewer than 100 reads and removed OTUs with less than 10 reads and/or 
present in less than 1% of the samples. After calculating the relative
abundance of each OTU, OTUs were collapsed to genus level by summing their
relative abundance values and by discarding any OTUs which were un-annotated
at the genus level.

The citation [13] is [Duvallet et al, 2017](https://pubmed.ncbi.nlm.nih.gov/29209090/)
which used a *de novo* appraoch, based on their methods? 

## Tidy data

```{r}
cleaned[['table']] <-
  data %>%
  select(c(doi, common_pipeline, taxa_profile, asvs, otus, 
           starts_with('otu_type'), qual_filter)) %>%
  mutate(., common_pipeline = ifelse(tolower(common_pipeline) == 'yes', 1, 0)
          , taxa_profile = case_when(doi == '10.1111/bjd.20626' ~ 1
                                    ,common_pipeline == 0 ~ 0
                                    ,taxa_profile == 'Yes' ~ 1
                                    ,taxa_profile == "No" ~ 0
                                    ,.default=NA)
          , asvs = case_when(taxa_profile == 0 ~ 0
                            ,asvs == 'Yes' ~ 1
                            ,asvs == 'No' ~ 0
                            ,asvs == 'Unclear/Not described' ~ 2)
          , otus = case_when(taxa_profile == 0 ~ 0
                            ,doi == '10.1111/bjd.20626' ~ 1
                            ,otus == 'Yes' ~ 1
                            ,otus == 'No' ~ 0
                            ,otus == 'Unclear/Not described' ~ 2)
          , qual_filter = case_when(taxa_profile == 0 ~ 0
                                   ,otus == 1 ~ 0
                                   ,asvs == 1 ~ 0
                                   ,qual_filter %in% c('0', 0, 'No') ~ 0
                                   ,qual_filter %in% c('1', 1, 'Yes') ~ 1
                                   ,qual_filter %in% c('unclear/not described') ~ 2
                                   )
                                      
          ) %>%
  mutate_at(., .vars=vars(starts_with('otu_type.'))
             , .funs=as.numeric) %>%
  mutate_at(., .vars=vars(starts_with('otu_type.'))
             , .funs=~case_when(common_pipeline == 0 ~ 0
                               ,otus == 0 ~ 0
                               ,.default = .)
            ) %>%
  mutate(., `otu_type.closed_ref.` = case_when(doi == '10.1111/bjd.20626' ~ 0
                                              ,doi == '10.1186/s40168-018-0479-3' ~ 0
                                              ,.default = `otu_type.closed_ref.`)
          , `otu_type.de_novo.` = case_when(doi == '10.1111/bjd.20626' ~ 1
                                           ,doi == '10.1186/s40168-018-0479-3' ~ 1
                                           ,.default = `otu_type.de_novo.`)
          ) %>%
  mutate(., table1 = case_when(common_pipeline == 0 ~ 'not same processing'
                              ,taxa_profile == 0 ~ 'no taxonomy'
                              ,otus == 1 ~ 'OTUs'
                              ,otus == 2 ~ 'unclear'
                              ,asvs == 1 ~ 'ASVs'
                              ,asvs == 2 ~ 'unclear'
                              ,qual_filter == 1 ~ 'quality filtered'
                              ,qual_filter == 2 ~ 'unclear'
                              ,.default = 'check'
                              )
          , table2 = case_when(doi == '10.1186/s40168-018-0479-3' ~ 'de novo OTUs'
                              ,doi == '10.1371/journal.pcbi.1002863' ~ 'OTUs multi'
                              ,table1 != 'OTUs' ~ table1
                              ,(`otu_type.unclear.` == 1) | (`otu_type.not_described.`) ~ 'unclear OTUs'
                              ,(`otu_type.de_novo.` == 1) ~ 'de novo OTUs'
                              ,(`otu_type.open_ref.` == 1) ~ 'open reference OTUs'
                              ,(`otu_type.closed_ref.` == 1) ~ 'closed reference OTUs'
                              ,(otus == 1) ~ 'unclear OTUs'
                              ,.default = 'check'
                              )
          , table3 = case_when(table2 %in% c('ASVs', 'de novo OTUs') ~ 0
                              ,table2 == 'closed reference OTUs' ~ 1
                              ,.default = NA)
          ) %>%
  rename(., all_of(c(table_construction1 = 'table1'
                    ,otu_method = 'table2'
                    ,table_feature_level = 'table3')))
```

# Analyses performed

We wanted to know which analyses were performed in a given paper. The analyses
performed were **supposed** to be extracted only for combined data sets,
although this was sometimes a challenge.

* Which aspects of the microbiome were analyzed?

This had a select all result with possible responses

| Column | Response Text |
| :--- | :--- |
| `analyses_perf[descriptive]` | Descriptive analysis including taxonomy plots |
| `analyses_perf[alpha]` | Alpha diversity |
| `analyses_perf[beta]`  | Beta diversity  |
| `analyses_perf[differenital_abundance]` | Differential abundance  |
| `analyses_perf[sample_classification]` | Sample classification (i.e. random forest)  |
| `analyses_perf[core_microbiome]` | Core microbiome analysis (looking for a set of features common across all the samples) |
| `analyses_perf[co_occurance]`  | Descriptive analysis including taxonomy plots |
| `analyses_perf[other]` | Other |
| `analyses_perf_other`  | *other text* |

Each response triggered a question about the taxonomic level at which that
analysis was performed; in some cases, additional questions about the analyses
performed were also asked.

## Missing data

```{r, echo=FALSE}
data %>%
  column_to_rownames('doi') %>%
  select(starts_with('analyses_perf.')) %>%
  mutate_all(.funs=~!(. %in% c(0, 1))) %>%
  summarise(across(everything(), sum)) %>%
  rename_with(., .cols=starts_with('analyses_perf.')
               , .fn=~str_remove(str_remove(., 'analyses_perf.'), '\\.')
               ) %>%
  reactable()
```

So, we need to check the `analyses_perf.core_microbiome.` and 
`analyses_perf.other.` for coding.

```{r, echo=FALSE}
data %>%
  column_to_rownames('doi') %>%
  select(starts_with('analyses_perf')) %>%
  filter(., is.na(`analyses_perf.core_microbiome.`) | is.na(`analyses_perf.other.`)) %>%
  select(c(`analyses_perf.core_microbiome.`, `analyses_perf.other.`, analyses_perf_other)) %>%
  mutate_at(., .vars=vars(everything())
             , .funs=~ifelse(is.na(.), 'N/A', as.character(.))) %>%
  reactable()
```



So, looking at **10.1128/msystems.00138-20**, I think we can say that no other
analysis was perfromed. That leaves us with three more articles and checking
whether they did core microbiome analysis.

### 10.1186/s13059-022-02637-7

This paper focused on identifying common microbial features as a marker of 
inflammatory bowel disease, and other diseaase, leverage several data sets 
amplified with 515F-806R primers. It's less a core microbiome idea, and more an
index of unhealthy bacteria, so I could say "No" and code 0.

### 10.1111/bjd.20626

Looking at their data, this article only performed alpha diversity and beta
diversity analsyis in their "meta-analysis". So, I'd say no core microbiome 
for the data set combination.

### 10.3389/fcimb.2020.00129

This is a vaginal microbiome paper. Based on their methods

> Data reduction for biostatistical modeling was done in three different ways.
First, the Simpson diversity index (1-D) was calculated for each sample,
ranging from 0 (no diversity) to 1 (infinite diversity). Second, each ASV/OTU
was assigned to one of four “bacterial groups” based on the published
literature (Supplementary Material 1) as follows: (1) lactobacilli; 
(2) BV-anaerobes; (3) pathobionts; and (4) a rest group called “other bacteria”
(which contained mostly skin and Bifidobacteria). Pathobionts were defined
as all bacterial taxa that have been reported in the literature as having
been associated with invasive disease, and are not typically associated with
BV; we also included STI pathogens in this category because their mean relative
abundances were too low to justify a separate bacterial group. For each
sample, relative abundances of ASVs/OTUs belonging to the same bacterial
group were summed. This resulted in four relative abundances (one for each
bacterial group) per sample, which sum to one in total. For example, one
sample could contain 0.5 (50%) lactobacilli reads, 0.4 (40%) BV-anaerobes
reads, 0.08 (8%) pathobionts reads, and 0.02 (2%) other bacteria reads.
Third, we classified samples into nine VMB types (with each sample assigned
to only one VMB type): (1) Lactobacillus iners-dominated (Li; ≥75% relative
abundance of lactobacilli of which L. iners was the most common); 
(2) L. crispatus-dominated (Lcr; also ≥75% lactobacilli of which L. crispatus
was the most common); (3) dominated by other Lactobacillus species (Lo; 
also containing ≥75% lactobacilli); (4) lactobacilli and anaerobes (LA; 
≥25% lactobacilli with the remainder BV-anaerobes); (5) high diversity 
BV-anaerobes with ≥10% G. vaginalis presence (BV_GV); (6) high diversity
BV-anaerobes with <10% G. vaginalis presence (BV_noGV); 
(7) G. vaginalis-dominated (GV; G. vaginalis ≥50%); (8) substantial presence
of pathobionts (PB; ≥20% pathobiont taxa); and (9) Bifidobacterium-dominated
(BD; ≥50% Bifidobacteria).

So, this is closer to a community state type than a core microbiome.

## Other values

```{r, echo=FALSE}
data %>%
  column_to_rownames('doi') %>%
  select(starts_with('analyses_perf')) %>%
  filter(., (`analyses_perf.other.` == '1') | !is.na(analyses_perf_other)) %>%
  select(contains('other')) %>% 
  reactable()
```

So, I think we can leave this as is?

## Clean data

```{r, echo=FALSE}
cleaned[['analyses_performed']] <-
  data %>%
  select(c(doi, starts_with('analyses_perf'))) %>%
  mutate(., `analyses_perf.core_microbiome.` = ifelse(is.na(`analyses_perf.core_microbiome.`), '0', `analyses_perf.core_microbiome.`)
          , `analyses_perf.other.` = ifelse(is.na(`analyses_perf.other.`), '0', `analyses_perf.other.`)
          ) %>%
  mutate_at(., .vars=vars(starts_with('analyses_perf.'))
             , .funs=as.numeric)
```

# Descriptive Taxonomy

For descriptive taxonomy, we only have a question about the taxonomic level
wehre it was performed. So, let's tidy the data.

We asked,

* "Which taxonomic levels were used for descriptive analysis?"

With select all answers including

| Column | Response Text  |
| :--- | :--- |
| `taxa_level[otu_asv]` | OTU/ASV |
| `taxa_level[genus]` | Genus |
| `taxa_level[family]` | Family |
| `taxa_level[phylum]` | Phylum |
| `taxa_level[not_described]` | Not described |

We'll start by checking to see if there are any places where the values are
missing?

```{r, echo=FALSE}
data %>%
  column_to_rownames('doi') %>%
  select(c(`analyses_perf.descriptive.`, starts_with('taxa_level.'))) %>%
  mutate_at(., .vars=vars(starts_with('taxa_level.'))
             , .funs=~ifelse(`analyses_perf.descriptive.` == '0', '0', .)
             ) %>%
  mutate_at(., .vars=vars(starts_with('taxa_level.'))
             , .funs=~!(. %in% c('0', '1'))
            ) %>%
  select(starts_with('taxa_level.')) %>%
  summarise(across(everything(), sum)) %>%
  reactable()
```

So, as long as we say that the taxonomy is not defined, we we're fine?

```{r}
cleaned[['describe_taxa']] <-
  data %>%
  select(c(doi, `analyses_perf.descriptive.`, starts_with('taxa_level.'))) %>%
  mutate_at(., .vars=vars(starts_with('taxa_level.'))
             , .funs=~ifelse(`analyses_perf.descriptive.` == '0', '0', .)
             ) %>%
  mutate_at(., .vars=vars(starts_with('taxa_level.'))
             , .funs=as.numeric
             ) %>%
  select(doi, starts_with('taxa_level.'))
```

# Alpha Diversity : Taxonomy

Alpha diversity triggered two questions, about the taxonomic level where
analysis was performed, and then about what analytically techniques were used
to deal with study effects.

* "At which taxonomic levels was alpha diversity analyzed? (Faith's PD must be analyzed at the OTU/ASV level)"

with a select all options of 

| Column | Response Text |
|:----|:---|
| `alpha_level[otu_asv]` | OTU/ASV |
| `alpha_level[species]` | Species |
| `alpha_level[genus]` | Genus |
| `alpha_level[family]` | Family |
| `alpha_level[order]` | Family |
| `alpha_level[class]` | Class |
| `alpha_level[phylum]` | Phylum |
| `alpha_level[not_described]` | Not described |

We specifically want to make sure that we don't have missing values at any
of these levels; we need to make sure that the data is either defined or
not described, and we want to define the minimum taxonomic level where
analyses were performed.

## Missing data

```{r, echo=FALSE}
data %>%
  column_to_rownames('doi') %>%
  select(starts_with('alpha_level')) %>%
  mutate_all(., .funs=~!(. %in% c('0', '1'))) %>%
  summarise(across(everything(), sum)) %>%
  pivot_longer(everything()) %>%
  reactable()
```

So all the levels are defined. Yay!

### Not described

I'd also like to check if there's a not described where we also see the 
levels given.

```{r, echo=FALSE}
data %>%
  column_to_rownames('doi') %>%
  select(`analyses_perf.alpha.`, starts_with('alpha_level.')) %>%
  mutate_all(., .funs=as.numeric) %>%
  mutate(., alpha_def = (rowSums(select(., `alpha_level.otu_asv.`:`alpha_level.phylum.`)) > 0) * 1) %>%
  filter(., (`alpha_level.not_described.` == 1) & (alpha_def == 1)) %>%
  rename_with(., .fn=~str_remove(str_remove(., 'analysis_perf.'), '\\.')
               , .cols=starts_with('analysis_perf.')) %>%
  reactable()
```

#### 10.3389/fmicb.2022.843170

Looking at the methods,

> Since we analyzed different variable regions of the 16S rRNA gene, the results
of each data set were joined in a single feature table and then processed with
the Fragment-Insertion QIIME2 plug-in (Janssen et al., 2018) using the
reference database sepp-refs-gg-13-8 with default parameters. This plug-in is
based on SATé-Enabled Phylogenetic Placement (SEPP) and has been reported as
a solution when analyzing different variable regions of the 16S gene
(Janssen et al., 2018). The process resulted in a phylogenetic tree that was
used to filter the feature table for the subsequent analyses. All analyses
related to sequence data processing were performed in QIIME2 (v.2020-2).
Finally, all ASVs that match with mitochondria and chloroplast were removed
and the number of sequences was standardized by applying a rarefaction of 
7,000 (700 permutations) sequences per sample with R’s phyloseq package
(McMurdie and Holmes, 2013).
> 
> **Statistical Analyses**
> *Alpha Diversity Analysis*
> To compare the gut microbiome diversity of populations living different
lifestyles, we calculated the Phylogenetic diversity (PD) and Shannon-Wiener
indexes. The indexes were calculated with the “vegan” and “PhyloMeasures”
packages in R (Oksanen et al., 2008; Tsirogiannis and Sandel, 2016).
Statistical significant differences in alpha diversity between microbial
communities were calculated with the Wilcoxon rank-sum test using “vegan”
package in R (Oksanen et al., 2019).

And then there's a line later in the methods, where

> Finally, in order to corroborate the patterns found at the family level,
> all the diversity analyses described above were also carried out with the
> same parameters at the genus level (Supplementary Figures 2, 5–7).

When I read teh paper, they say that the **unweighted UniFrac distance** relying
on a Greengenes fragmetn insertion tree was calcualted at family level. I'm
not sure how you calculate unweighted UniFrac distance for the greengenes 13_8
tree at family level because the families in GG 13_8 are polyphyletic in some
cases. (GG 13_8 preceeds GTDB which wasn't released until 2018 and defination
wasn't on Daniel McDonald's radar when he was curated GG 13_5/GG 13_8). So, 
I think we should probably flag this as "not described" becuase the 
description doesn't make sense for the data being presented. 

```{r}
cleaned[['alpha_level']] <-
  data %>%
  select(c(doi, `analyses_perf.alpha.`, starts_with('alpha_l'))) %>%
  mutate(., `alpha_level.otu_asv.` = case_when(doi == '10.3389/fmicb.2022.843170' ~ '0'
                                              ,.default = `alpha_level.otu_asv.`)
          ) %>%
  mutate_at(., .vars=vars(`analyses_perf.alpha.`, starts_with('alpha_level.'))
             , .funs=as.numeric) %>%
  rowwise() %>%
  mutate(., alpha_level_min = max(`alpha_level.otu_asv.` * 8,
                                  `alpha_level.species.` * 7,
                                  `alpha_level.genus.` * 6,
                                  `alpha_level.family.` * 5,
                                  `alpha_level.order.` * 4,
                                  `alpha_level.class.` * 3,
                                  `alpha_level.phylum.` * 2,
                                  `alpha_level.not_described.` * 9)
          , alpha_level2 = case_when(alpha_level_min == 9 ~ NA
                                    ,`analyses_perf.alpha.` == 0 ~ NA
                                    ,.default=(alpha_level_min >= 8) * 1)
          , alpha_level_min = case_when(`analyses_perf.alpha.` == 0 ~ 'no alpha'
                                       ,alpha_level_min == 9 ~ 'not described'
                                       ,alpha_level_min == 8 ~ 'feature'
                                       ,alpha_level_min == 7 ~ 'species'
                                       ,alpha_level_min == 6 ~ 'genus')
            ) %>%
    select(-c(`analyses_perf.alpha.`))
```
```{r, echo=FALSE}
cleaned[['alpha_level']] %>% 
  group_by(alpha_level2, alpha_level_min) %>% 
  count() %>% 
  ungroup() %>% 
  reactable()
```

# Alpha diversity: Analyses perfromed

And then we have a list of the alpha diveristy analyses whcih were performed.

We asked this with,

* How did the authors handle differences between studies in their alpha diversity analyzes?

Which was a "select all" question with possible responses:

| Column  | Response Text |
|:---|:---|
| `alpha_study[pooled]` | Pooled all samples within a study |
| `alpha_study[no_adjustment]` | Not adjustment for study effects (i.e. kruskal wallis testing)  |
| `alpha_study[fixed]`  | Adjusted for study as a fixed term (linear regression)  |
| `alpha_study[random]` | Linear mixed effects model with study as a random effect  |
| `alpha_study[meta_analysis]` | A comparison of results across individual cohorts (i.e. forest plot, effect pooling) |
| `alpha_study[not_described]` | Not described |
| `alpha_study[other]`  | Other |
| `alpha_study_other` | *other text* |

Here, we want to be sure that the data is defined, and the answer is semi
logical (if we say there's an analysis described, there shouldn't be a value
for "not described".

### Missing levels

```{r, echo=FALSE}
data %>%
  column_to_rownames('doi') %>%
  select(starts_with('alpha_study.')) %>%
  mutate_all(., .funs=~!(. %in% c('0', '1'))) %>%
  summarise(across(everything(), sum)) %>%
  reactable()
```
So, we need to check the missing levels.

```{r, echo=FALSE}
data %>%
  column_to_rownames('doi') %>%
  select(starts_with('alpha_study.')) %>%
  filter(., (!(`alpha_study.meta_analysis.` %in% c('0', '1')) | 
             !(`alpha_study.not_described.` %in% c('0', '1')))
        ) %>%
  reactable()
```

So, we have one article, let's check the data.

#### 10.1111/bjd.20626

The study uses combined data set to highlight a difference from early stage
melenoma; the combination is shown in Figure 4. 

From the text:

> **Evidence of different gut microbiota composition between patients with metastatic and nonmetastatic melanoma: a meta‐analysis**
> 
> Finally, we compared our cohorts of patients with early‐stage melanoma against patients with metastatic melanoma from a previous study who did or did not respond to therapy.6 PCoA ordinations (Figure 4a) showed changes in community structure between tumour stages (from early to metastatic melanoma). We did not find a clear separation of samples between patients with early melanoma and healthy controls; on the contrary, samples of patients with metastatic melanoma (both responders and nonresponders) clustered separately.
> 
> Pairwise permanova analysis confirmed significant differences between the bacterial communities of patients with metastatic melanoma and both patients with early melanoma and healthy controls (Figure 4b). However, no significant differences were found between patients with early melanoma and controls. Intriguingly, bacterial richness (Figure 4c) and evenness (Figure 4d) showed a significant decline in community diversity in advanced metastatic melanoma vs. early melanoma. A similar trend was also observed in invasive melanoma with respect to in situ melanoma (Figure 3).

Based on the text and Figure 4, we only see an alpha and beta diveristy comaprison and there's not meta-analysis framework. It looks like it's a univariate comparison. So, I would flag this as not adjusted, and not a meta analysis, but described.

### Analyses not described and coded

Next, I want to check if there are any cases where the data is coded as not
having a described analysis and then having the analysis described.

```{r, echo=FALSE}
data %>% 
  select(doi, starts_with('alpha_study.')) %>%
  mutate(., `alpha_study.meta_analysis.` = ifelse(doi == '10.1111/bjd.20626', '0', `alpha_study.meta_analysis.`)
          , `alpha_study.not_described.` = ifelse(doi == '10.1111/bjd.20626', '0', `alpha_study.not_described.`)
          , `alpha_study.no_adjustment.` = ifelse(doi == '10.1111/bjd.20626', '1', `alpha_study.no_adjustment.`)
          ) %>%
  mutate_at(., .vars=vars(starts_with('alpha_study.'))
             , .funs=as.numeric) %>%
  mutate(., alpha_code = ((`alpha_study.pooled.` * 2) + 
                          (`alpha_study.no_adjustment.` * 4) + 
                          (`alpha_study.fixed.` * 8) + 
                          (`alpha_study.random.` * 16) + 
                          (`alpha_study.meta_analysis.` * 32) + 
                          (`alpha_study.other.`) * 64)
          ) %>%
  filter(., (alpha_code > 0) & (`alpha_study.not_described.` == 1)) %>%
  select(-c(alpha_code)) %>%
  rename_with(., .fn=~str_remove(str_remove(., 'alpha_study.'), '\\.')
               , .cols=starts_with('alpha_study.')
              ) %>%
  reactable()
  
```

So, let's check the two papers?

#### 10.1136/gutjnl-2015-310376

Let's look at the statistical analysis of the methods.

> **Statistical analysis**
> 
> In each cohort, differentially abundant taxa in the gut microbiome between 
PPI users and non-PPI users were analysed using the multivariate statistical 
framework MaAsLin.17 MaAsLin performs boosted, additive, general linear models
between metadata and microbial abundance data. After running the association
studies in the individual cohorts, we performed a meta-analysis of the three
cohorts, using the weighted Z-score method. The Cochran's Q test was used to
check for heterogeneity. The significance cut-off for the Cochran's Q test was
determined by Bonferroni correction for the 92 significant results: p<5.43×10−4.
Differences in richness (the number of species within a sample), principal 
coordinate analyses (PCoA) and Shannon diversity analysis were determined using
the QIIME microbiome analysis software.18 The Wilcoxon test and Spearman's
correlations were used to identify differences in Shannon's diversity and 
relations between the PCoA scores of PPI users and non-PPI users, while the
χ2 test, Fisher's exact test, Spearman's correlation and Wilcoxon-Mann-Whitney 
test (WMW test) were used to determine differences in age, gender, BMI, 
antibiotics use, and gut complaints between PPI users and non-users. In all 
the microbiome analyses, multiple test corrections were based on the false 
discovery rate (FDR). An FDR value of 0.05 was used as a statistically 
significant cut-off.

So, this paper meta-analyzes the MaAslin results, but the main text doesn't 
explicitly describe the methodology. So, let's go back to the supplemental 
methods, where we find...

> Both alpha and beta diversity were calculated in a single rarefaction of the
abundance table, setting the read depth at 10,000 reads. Differences in 
diversity, PCoA scores and other factors associated to PPI differences were 
performed using the Wilcoxon test, the Spearman correlation, Fischer exact 
test and the chi‐squared test as implemented in R 10. In the PCoA analysis, 
we compared the PPI group to the non‐PPI group in terms of their scores on the
first three components using a Wilcoxon test. After the test we compared the 
group averages to determine the direction of the shift.
 
And then from the meta-analysis section:

> **Meta‐analysis**
> The individual analyses were combined in a meta‐analysis. For the 
meta‐analysis a weighted Z‐score approach was used. The individual P‐values
derived from the tests were transformed to Z‐scores and the direction was taken
from the coefficient of the test. The Z‐ scores were weighted according to the
sample size and combined in a meta Z‐score, this was subsequently transformed
in a meta P‐value.

However, it's not clear to me based on this methods section whether the 
meta-analysis was only applied to the MaAslin data. 

But, looking at the methods, main text figure, and supplemental figure, I could 
guess, but can't confirm, that they did not do a meta-analysis. it looks like a
single study coding. But, I think I'll code as "not described".

#### 10.1016/j.csbj.2020.08.028

We're looking for a value which is "no adjustment" and "not described". Looking
at the methods, we have

> **2.4 Statistical analysis**
> QIIME 2 and SPSS software (www.ibm.com/software/it/analytics/spss/) were used
to compute statistical analyses. PERMANOVA analyses were performed using 1000 
permutations to estimate p-values for differences among populations in PCoA 
analyses. Furthermore, differential abundance of bacterial genera and 
alpha-diversity was tested by ANOVA analysis. Moreover, we also calculated 
the post hoc analysis LSD (least significant difference) for multiple 
comparison.

Looking through the paper, we don't see any adjustment. So, I think we can
assume that its' unadjusted.

### Adjust and dont adjust

It feels like ignoring study effects should not be possible with a meta-analysis
so I want to see how often this happens and make sure its coded correctly.

```{r, echo=FALSE}
  data %>%
  filter(., `analyses_perf.alpha.` == '1') %>% 
  select(c(doi, starts_with('alpha_study'))) %>%
  mutate(.,  `alpha_study.meta_analysis.` = case_when(doi == '10.1136/gutjnl-2015-310376' ~ '0'
                                                     ,doi == '10.1111/bjd.20626' ~ '0'
                                                     ,.default =  `alpha_study.meta_analysis.`)
          , `alpha_study.not_described.` = case_when(doi == '10.1016/j.csbj.2020.08.028' ~ '0'
                                                    ,doi == '10.1111/bjd.20626' ~ '0'
                                                    ,.default = `alpha_study.not_described.`)
          , `alpha_study.no_adjustment.` = case_when(doi == '10.1111/bjd.20626' ~ '1'
                                                    ,.default = `alpha_study.no_adjustment.`)
          , alpha_study_other = ifelse(`alpha_study.other.` == '0', NA, alpha_study_other)
          ) %>%
  mutate_at(., .vars=vars(starts_with('alpha_study.'))
             , .funs=as.numeric) %>%
  rename_with(., .cols = starts_with('alpha_study.')
               , .fn=~str_remove(str_remove(., 'alpha_study.'), '\\.')
               ) %>%
  mutate(., adjust = ((fixed + random + meta_analysis) > 0) * 1) %>%
  filter(., (adjust == 1) & (no_adjustment == 1)) %>%
  reactable()
```
#### Article check

We have one article we need to check.

#### 10.3389/fcimb.2022.816526

According to the methods section, 

> Alpha and beta diversity metrics were evaluated by QIIME2 (Fung et al., 2021).
In identifying T2DM versus healthy controls, the Wilcoxon rank sum test was 
used to determine statistical differences between groups, considering that 
there were only two groups which did not follow a normal distribution. 

Which would be no adjustment. I'm also going to look in the results. 

The results opens with a discussion of the study effects looking at taxonomy
and beta diversity, but alpha effects are not discussed, and there is no 
mention of supplemental materials.

And then the discussion of alpha diversity shows only a difference between 
cases and controls across metrics (Table 2). 

So, very much not a meta-analysis.

### Other data 

Finally, I'd like to see if we classify and correct the alpha diversity "other"
analyses. 

```{r, echo=FALSE}
data %>%
  filter(., `analyses_perf.alpha.` == '1') %>% 
  select(c(doi, `envo.other.`, starts_with('alpha_study'))) %>%
  mutate(.,  `alpha_study.meta_analysis.` = case_when(doi == '10.1136/gutjnl-2015-310376' ~ '0'
                                                     ,doi == '10.3389/fcimb.2022.816526' ~ '0'
                                                     ,doi == '10.1111/bjd.20626' ~ '0'
                                                     ,.default =  `alpha_study.meta_analysis.`)
          , `alpha_study.not_described.` = case_when(doi == '10.1016/j.csbj.2020.08.028' ~ '0'
                                                    ,doi == '10.1111/bjd.20626' ~ '0'
                                                    ,.default = `alpha_study.not_described.`)
          , `alpha_study.no_adjustment.` = case_when(doi == '10.1111/bjd.20626' ~ '1'
                                                    ,.default = `alpha_study.no_adjustment.`)
          , alpha_study_other = ifelse(`alpha_study.other.` == '0', NA, alpha_study_other)
          ) %>%
  mutate_at(., .vars=vars(starts_with('alpha_study.'))
             , .funs=as.numeric) %>%
  rename_with(., .cols = starts_with('alpha_study.')
               , .fn=~str_remove(str_remove(., 'alpha_study.'), '\\.')
               ) %>%
  filter(., !is.na(alpha_study_other) | other == 1) %>%
  filter(., `envo.other.` == '0') %>%
  select(-c(`envo.other.`)) %>%
  select(doi, no_adjustment, meta_analysis, other, alpha_study_other) %>%
  reactable()
```

So, let's skip over the data where we have "other" environments, because we'll
treat these as multi-enviroment studies and not descibe the analysis.

So, let's look at the remainder.

#### Article check

We have one article we need to check.

#### 10.1111/apt.15375

This lists the analysis as "parallel" analysis.

The article combined data from two data sets collected at two different sites.
Analytically, they analyzed the data in parallel, rather than combining it. So,
the analysis description here, is accurate, and maybe we should add a coding 
for that?

#### 10.1007/s00248-018-1176-2

This paper did a primary analysis wtih a specific cohort, and then combined data
to check a specific association with *Lactobacillus* in a combined analysis. 
There is no alpha diveristy analysis in the combined data. So, I think I would
code this as saying the alpha diveristy is on a single study?

#### 10.3389/fimmu.2021.722206

This article looks at the association between Type 2 diabetes and the microbiome,
using a standard meta analytical framework (PRISMA search, etc). 

From the methods section, 

> **The α diversity indexes, bacterial richness (observed OTUs), Shannon index, 
and evenness (J) were calculated based on OTU tables of each study. 
Significance tests between T2DM patients and healthy controls were conducted by
the Wilcoxon test method.** Differences in community structure across samples 
(β diversity) were visualized by principal coordinates analysis (PCoA) plots 
based on Bray-Curtis distance. Significance tests were determined using 
permutational multivariate analysis of variance (PERMANOVA) with 104 
permutations in vegan (52). **Meta-analysis of bacterial alpha diversity indexes
and microbial taxa among the 7 studies was performed to determine the 
consistency using both the random effects (RE) model and fixed effects (FE) 
model in the metafor package (53). Generally, we calculated the odd ratios 
(ORs) of these metrics by assigning any value above the median of the metric 
within the study as positive.**

Looking at the data (Figure 2), I think we can conclusively say they do a meta-
analysis. (The supplement shows a by-study analysis as well). So, I think we 
can stick with te meta-analsysi coding here?

#### 10.1016/j.tube.2018.02.006

This looks at the lung microbiome in TB. It's coded as "study-specific 
differences" where they show a study effect (boxplot) and a univariate 
comparison. From the results:

> Since it appeared that a diverse composition of the lung microbiome from 
different studies might exist (Fig. 1), we compared the microbiome diversity 
across all studies, and found no statistical difference (Fig. 2A). We then 
compared Shannon diversity between TB cases and controls from all studies and 
found no statistical differences (Fig. 2B).

So, I guess this is a study effect check? 

### 10.1186/s40168-017-0248-8

The paper analyzed heterogeneity across studies (Figure 5, Figure 6) and found
none, and then they didn't adjust. So, I think it's the same as 
`10.1016/j.tube.2018.02.006`: study effect check and then analyze.

### 10.1186/s40168-017-0368-1

This study was a combination of multiple studies looking at CDI. From the 
"meta-analysis" section fo the results:

> **Meta-analysis**
> 
> We combined our high-throughput 16S rRNA gene sequence data with a recent 
study by Khanna et al. [22]. Although both studies shared a common experimental
approach, results revealed a strong study effect as the most clearly 
discernible signal in the data (PERMANOVA, p value = 0.002). The random forest 
trained to classify which samples come from which study had an error rate of 
about 2% with AUC of 0.98 (Additional file 3). This resilient study-level effect
was consistent, even when we included only shared OTUs between two studies. We 
then generated predictive models using our dataset with truncated sequences 
(200 bp, leave-one-out cross-validation), and the results showed a performance 
reduction at all taxonomic levels compared to our original dataset with 
sequence read lengths of 250 bp (Table 2). We also constructed three separate 
random forest (RF) classifiers of CDI recurrence using the Khanna et al. [22] 
dataset. Members of Veillonellaceae family were ranked first for all 
constructed models, albeit with no statistically significant discriminatory 
powers (Table 2). Finally, when we trained on our data and used the Khanna et 
al. [22] for cross-validation, the error rate was 0.29, and vice versa, the 
error rate was 0.32; none of these RF models were significant.

So, it looks like the alpha diveristy was done as part of a single study, and 
not across multiple studies?

### Study design codes

Finally,  I want to look at the alpha diveristy values and see if we can
code the data sets consistently.

```{r, echo=FALSE}
data %>%
  select(c(doi, `analyses_perf.alpha.`, starts_with('envo.'), starts_with('alpha_study'))) %>%
  mutate(.,  `alpha_study.meta_analysis.` = case_when(doi == '10.1136/gutjnl-2015-310376' ~ '0'
                                                     ,doi == '10.3389/fcimb.2022.816526' ~ '0'
                                                     ,doi == '10.1111/bjd.20626' ~ '0'
                                                     ,.default =  `alpha_study.meta_analysis.`)
          , `alpha_study.not_described.` = case_when(doi == '10.1016/j.csbj.2020.08.028' ~ '0'
                                                    ,doi == '10.1111/bjd.20626' ~ '0'
                                                    ,.default = `alpha_study.not_described.`)
          , `alpha_study.no_adjustment.` = case_when(doi == '10.1111/bjd.20626' ~ '1'
                                                    ,.default = `alpha_study.no_adjustment.`)
          , `alpha_study.parallel.` = case_when(alpha_study_other == 'Parallel analyses' ~ '1'
                                               ,.default = '0')
          , `alpha_study.check.` = case_when(doi == '10.1016/j.tube.2018.02.006' ~ '1'
                                            ,doi == '10.1016/j.tube.2018.02.006' ~ '1'
                                            ,.default = '0')
          , `alpha_study.only.` = case_when(doi == '10.1007/s00248-018-1176-2' ~ '1'
                                           ,doi == '10.1186/s40168-017-0368-1' ~ '1'
                                           ,.default = '0')
          , alpha_study_other = ifelse(`alpha_study.other.` == '0', NA, alpha_study_other)
          ) %>%
  mutate_at(., .vars=vars(`analyses_perf.alpha.`, starts_with('envo.'), 
                          starts_with('alpha_study.'))
             , .funs=as.numeric) %>%
  mutate(., envo = case_when((`envo.other.` == 1) ~ 'pan envo'
                            ,(`envo.host.` == 1) ~ 'human host'
                            ,.default = 'human only')
          ) %>%
  select(-c(starts_with('envo.'), alpha_study_other, `alpha_study.other.`)) %>%
  rename_with(., .fn = ~str_remove(str_remove(., 'alpha_study.'), '\\.')
               , .cols=starts_with('alpha_study.')
               ) %>%
  mutate(., alpha_num = ((pooled * 1) + 
                          (no_adjustment * 1) + 
                          (fixed * 1) + 
                          (random * 1) + 
                          (meta_analysis * 1) + 
                          (not_described * 1) + 
                          (parallel * 1) + 
                          (check * 1) + 
                          (only * 1))
          , alpha_num = case_when(envo == 'pan envo' ~ 0
                                  ,.default = alpha_num
                                  )
          ) %>%
  group_by(alpha_num) %>% count() %>% reactable()
```
We have three cases where the code indicates multiple responses. Let's pull
those out?

```{r, echo=FALSE}
data %>%
  select(c(doi, `analyses_perf.alpha.`, starts_with('envo.'), starts_with('alpha_study'))) %>%
  mutate(.,  `alpha_study.meta_analysis.` = case_when(doi == '10.1136/gutjnl-2015-310376' ~ '0'
                                                     ,doi == '10.3389/fcimb.2022.816526' ~ '0'
                                                     ,doi == '10.1111/bjd.20626' ~ '0'
                                                     ,.default =  `alpha_study.meta_analysis.`)
          , `alpha_study.not_described.` = case_when(doi == '10.1016/j.csbj.2020.08.028' ~ '0'
                                                    ,doi == '10.1111/bjd.20626' ~ '0'
                                                    ,.default = `alpha_study.not_described.`)
          , `alpha_study.no_adjustment.` = case_when(doi == '10.1111/bjd.20626' ~ '1'
                                                    ,.default = `alpha_study.no_adjustment.`)
          , `alpha_study.parallel.` = case_when(alpha_study_other == 'Parallel analyses' ~ '1'
                                               ,.default = '0')
          , `alpha_study.check.` = case_when(doi == '10.1016/j.tube.2018.02.006' ~ '1'
                                            ,doi == '10.1016/j.tube.2018.02.006' ~ '1'
                                            ,.default = '0')
          , `alpha_study.only.` = case_when(doi == '10.1007/s00248-018-1176-2' ~ '1'
                                           ,doi == '10.1186/s40168-017-0368-1' ~ '1'
                                           ,.default = '0')
          , alpha_study_other = ifelse(`alpha_study.other.` == '0', NA, alpha_study_other)
          ) %>%
  mutate_at(., .vars=vars(`analyses_perf.alpha.`, starts_with('envo.'), 
                          starts_with('alpha_study.'))
             , .funs=as.numeric) %>%
  mutate(., envo = case_when((`envo.other.` == 1) ~ 'pan envo'
                            ,(`envo.host.` == 1) ~ 'human host'
                            ,.default = 'human only')
          ) %>%
  select(-c(starts_with('envo.'), alpha_study_other, `alpha_study.other.`)) %>%
  rename_with(., .fn = ~str_remove(str_remove(., 'alpha_study.'), '\\.')
               , .cols=starts_with('alpha_study.')
               ) %>%
  mutate(., alpha_num = ((pooled * 1) + 
                          (no_adjustment * 1) + 
                          (fixed * 1) + 
                          (random * 1) + 
                          (meta_analysis * 1) + 
                          (not_described * 1) + 
                          (parallel * 1) + 
                          (check * 1) + 
                          (only * 1))
          , alpha_num = case_when(envo == 'pan envo' ~ 0
                                  ,.default = alpha_num
                                  )
          ) %>%
  filter(., alpha_num > 1) %>%
  reactable()
```

So, I'd like to check all of these?


#### 10.1093/femsec/fix153

This study looked at the relationship between the micorbiome and three gastric
diseases (crohn's disease, ulcerative colitis, colorectal cancer, and 
clostridium dificile infection). The authors did an *in silico* primer pair 
evaluation and checked for study effects. So, the "pooled" effect is a 
study-specific rarefaction curve. So, I think I'd code this more closely with 
the "check study effect".

#### 10.1093/cid/ciz258

This is a straight up meta analysis and random-effects analysis and correctly
coded.

> Four measures of α-diversity were calculated using QIIME. Observed species 
reports the total species observed and reflects sample richness [47]. Chao 1 
also reflects richness, particularly in settings with many low-abundance 
classes [48]. It may not perform well in settings with low or different 
sequencing depths.[49]. In contrast, the Shannon index [50] and the inverse 
Simpson index [51] estimate both richness and evenness (Supplementary Table 3).
> 
> Generalized linear mixed models were used to relate raw and log-transformed 
α-diversity measures to clinical metadata. For all models, a random intercept 
for each study was included to account for study-specific variations in 
α-diversity. All P values were corrected for multiple comparisons through false 
discovery rate (FDR) across the 4 α-diversity measurements. Results in 
stratified analysis were validated with a nonparametric test (Wilcoxon rank 
sum). For regression coefficients to be comparable across α-diversity 
measurements, we divided the regression coefficients by their corresponding 
pooled standard deviation across all studies to generate stβ, which estimated 
effect size. To explore study-related heterogeneity, boxplots were generated 
using the ggplot2 package in R (version 3.3.0), after transformation of the 
α-diversity values through mean centering to zero (within each study) and 
scaling to unit variance. We additionally constructed Forest plots (requiring 
at least 5 patients/categories), using Hedge’s G statistic, and calculated I2 
as a measure of heterogeneity for each subanalysis.

#### 10.1093/braincomms/fcab113	

From the methods section:

> Next, for each disease (Parkinson’s disease and MS), we performed a meta-
analysis of richness and evenness. Linear regressions were fitted to estimate 
standardized mean differences (SMD) for each diversity measure, with or without 
adjustment for cofounding factors (age and sex). Then, overall estimates were 
estimated by doing a weighting inverse variance meta-analysis with a random 
model. The generic function metagen from the R package meta (version 4.12) was 
used. The overall estimate and its 95% confidence interval were reported, in 
addition to P-value calculated based on the t distribution. P-values were 
adjusted using Benjamini–Hochberg correction. For Parkinson’s disease studies, 
adjustment for confounding factors was limited to age and sex as no other 
clinical variable was available. For MS, clinical data were available for 
only one study, hence no adjustment was possible.

There doesn't appear ot be a fixed effects adjustment, other than confounders
based on either the methods or the data. 

## Tidy Data

```{r}
cleaned[['alpha_study']] <-
  data %>%
  select(c(doi, `analyses_perf.alpha.`, starts_with('alpha_s'))) %>%
  mutate(.,  `alpha_study.meta_analysis.` = case_when(doi == '10.1136/gutjnl-2015-310376' ~ '0'
                                                     ,doi == '10.3389/fcimb.2022.816526' ~ '0'
                                                     ,doi == '10.1111/bjd.20626' ~ '0'
                                                     ,.default =  `alpha_study.meta_analysis.`)
          , `alpha_study.not_described.` = case_when(doi == '10.1016/j.csbj.2020.08.028' ~ '0'
                                                    ,doi == '10.1111/bjd.20626' ~ '0'
                                                    ,.default = `alpha_study.not_described.`)
          , `alpha_study.no_adjustment.` = case_when(doi == '10.1111/bjd.20626' ~ '1'
                                                    ,doi == '10.1093/femsec/fix153' ~ '0'
                                                    ,.default = `alpha_study.no_adjustment.`)
          , `alpha_study.fixed.` = case_when(doi == '10.1093/braincomms/fcab113' ~ '0'
                                            ,.default = `alpha_study.fixed.`)
          , `alpha_study.pooled.` = case_when(doi == '10.1093/femsec/fix153' ~ '0'
                                             ,.default = `alpha_study.pooled.`)
          , `alpha_study.parallel.` = case_when(alpha_study_other == 'Parallel analyses' ~ '1'
                                               ,.default = '0')
          , `alpha_study.check.` = case_when(doi == '10.1016/j.tube.2018.02.006' ~ '1'
                                            ,doi == '10.1016/j.tube.2018.02.006' ~ '1'
                                            ,doi == '10.1093/femsec/fix153' ~ '1'
                                            ,.default = '0')
          , `alpha_study.only.` = case_when(doi == '10.1007/s00248-018-1176-2' ~ '1'
                                           ,doi == '10.1186/s40168-017-0368-1' ~ '1'
                                           ,.default = '0')
          , alpha_study_other = ifelse(`alpha_study.other.` == '0', NA, alpha_study_other)
          ) %>%
  mutate_at(., .vars=vars(`analyses_perf.alpha.`, starts_with('alpha_study.'))
             , .funs=as.numeric) %>%
  mutate(., alpha_study_code = ((`alpha_study.pooled.` * 2) +
                                (`alpha_study.no_adjustment.` * 4) +
                                (`alpha_study.fixed.` * 8) + 
                                (`alpha_study.random.` * 16) + 
                                (`alpha_study.meta_analysis.` * 32) + 
                                (`alpha_study.not_described.` * 64) + 
                                (`alpha_study.parallel.` * 128) + 
                                (`alpha_study.check.` * 256) + 
                                (`alpha_study.only.` * 512))
          , alpha_study_code = ifelse(`analyses_perf.alpha.` == 0, 0, alpha_study_code)
          , alpha_study_label = case_when(alpha_study_code == 0 ~ 'no alpha'
                                         ,alpha_study_code == 2 ~ 'all samples pooled for study'
                                         ,alpha_study_code == 4 ~ 'no adjustment for dataset'
                                         ,alpha_study_code == 8 ~ 'dataset as fixed effect'
                                         ,alpha_study_code == 16 ~ 'dataset as random effect'
                                         ,alpha_study_code == 32 ~ 'statistical meta analysis of dataset'
                                         ,alpha_study_code == 48 ~ 'dataset as random effect and statistical meta analysis of dataset'
                                         ,alpha_study_code == 64 ~ 'not described'
                                         ,alpha_study_code == 128 ~ 'parallel analysis by dataset'
                                         ,alpha_study_code == 256 ~ 'check for study effects and ignore if not significant'
                                         ,alpha_study_code == 512 ~ 'analyzed alpha for one study only'
                                         )
          , alpha_study_cat = case_when(alpha_study_code == 0 ~ 'F.no alpha'
                                       ,alpha_study_code == 512 ~ 'F. no alpha'
                                       ,alpha_study_code %in% c(2, 256) ~ 'B.acknoweldge dataset'
                                       ,alpha_study_code == 4 ~ 'A.ignore study'
                                       ,alpha_study_code %in% c(8, 16, 32, 48)  ~ 'C.adjust for data set'
                                       ,alpha_study_code == 64 ~ 'E.not described'
                                       ,alpha_study_code == 128 ~ 'D.other'
                                       )
          ) %>%
    select(-c(`analyses_perf.alpha.`))
```
```{r, echo=FALSE}
cleaned[['alpha_study']] %>% 
  group_by(alpha_study_cat, alpha_study_label) %>% 
  count() %>% 
  ungroup() %>% 
  reactable()
```

# Beta Diveristy - Taxonomy

Beta diversity analysis triggered two questions, one about what level the
taxonomic analysis was performed at, and then one about how multiple studies
were analyzed.

## Taxonomic Level

The question was

* At which taxonomic levels was beta diversity analyzed? (UniFrac distance must be analyzed at the OTU/ASV level) 

Where it was a select all with options:

| Column | Response Text |
| :--- | :--- |
| `beta_level[otu_asv]` | OTU/ASV |
| `beta_level[species]` | Species |
| `beta_level[genus]` | Genus |
| `beta_level[family]` | Family |
| `beta_level[order]` | Family |
| `beta_level[class]` | Class |
| `beta_level[phylum]` | Phylum |
| `beta_level[not_described]` | Not described |

We want to find the lowest taxonomic level for the OTU clustering?

### Missing levels

We'll start by checking where there are missing levels for the taxonomic
assignment. So, we're missing columsn for features, species, gneus, 
and not described.

```{r, echo=FALSE}
data %>%
  column_to_rownames('doi') %>%
  select(starts_with('beta_level.')) %>%
  mutate_at(., .vars=vars(starts_with('beta_level.'))
             , .funs=~!(. %in% c('0', '1'))
             ) %>%
 summarise(across(everything(), sum)) %>%
  reactable()
```
So, lets go look for the samples?

```{r, echo=FALSE}
data %>%
  column_to_rownames('doi') %>%
  select(starts_with('beta_level.')) %>%
  mutate(., miss_any = (!(`beta_level.otu_asv.` %in% c('0', '1')) | 
                        !(`beta_level.species.` %in% c('0', '1')) | 
                        !(`beta_level.genus.` %in% c('0', '1')) | 
                        !(`beta_level.not_described.` %in% c('0', '1')))
         ) %>%
  filter(., miss_any) %>%
  select(-c(miss_any)) %>%
  rename_with(., .fn=~str_remove(str_remove(., 'beta_level.'), '\\.')) %>%
  reactable()
```

So, looking at the three papers...

#### 10.1111/1462-2920.13632

Looking at their methods of table construction:

>  Sequence processing and quality control
> 
> All raw data were re-analysed following a published pipeline from a 
previous meta-analysis (Waite et al., 2014) using mothur version 1.37.0 
(Schloss et al., 2009). Details regarding bioinformatic processing can be 
found in the Supporting Information Methods. In order to compare data between
studies that target different regions of the bacterial 16S rRNA gene,
genus-level phylotypes were constructed using sequence classification. 
Phylotypes are analogous to OTUs, however in phylotype analysis each sequence 
is compared with a reference database and binned into a group based on its 
similarity to the database. As part of phylotype generation, only one group 
is assigned to each taxon. In this study, quality-filtered sequences were 
classified against the Silva database (version 123) using the naive Bayesian 
method (Quast et al., 2013; Wang et al., 2007). Sequences that could not 
align or were chimeric, as well as sequences that could not be classified to 
at least Domain level, or assigned to mitochondria, chloroplasts or 
Eukaryota, were removed.
>
> The phylotype table was rarefied to a depth of 1000 data points per sample, 
then collapsed by individual subjects using median values for each phylotype, 
when multiple samples per subject were present in the data set. Median counts
were used to capture the most central phylotype count and therefore minimize
skewing data due to outliers

So, we have the fact that phylotypes are essentially ASVs. I think based on the 
context in the rest of the methods, we sort of have to assume that the 
phylotypes were also used for Bray Curtis dissimilarity as well as LefSe.

Additionally, from the results we get, 

> Imaging the spread of data in a two-dimensional, non-metric multidimensional 
scaling (nMDS) plot revealed a high degree of overlap in the clustering of CRS
and healthy subjects (Fig. 2A). Although distinct clusters based on disease 
status were not evident, the multivariate homogeneity of groups dispersions 
with Tukey's honest significant difference, or the distance of samples from 
the centroid of each group, revealed a significantly increased dispersion of 
CRS samples versus healthy samples (p=0.002) (Fig. 2B). Additionally, 
similarity percentage (SIMPER) analysis showed similarities between CRS and 
healthy samples based on phylotype abundances were generally very low (SIMPER 
average dissimilarity=78%). The similarity within healthy subjects (SIMPER 
average similarity=26%) was greater than within CRS patients (SIMPER 
average similarity=21%) (Supporting Information Table S6).

So, I think it's reasonable to assume that beta diversity analsyes were 
conducted at the phylotype level.

#### 10.3389/fmicb.2021.711134

We're not sure if the data was analyzed at feature level or not. This is 
another case where they describe their features as "phylotypes". 

As I look at their description, they say, 

> The wide range of sequenced 16S rRNA regions (Supplementary File A and 
Supplementary Figure E1) necessitated a genus-level phylotype approach 
(Waite and Taylor, 2014; Callahan et al., 2017). 

So, they probably collapsed everything to genus level. There is no other
explicit language, so I'm going to assume they didn't work at feature level.

#### 10.1111/bjd.20626

The text says they performed OTU clustering for the methods. However, the
additional analytical description that's promised in the supplementary
material is not apparent. The text doesn't make it clear anywhere which level
the analsyis was conducted at, but the methods also don't mention collapsing
anywhere.

### Not described

I also want to make sure we dont have any cases where the data is flagged 
as "not described" but also defined.

```{r, echo=FALSE}
data %>%
  select(doi, starts_with('beta_level.')) %>%
  mutate(., `beta_level.not_described.` = case_when(doi == '10.1111/1462-2920.13632' ~ '0'
                                                   ,.default =  `beta_level.not_described.`)
          , `beta_level.otu_asv.` = case_when(doi == '10.1111/bjd.20626' ~ '1'
                                              ,doi == '10.3389/fmicb.2021.711134' ~ '0'
                                              ,.default=`beta_level.otu_asv.`)
          , `beta_level.species.` = case_when(doi == '10.1111/bjd.20626' ~ '0'
                                              ,.default=`beta_level.species.`)
          , `beta_level.genus.` = case_when(doi == '10.1111/bjd.20626' ~ '0'
                                              ,.default=`beta_level.genus.`)
         ) %>%
  mutate_at(., .vars=vars(starts_with('beta_level.'))
             , .funs=as.numeric) %>%
  rowwise() %>%
  mutate(., beta_level_def = sum(select(., `beta_level.otu_asv.`:`beta_level.phylum.`))
          ) %>%
  ungroup() %>%
  filter(., (`beta_level.not_described.` == 1) & (beta_level_def == 1)) %>%
  reactable()
```
So, where the data is supplied, the data is supplied.

## Tidy Data

```{r}
cleaned[['beta_level']] <-
  data %>%
  select(doi, `analyses_perf.beta.`, starts_with('beta_level.')) %>%
  mutate(., `beta_level.not_described.` = case_when(doi == '10.1111/1462-2920.13632' ~ '0'
                                                   ,.default =  `beta_level.not_described.`)
          , `beta_level.otu_asv.` = case_when(doi == '10.1111/bjd.20626' ~ '1'
                                              ,doi == '10.3389/fmicb.2021.711134' ~ '0'
                                              ,.default=`beta_level.otu_asv.`)
          , `beta_level.species.` = case_when(doi == '10.1111/bjd.20626' ~ '0'
                                              ,.default=`beta_level.species.`)
          , `beta_level.genus.` = case_when(doi == '10.1111/bjd.20626' ~ '0'
                                              ,.default=`beta_level.genus.`)
         ) %>%
  mutate_at(., .vars=vars(contains('beta.'), starts_with('beta_level.'))
             , .funs=as.numeric) %>%
  rowwise() %>%
  mutate(., beta_level_min = max((`beta_level.otu_asv.` * 8),
                                 (`beta_level.species.` * 7),
                                 (`beta_level.genus.` * 6),
                                 (`beta_level.family.` * 5),
                                 (`beta_level.order.` * 3),
                                 (`beta_level.class.` * 2),
                                 (`beta_level.phylum.` * 1),
                                 (`beta_level.not_described.` * 9))
          ) %>%
  ungroup() %>%
  mutate(.,beta_level2 = case_when(`analyses_perf.beta.` == 0 ~ NA
                                   ,beta_level_min %in% c(0, 9) ~ NA
                                   ,.default = (beta_level_min < 8) * 1
                                   )
         , beta_level_min = case_when(`analyses_perf.beta.` == 0 ~ 'no beta'
                                      ,beta_level_min == 9 ~ 'no described'
                                      ,beta_level_min == 8 ~ 'feature'
                                      ,beta_level_min == 7 ~ 'species'
                                      ,beta_level_min == 6 ~ 'genus'
                                      ,beta_level_min == 5 ~ 'family'
                                      ,beta_level_min == 4 ~ 'order'
                                      ,beta_level_min == 3 ~ 'class'
                                      ,beta_level_min == 2 ~ 'phylum'
                                      ,.default = 'no described'
                                      )
          ) %>%
  select(doi, starts_with('beta'))
  
```
```{r, echo = FALSE}
cleaned[['beta_level']] %>%
  group_by(beta_level2, beta_level_min) %>%
  count() %>%
  ungroup() %>%
  reactable()
```
# Beta - Analyses performed

And then we asked about the beta diveristy analyses which were performed. It's 
a bit of a messy question. 

So, we asked

* How did beta diversity analysis handle study effects? 

With possible select all options of

| Column | Response Text |
| :--- | :--- |
| `beta_study[no_study_pcoa]` | Descriptive PCoA that did not show study effects |
| `beta_study[pcoa_study_effects]` | PCoA showing study effects |
| `beta_study[adonis_adj]` | Adonis permanova showing or adjusting for study effect size |
| `beta_study[other_test_adj]` | Other test adjusted for study effect |
| `beta_study[no_adj]` | Not adjustment or acknowledgment of study effect |
| `beta_study[other]` | Other |
| `beta_study[not_described]` | Not described |
| `beta_study_other` | *other free text* |

## Missing data

```{r, echo=FALSE}
data %>%
  select(starts_with('beta_study.')) %>%
  mutate_at(., .vars=vars(starts_with('beta_study.'))
             , .funs=~!(. %in% c('0', '1'))
            ) %>%
  summarise(across(everything(), sum)) %>%
  reactable()
```

We're missing values for the PCoA study effect, adjusted Adonis, and
other category.

```{r, echo=FALSE}
data %>%
  column_to_rownames('doi') %>%
  select(starts_with('beta_study')) %>%
  mutate(., check_col = (!(`beta_study.pcoa_study_effects.` %in% c('0', '1')) |
                         !(`beta_study.adonis_adj.` %in% c('0', '1')) |
                         !(`beta_study.other.` %in% c('0', '1')))
          ) %>%
  filter(., check_col) %>%
  select(-c(check_col)) %>%
  rename_with(., .cols=starts_with('beta_study.')
               , .fn=~str_remove(str_remove(., 'beta_study.'), '\\.')
               ) %>%
  reactable()
```

###  Article Checks

So, we have two articles to check. We dont see coding for 
**10.1093/femsec/fix153** with other, so we're going to code it as not other.

And then, there's **10.1111/bjd.20626**, which continues to be a difficult
article. Going through the main text and supplemental material, there is no
PCoA by study and no  adnois adjusted for study effect. Part of the issue
here is that the study is confounded by the comparative group, but I dont see
any PCoA or statistical analysis discussing the study effect, so we'll code 
that as no.

## Described/Not described

I want to make sure that we dont have cases where the analyses are listed as 
both described and then are described.

```{r, echo=FALSE}
data %>%
  select(doi, starts_with('beta_study')) %>%
  mutate(., `beta_study.no_study_pcoa.` = case_when(doi == '10.1111/bjd.20626' ~ '1'
                                                   ,.default = `beta_study.no_study_pcoa.`)
          , `beta_study.pcoa_study_effects.` = case_when(doi == '10.1111/bjd.20626' ~ '0'
                                                        ,.default = `beta_study.pcoa_study_effects.`
                                                         )
          , `beta_study.adonis_adj.` = case_when(doi == '10.1111/bjd.20626'  ~ '0'
                                               ,.default = `beta_study.adonis_adj.`)
          , `beta_study.no_adj.` = case_when(doi == '10.1111/bjd.20626' ~ '1'
                                            ,.default = `beta_study.no_adj.`)
          , `beta_study.other.` = case_when(doi == '10.1093/femsec/fix153' ~ '0'
                                           ,.default = `beta_study.other.`)
          ) %>%
  mutate_at(., .vars=vars(starts_with('beta_study.'))
             , .funs=as.numeric) %>%
  mutate(., defined = (`beta_study.no_study_pcoa.` + 
                       `beta_study.pcoa_study_effects.` + 
                       `beta_study.adonis_adj.` + 
                       `beta_study.other_test_adj.` + 
                       `beta_study.no_adj.` + 
                       `beta_study.other.`)
          , defined = (defined > 0) * 1
          ) %>%
  ungroup() %>%
  filter(., ((defined == 1) & (`beta_study.not_described.` == 1))) %>%
  rename_with(., .fn=~str_remove(str_remove(., 'beta_study.'), '\\.')
               , .cols=starts_with('beta_study.')) %>%
  reactable()
```
So, we have one data set where we have a "not described" data set and we have
multiple analyses shown. So, let's go check the data?



### 10.1186/s13059-022-02637-7

The paper pulls several V4 data sets and looks for an assocation between
the microbiome structure and both IBD and a general health index. From the
methods:

> **PERMANOVA**
> 
> Samples were rarified to a constant depth of 3000 reads per sample, and 
sample-sample distances were calculated using the Bray-Curtis and 
unweighted-unifrac metrics using qiime2. To quantify the contribution of 
different factors to the microbial composition, PERMANOVA was applied using the
Adonis function in the R package Vegan (vegan: Community Ecology Package. 
R package version 2.5-6.https://CRAN.R-project.org/package=vegan) [61] using 
both metrics. Variables tested were as follows: case/control, specific disease, 
country, cohort, disease cohort, and age group (adult/child). Analysis was 
performed on the original samples as well as on the aggregated samples (i.e., 
one sample per case/control group in each disease cohort). The total variance 
explained by each variable was calculated independently of other variables 
(that is, as the sole variable in the model).
>
> **Quantification of cohort vs. case/control distance contribution**
> 
> Bray-Curtis distances between pairs of aggregated samples were calculated for
the following pairs: case and control pairs from the same disease cohort, cases
from different disease cohorts, and controls from different disease cohorts. 
The distribution of the distances of these three groups was compared using a 
two-sided non-parametric Mann-Whitney test.

We see the pooled PCoA colored somewhat by data set (Figure 2B, Figure S1C, 
Figure S1D) and colored by disease (Figure 2C, Figure 1D). So, we don't see 
PCoAs by individual values. 

The PERMANOVA shown in Figure S1A is calcualted for all the samples, and in 
that effect modified pooled data.

So, I think the "other" is probably correctly coded here, but Im not sure the
"not described" is correctly coded here.


## PCoA Consistency

In beta diversity analysis, we haev two concepts included: data visualization
and statistical analysis. So, I want to start with the PCoA visualization
where we'd either expect there to be a PCoA of study effects or no PCoA
of study effects.

```{r, echo=FALSE}
data %>%
  select(doi, starts_with('beta_study')) %>%
  mutate(., `beta_study.no_study_pcoa.` = case_when(doi == '10.1111/bjd.20626' ~ '1'
                                                   ,.default = `beta_study.no_study_pcoa.`)
          , `beta_study.pcoa_study_effects.` = case_when(doi == '10.1111/bjd.20626' ~ '0'
                                                        ,.default = `beta_study.pcoa_study_effects.`
                                                         )
          , `beta_study.adonis_adj.` = case_when(doi == '10.1111/bjd.20626'  ~ '0'
                                               ,.default = `beta_study.adonis_adj.`)
          , `beta_study.no_adj.` = case_when(doi == '10.1111/bjd.20626' ~ '1'
                                            ,.default = `beta_study.no_adj.`)
          , `beta_study.other.` = case_when(doi == '10.1093/femsec/fix153' ~ '0'
                                           ,.default = `beta_study.other.`)
          ,`beta_study.not_described.` = case_when(doi == '10.1186/s13059-022-02637-7' ~ '0'
                                                   ,.default = `beta_study.not_described.`)
          ) %>%
  select(doi, starts_with('beta_study')) %>%
  mutate_at(., .vars=vars(starts_with('beta_study.'))
             , .funs=as.numeric) %>%
  filter((`beta_study.no_study_pcoa.` == 1) & (`beta_study.pcoa_study_effects.` == 1)) %>%
  select(c(doi, contains('pcoa'))) %>%
  reactable()
```



So, I want to check the four articles where we see the discrepancy.

### 10.1016/j.chom.2013.08.006	

Figure 3A in this shows a study effect, so I'd say that we have that PCoA of
study effects, and those labels are consistently there. There are other PCoAs in
the analysis (for example, a single-study PCoA, from the HIV data set), but the
data very much has as PCoA by the data set. So, this should be coded as "no" 
for `beta_study.no_study_pcoa.`.

### 10.1136/gutjnl-2015-310376

In the main text, there's a PCoA figure that's colored by body site and PPI use,
which is the main focus of the paper. Going through the supplement, Figure S7
is the PCoA from the main text, colored by data set and body site. 

This should be coded as "no" for `beta_study.no_study_pcoa.`.

### 10.1186/s40168-017-0248-8

We see an unadjusted PCoA in figure 3C and D, and then we see a PCoA colored
by study effect in Figure 6C and D. So, once again, this should be coded as 
"no" for `beta_study.no_study_pcoa.`.

### 10.1016/j.febslet.2014.09.039

This is a delightful edge case where there are two analyses that have been 
performed. For the obesity analysis, we see a PCoA by study effect (Figure 6).
The main text doesn't contain a PCoA by study for the IBD analysis, and I can't
access the supplement. So, based on what's currently available, this is 
correctly coded.

## PCoA Other Overlap

I'd also like to check the coding where we have "Other" with PCoA.

```{r, echo=FALSE}
data %>%
  select(doi, starts_with('beta_study')) %>%
  mutate(., `beta_study.no_study_pcoa.` = case_when(doi == '10.1111/bjd.20626' ~ '1'
                                                   ,doi == '10.1016/j.chom.2013.08.006' ~ '0'
                                                   ,doi == '10.1136/gutjnl-2015-310376' ~ '0'
                                                   ,doi == '10.1186/s40168-017-0248-8' ~ '0'
                                                   ,.default = `beta_study.no_study_pcoa.`)
          , `beta_study.pcoa_study_effects.` = case_when(doi == '10.1111/bjd.20626' ~ '0'
                                                        ,.default = `beta_study.pcoa_study_effects.`
                                                         )
          , `beta_study.adonis_adj.` = case_when(doi == '10.1111/bjd.20626'  ~ '0'
                                               ,.default = `beta_study.adonis_adj.`)
          , `beta_study.no_adj.` = case_when(doi == '10.1111/bjd.20626' ~ '1'
                                            ,.default = `beta_study.no_adj.`)
          , `beta_study.other.` = case_when(doi == '10.1093/femsec/fix153' ~ '0'
                                           ,.default = `beta_study.other.`)
          ,`beta_study.not_described.` = case_when(doi == '10.1186/s13059-022-02637-7' ~ '0'
                                                   ,.default = `beta_study.not_described.`)
          ) %>%
  mutate_at(., .vars=vars(starts_with('beta_study.'))
             , .funs=as.numeric) %>%
  rename_with(., .fn=~str_remove(str_remove(., 'beta_study.'), '\\.')
               , .cols=starts_with('beta_study.')) %>%
  filter(., (other == 1) | !is.na(beta_study_other)) %>%
  select(c(doi, no_study_pcoa, pcoa_study_effects, other, beta_study_other)) %>%
  reactable()
```
I think we maybe need to code a "other" PCoA flag based on the "other" category,
but I also want to check the articles.

### 10.1186/s13059-022-02637-7

```{r, echo=FALSE}
data %>%
  select(doi, starts_with('beta_study')) %>%
  mutate(., `beta_study.no_study_pcoa.` = case_when(doi == '10.1111/bjd.20626' ~ '1'
                                                   ,doi == '10.1016/j.chom.2013.08.006' ~ '0'
                                                   ,doi == '10.1136/gutjnl-2015-310376' ~ '0'
                                                   ,doi == '10.1186/s40168-017-0248-8' ~ '0'
                                                   ,.default = `beta_study.no_study_pcoa.`)
          , `beta_study.pcoa_study_effects.` = case_when(doi == '10.1111/bjd.20626' ~ '0'
                                                        ,.default = `beta_study.pcoa_study_effects.`
                                                         )
          , `beta_study.adonis_adj.` = case_when(doi == '10.1111/bjd.20626'  ~ '0'
                                               ,.default = `beta_study.adonis_adj.`)
          , `beta_study.no_adj.` = case_when(doi == '10.1111/bjd.20626' ~ '1'
                                            ,.default = `beta_study.no_adj.`)
          , `beta_study.other.` = case_when(doi == '10.1093/femsec/fix153' ~ '0'
                                           ,.default = `beta_study.other.`)
          ,`beta_study.not_described.` = case_when(doi == '10.1186/s13059-022-02637-7' ~ '0'
                                                   ,.default = `beta_study.not_described.`)
          ) %>%
  mutate_at(., .vars=vars(starts_with('beta_study.'))
             , .funs=as.numeric) %>%
  rename_with(., .fn=~str_remove(str_remove(., 'beta_study.'), '\\.')
               , .cols=starts_with('beta_study.')) %>%
  filter(., doi == '10.1186/s13059-022-02637-7') %>%
  select(c(doi, no_study_pcoa, pcoa_study_effects, other, beta_study_other)) %>%
  reactable()
```
They do a modified Bray Curtis ordination, which is essentially a study pooled 
PCoA (Figure 2B/C) as well a PCoA by study (Figure S1). So, I think this
can be coded as a study-pooled PCoA.

### 10.1186/s40168-017-0248-8

```{r, echo=FALSE}
data %>%
  select(doi, starts_with('beta_study')) %>%
  mutate(., `beta_study.no_study_pcoa.` = case_when(doi == '10.1111/bjd.20626' ~ '1'
                                                   ,doi == '10.1016/j.chom.2013.08.006' ~ '0'
                                                   ,doi == '10.1136/gutjnl-2015-310376' ~ '0'
                                                   ,doi == '10.1186/s40168-017-0248-8' ~ '0'
                                                   ,.default = `beta_study.no_study_pcoa.`)
          , `beta_study.pcoa_study_effects.` = case_when(doi == '10.1111/bjd.20626' ~ '0'
                                                        ,.default = `beta_study.pcoa_study_effects.`
                                                         )
          , `beta_study.pcoa_study_avg.` = case_when(doi == '10.1186/s13059-022-02637-7' ~ '1'
                                                    ,.default = '0')
          , `beta_study.adonis_adj.` = case_when(doi == '10.1111/bjd.20626'  ~ '0'
                                               ,.default = `beta_study.adonis_adj.`)
          , `beta_study.no_adj.` = case_when(doi == '10.1111/bjd.20626' ~ '1'
                                            ,.default = `beta_study.no_adj.`)
          , `beta_study.other.` = case_when(doi == '10.1093/femsec/fix153' ~ '0'
                                           ,.default = `beta_study.other.`)
          ,`beta_study.not_described.` = case_when(doi == '10.1186/s13059-022-02637-7' ~ '0'
                                                   ,.default = `beta_study.not_described.`)
          ) %>%
  mutate_at(., .vars=vars(starts_with('beta_study.'))
             , .funs=as.numeric) %>%
  rename_with(., .fn=~str_remove(str_remove(., 'beta_study.'), '\\.')
               , .cols=starts_with('beta_study.')) %>%
  filter(., doi == '10.1186/s40168-017-0248-8') %>%
  select(c(doi, no_study_pcoa, pcoa_study_effects, other, beta_study_other)) %>%
  reactable()
```

This is a Study PCoA (Figure 3) but also a PCoA by primer pair, so I think 
we're going to just leave it for now and not code it as PCoA other?

### 10.3389/fmicb.2021.711134

```{r, echo=FALSE}
data %>%
  select(doi, starts_with('beta_study')) %>%
  mutate(., `beta_study.no_study_pcoa.` = case_when(doi == '10.1111/bjd.20626' ~ '1'
                                                   ,doi == '10.1016/j.chom.2013.08.006' ~ '0'
                                                   ,doi == '10.1136/gutjnl-2015-310376' ~ '0'
                                                   ,doi == '10.1186/s40168-017-0248-8' ~ '0'
                                                   ,.default = `beta_study.no_study_pcoa.`)
          , `beta_study.pcoa_study_effects.` = case_when(doi == '10.1111/bjd.20626' ~ '0'
                                                        ,.default = `beta_study.pcoa_study_effects.`
                                                         )
          , `beta_study.pcoa_study_avg.` = case_when(doi == '10.1186/s13059-022-02637-7' ~ '1'
                                                    ,.default = '0')
          , `beta_study.adonis_adj.` = case_when(doi == '10.1111/bjd.20626'  ~ '0'
                                               ,.default = `beta_study.adonis_adj.`)
          , `beta_study.no_adj.` = case_when(doi == '10.1111/bjd.20626' ~ '1'
                                            ,.default = `beta_study.no_adj.`)
          , `beta_study.other.` = case_when(doi == '10.1093/femsec/fix153' ~ '0'
                                           ,doi == '10.1186/s40168-017-0248-8' ~ '0'
                                           ,.default = `beta_study.other.`)
          ,`beta_study.not_described.` = case_when(doi == '10.1186/s13059-022-02637-7' ~ '0'
                                                   ,.default = `beta_study.not_described.`)
          ) %>%
  mutate_at(., .vars=vars(starts_with('beta_study.'))
             , .funs=as.numeric) %>%
  rename_with(., .fn=~str_remove(str_remove(., 'beta_study.'), '\\.')
               , .cols=starts_with('beta_study.')) %>%
  filter(., doi == '10.3389/fmicb.2021.711134') %>%
  select(c(doi, no_study_pcoa, pcoa_study_effects, other, beta_study_other)) %>%
  reactable()
```
An NMDS is very similar to a PCoA, so, again, we're coding this as a study
effect. And for visualization, we're not going to describe this an other 
PCoA.

### 10.3389/fcimb.2020.00434
```{r, echo=FALSE}
data %>%
  select(doi, starts_with('beta_study')) %>%
  mutate(., `beta_study.no_study_pcoa.` = case_when(doi == '10.1111/bjd.20626' ~ '1'
                                                   ,doi == '10.1016/j.chom.2013.08.006' ~ '0'
                                                   ,doi == '10.1136/gutjnl-2015-310376' ~ '0'
                                                   ,doi == '10.1186/s40168-017-0248-8' ~ '0'
                                                   ,.default = `beta_study.no_study_pcoa.`)
          , `beta_study.pcoa_study_effects.` = case_when(doi == '10.1111/bjd.20626' ~ '0'
                                                        ,.default = `beta_study.pcoa_study_effects.`
                                                         )
          , `beta_study.pcoa_study_avg.` = case_when(doi == '10.1186/s13059-022-02637-7' ~ '1'
                                                    ,.default = '0')
          , `beta_study.adonis_adj.` = case_when(doi == '10.1111/bjd.20626'  ~ '0'
                                               ,.default = `beta_study.adonis_adj.`)
          , `beta_study.no_adj.` = case_when(doi == '10.1111/bjd.20626' ~ '1'
                                            ,.default = `beta_study.no_adj.`)
          , `beta_study.other.` = case_when(doi == '10.1093/femsec/fix153' ~ '0'
                                           ,doi == '10.1186/s40168-017-0248-8' ~ '0'
                                           ,.default = `beta_study.other.`)
          ,`beta_study.not_described.` = case_when(doi == '10.1186/s13059-022-02637-7' ~ '0'
                                                   ,.default = `beta_study.not_described.`)
          ) %>%
  mutate_at(., .vars=vars(starts_with('beta_study.'))
             , .funs=as.numeric) %>%
  rename_with(., .fn=~str_remove(str_remove(., 'beta_study.'), '\\.')
               , .cols=starts_with('beta_study.')) %>%
  filter(., doi == '10.3389/fcimb.2020.00434') %>%
  select(c(doi, no_study_pcoa, pcoa_study_effects, other, beta_study_other)) %>%
  reactable()
```

This is a parallel analysis, and I think I want a seperate flag for that.

## Statistical test: Missing

I also want to look at the statistical test, and start by checking if there are
any cases where the statistical test we applied conflicts (adjusted for study
effect and not adjusted for study effect.)
 
```{r, echo=FALSE}
data %>%
  select(doi, starts_with('beta_study')) %>%
  mutate(., `beta_study.no_study_pcoa.` = case_when(doi == '10.1111/bjd.20626' ~ '1'
                                                   ,doi == '10.1016/j.chom.2013.08.006' ~ '0'
                                                   ,doi == '10.1136/gutjnl-2015-310376' ~ '0'
                                                   ,doi == '10.1186/s40168-017-0248-8' ~ '0'
                                                   ,.default = `beta_study.no_study_pcoa.`)
          , `beta_study.pcoa_study_effects.` = case_when(doi == '10.1111/bjd.20626' ~ '0'
                                                        ,.default = `beta_study.pcoa_study_effects.`
                                                         )
          , `beta_study.pcoa_study_avg.` = case_when(doi == '10.1186/s13059-022-02637-7' ~ '1'
                                                    ,.default = '0')
          , `beta_study.parallel_pcoa` = case_when(doi == '10.3389/fcimb.2020.00434' ~ '1'
                                                   ,.default = '0')
          , `beta_study.pcoa_other.` = case_when(doi == '10.3389/fmicb.2021.711134' ~ '0'
                                                ,doi == '10.1186/s40168-017-0248-8' ~ '0' 
                                                ,grepl('pcoa', tolower(beta_study_other)) ~ '1'
                                                ,doi == '10.1186/s13059-022-02637-7' ~ '1'
                                                ,.default = '0')
          , `beta_study.adonis_adj.` = case_when(doi == '10.1111/bjd.20626'  ~ '0'
                                               ,.default = `beta_study.adonis_adj.`)
          , `beta_study.no_adj.` = case_when(doi == '10.1111/bjd.20626' ~ '1'
                                            ,.default = `beta_study.no_adj.`)
          , `beta_study.other.` = case_when(doi == '10.1093/femsec/fix153' ~ '0'
                                           ,doi == '10.1186/s40168-017-0248-8' ~ '0'
                                           ,.default = `beta_study.other.`)
          ,`beta_study.not_described.` = case_when(doi == '10.1186/s13059-022-02637-7' ~ '0'
                                                   ,.default = `beta_study.not_described.`)
          , beta_study_other = ifelse(beta_study_other %in% c('0', 0), NA, beta_study_other)
          ) %>%
  mutate_at(., .vars=vars(starts_with('beta_study.'))
             , .funs=as.numeric) %>%
  select(c(doi, `beta_study.no_adj.`, `beta_study.adonis_adj.`, `beta_study.other_test_adj.`, `beta_study.other.`)) %>%
  rename_with(., .fn=~str_remove(str_remove(., 'beta_study.'), '\\.')
               , .cols=starts_with('beta_study')) %>%
  mutate(., has_adj = ((adonis_adj + other_test_adj) > 0) * 1) %>%
  filter(., (has_adj == 1) & (no_adj == 1)) %>%
  reactable()
```

## Statistical test: Other categories

I want to look at the "other" categories we coded, and see how they interact
with the actual definitions.

```{r, echo=FALSE}
  data %>%
  mutate(., multi_envo = 1 * (`envo.other.` %in% c('1', 1))) %>%
  select(doi, multi_envo, starts_with('beta_study')) %>%
  mutate(., `beta_study.no_study_pcoa.` = case_when(doi == '10.1111/bjd.20626' ~ '1'
                                                   ,doi == '10.1016/j.chom.2013.08.006' ~ '0'
                                                   ,doi == '10.1136/gutjnl-2015-310376' ~ '0'
                                                   ,doi == '10.1186/s40168-017-0248-8' ~ '0'
                                                   ,.default = `beta_study.no_study_pcoa.`)
          , `beta_study.pcoa_study_effects.` = case_when(doi == '10.1111/bjd.20626' ~ '0'
                                                        ,.default = `beta_study.pcoa_study_effects.`
                                                         )
          , `beta_study.pcoa_study_avg.` = case_when(doi == '10.1186/s13059-022-02637-7' ~ '1'
                                                    ,.default = '0')
          , `beta_study.parallel_pcoa` = case_when(doi == '10.3389/fcimb.2020.00434' ~ '1'
                                                   ,.default = '0')
          , `beta_study.pcoa_other.` = case_when(doi == '10.3389/fmicb.2021.711134' ~ '0'
                                                ,doi == '10.1186/s40168-017-0248-8' ~ '0' 
                                                ,grepl('pcoa', tolower(beta_study_other)) ~ '1'
                                                ,doi == '10.1186/s13059-022-02637-7' ~ '1'
                                                ,.default = '0')
          , `beta_study.adonis_adj.` = case_when(doi == '10.1111/bjd.20626'  ~ '0'
                                               ,.default = `beta_study.adonis_adj.`)
          , `beta_study.no_adj.` = case_when(doi == '10.1111/bjd.20626' ~ '1'
                                            ,.default = `beta_study.no_adj.`)
          , `beta_study.other.` = case_when(doi == '10.1093/femsec/fix153' ~ '0'
                                           ,doi == '10.1186/s40168-017-0248-8' ~ '0'
                                           ,.default = `beta_study.other.`)
          ,`beta_study.not_described.` = case_when(doi == '10.1186/s13059-022-02637-7' ~ '0'
                                                   ,.default = `beta_study.not_described.`)
          , beta_study_other = ifelse(beta_study_other %in% c('0', 0), NA, beta_study_other)
          ) %>%
  mutate_at(., .vars=vars(starts_with('beta_study.'))
             , .funs=as.numeric) %>%
  filter(., (`beta_study.other.` == 1) | !is.na(beta_study_other)) %>%
  rename_with(., .cols=starts_with('beta_study.')
               , .fn=~str_remove(str_remove(., 'beta_study.'), '\\.')
               ) %>%
  select(c(doi, beta_study_other, multi_envo, pcoa_other, )) %>%
  arrange(., multi_envo, pcoa_other) %>%
  reactable(.,  defaultPageSize = 12)
```
I'd like to check these articles to see how things were done, because some of 
these seem like a weird approach to beta diversity.

### 10.1136/gutjnl-2015-310376
If we go back to the original coding, 

```{r, echo=FALSE}
data %>%
  mutate(., multi_envo = 1 * (`envo.other.` %in% c('1', 1))) %>%
  select(doi, multi_envo, starts_with('beta_study')) %>%
  mutate(., `beta_study.no_study_pcoa.` = case_when(doi == '10.1111/bjd.20626' ~ '1'
                                                   ,doi == '10.1016/j.chom.2013.08.006' ~ '0'
                                                   ,doi == '10.1136/gutjnl-2015-310376' ~ '0'
                                                   ,doi == '10.1186/s40168-017-0248-8' ~ '0'
                                                   ,.default = `beta_study.no_study_pcoa.`)
          , `beta_study.pcoa_study_effects.` = case_when(doi == '10.1111/bjd.20626' ~ '0'
                                                        ,.default = `beta_study.pcoa_study_effects.`
                                                         )
          , `beta_study.pcoa_study_avg.` = case_when(doi == '10.1186/s13059-022-02637-7' ~ '1'
                                                    ,.default = '0')
          , `beta_study.parallel_pcoa` = case_when(doi == '10.3389/fcimb.2020.00434' ~ '1'
                                                   ,.default = '0')
          , `beta_study.pcoa_other.` = case_when(doi == '10.3389/fmicb.2021.711134' ~ '0'
                                                ,doi == '10.1186/s40168-017-0248-8' ~ '0' 
                                                ,grepl('pcoa', tolower(beta_study_other)) ~ '1'
                                                ,doi == '10.1186/s13059-022-02637-7' ~ '1'
                                                ,.default = '0')
          , `beta_study.adonis_adj.` = case_when(doi == '10.1111/bjd.20626'  ~ '0'
                                               ,.default = `beta_study.adonis_adj.`)
          , `beta_study.no_adj.` = case_when(doi == '10.1111/bjd.20626' ~ '1'
                                            ,.default = `beta_study.no_adj.`)
          , `beta_study.other.` = case_when(doi == '10.1093/femsec/fix153' ~ '0'
                                           ,doi == '10.1186/s40168-017-0248-8' ~ '0'
                                           ,.default = `beta_study.other.`)
          ,`beta_study.not_described.` = case_when(doi == '10.1186/s13059-022-02637-7' ~ '0'
                                                   ,.default = `beta_study.not_described.`)
          , beta_study_other = ifelse(beta_study_other %in% c('0', 0), NA, beta_study_other)
          ) %>%
  mutate_at(., .vars=vars(starts_with('beta_study.'))
             , .funs=as.numeric) %>%
  filter(., (`beta_study.other.` == 1) | !is.na(beta_study_other)) %>%
  rename_with(., .cols=starts_with('beta_study.')
               , .fn=~str_remove(str_remove(., 'beta_study.'), '\\.')
               ) %>%
  filter(., doi == '10.1136/gutjnl-2015-310376') %>%
  mutate_all(., as.character) %>%
  pivot_longer(cols=everything()) %>%
  reactable(.,  defaultPageSize = 12)
```

We get a PCoA by study effects and a wilcoxen test. In this study, they 
tested on dimensionality reduction. From the methods:

> Differences in richness (the number of species within a sample), 
principal coordinate analyses (PCoA) and Shannon diversity analysis were 
determined using the QIIME microbiome analysis software.18 The Wilcoxon test 
and Spearman's correlations were used to identify differences in Shannon's 
diversity and relations between the PCoA scores of PPI users and non-PPI users,
while the χ2 test, Fisher's exact test, Spearman's correlation and 
Wilcoxon-Mann-Whitney test (WMW test) were used to determine differences in 
age, gender, BMI, antibiotics use, and gut complaints between PPI users and 
non-users. In all the microbiome analyses, multiple test corrections were 
based on the false discovery rate (FDR). An FDR value of 0.05 was used as a 
statistically significant cut-off.

So, they did KW testing against the PCs, I think. This is cooberated in the 
supplement:

> The overall difference of the gut microbiome associated with PPI use was 
also observed in the PCoA of all the data sets together (figure 3 and see 
online supplementary figure S6). The same PCoA with separate colours for each
cohort has been added in online supplementary figure S7. Notably, we observed
statistically significant differences between PPI users and non-users in two
principal coordinates (PCoA1: p=1.39×10−20, PCoA3: p=0.0004, Wilcoxon test).

We can see the association in Figture 2; looking at Figure S6 and S7, we 
don't see an adjustment for data set or study effect. 

In the original survey, the answer associated with `beta_study.no_adj.` was 

| Column | Response Text |
| :--- | :--- |
| `beta_study[no_study_pcoa]` | Descriptive PCoA that did not show study effects |
| `beta_study[pcoa_study_effects]` | PCoA showing study effects |
| `beta_study[adonis_adj]` | Adonis permanova showing or adjusting for study effect size |
| `beta_study[other_test_adj]` | Other test adjusted for study effect |
| `beta_study[no_adj]` | Not adjustment or acknowledgment of study effect |
| `beta_study[other]` | Other |
| `beta_study[not_described]` | Not described |
| `beta_study_other` | *other free text* |

And so, which I think the "other" flag is okay, I think we really want to 
code this as adjusted. 

### 10.1038/s41396-020-0727-y

```{r, echo=FALSE}
data %>%
  mutate(., multi_envo = 1 * (`envo.other.` %in% c('1', 1))) %>%
  select(doi, multi_envo, starts_with('beta_study')) %>%
  mutate(., beta_study_other = ifelse(`beta_study.other.` == '0', NA, beta_study_other)
          , `beta_study.adonis_adj.` = case_when(doi == '10.1111/bjd.20626'  ~ '0'
                                               ,.default = `beta_study.adonis_adj.`)
          , `beta_study.no_adj.` = case_when(doi == '10.1111/bjd.20626' ~ '1'
                                            ,doi == '10.1136/gutjnl-2015-310376' ~ '1'
                                            ,.default = `beta_study.no_adj.`)
          , `beta_study.other.` = case_when(doi == '10.1093/femsec/fix153' ~ '0'
                                           ,.default = `beta_study.other.`)
          ,`beta_study.not_described.` = case_when(doi == '10.1186/s13059-022-02637-7' ~ '0'
                                                   ,.default = `beta_study.not_described.`)
         , `beta_study.no_study_pcoa.` = case_when(doi == '10.1111/bjd.20626' ~ '1'
                                                   ,doi == '10.1016/j.chom.2013.08.006' ~ '0'
                                                   ,doi == '10.1136/gutjnl-2015-310376' ~ '0'
                                                   ,doi == '10.1186/s40168-017-0248-8' ~ '0'
                                                   ,.default = `beta_study.no_study_pcoa.`)
          , `beta_study.pcoa_study_effects.` = case_when(doi == '10.1111/bjd.20626' ~ '0'
                                                        ,.default = `beta_study.pcoa_study_effects.`
                                                         )
          , `beta_study.pcoa_study_avg.` = case_when(doi == '10.1186/s13059-022-02637-7' ~ '1'
                                                    ,.default = '0')
          , `beta_study.parallel_pcoa` = case_when(doi == '10.3389/fcimb.2020.00434' ~ '1'
                                                   ,.default = '0')
          , `beta_study.pcoa_other.` = case_when(doi == '10.3389/fmicb.2021.711134' ~ '0'
                                                ,doi == '10.1186/s40168-017-0248-8' ~ '0' 
                                                ,grepl('pcoa', tolower(beta_study_other)) ~ '1'
                                                ,doi == '10.1186/s13059-022-02637-7' ~ '1'
                                                ,.default = '0')
          ) %>%
  mutate_at(., .vars=vars(starts_with('beta_study.'))
             , .funs=as.numeric) %>%
  filter(., (`beta_study.other.` == 1) | !is.na(beta_study_other)) %>%
  rename_with(., .cols=starts_with('beta_study.')
               , .fn=~str_remove(str_remove(., 'beta_study.'), '\\.')
               ) %>%
  filter(., doi == '10.1038/s41396-020-0727-y') %>%
  mutate_all(., as.character) %>%
  pivot_longer(cols=everything()) %>%
  filter(., value != '0') %>%
  reactable(.,  defaultPageSize = 12)
```

So, we see a test for the study effect, but we dont actually see a test for the
effect. We'll define this as "no inference"

So, I think we maybe need to note that there is no test for inference.


### 10.1038/s41531-021-00156-z

```{r, echo=FALSE}
data %>%
  mutate(., multi_envo = 1 * (`envo.other.` %in% c('1', 1))) %>%
  select(doi, multi_envo, starts_with('beta_study')) %>%
  mutate(., beta_study_other = ifelse(`beta_study.other.` == '0', NA, beta_study_other)
          , `beta_study.adonis_adj.` = case_when(doi == '10.1111/bjd.20626'  ~ '0'
                                               ,.default = `beta_study.adonis_adj.`)
          , `beta_study.no_adj.` = case_when(doi == '10.1111/bjd.20626' ~ '1'
                                            ,doi == '10.1136/gutjnl-2015-310376' ~ '1'
                                            ,.default = `beta_study.no_adj.`)
          , `beta_study.other.` = case_when(doi == '10.1093/femsec/fix153' ~ '0'
                                           ,.default = `beta_study.other.`)
          ,`beta_study.not_described.` = case_when(doi == '10.1186/s13059-022-02637-7' ~ '0'
                                                   ,.default = `beta_study.not_described.`)
          , `beta_study.no_inference_test.` = case_when(doi == '10.1038/s41396-020-0727-y' ~ '1'
                                                       ,.default = '0')
          , `beta_study.no_study_pcoa.` = case_when(doi == '10.1111/bjd.20626' ~ '1'
                                                   ,doi == '10.1016/j.chom.2013.08.006' ~ '0'
                                                   ,doi == '10.1136/gutjnl-2015-310376' ~ '0'
                                                   ,doi == '10.1186/s40168-017-0248-8' ~ '0'
                                                   ,.default = `beta_study.no_study_pcoa.`)
          , `beta_study.pcoa_study_effects.` = case_when(doi == '10.1111/bjd.20626' ~ '0'
                                                        ,.default = `beta_study.pcoa_study_effects.`
                                                         )
          , `beta_study.pcoa_study_avg.` = case_when(doi == '10.1186/s13059-022-02637-7' ~ '1'
                                                    ,.default = '0')
          , `beta_study.parallel_pcoa` = case_when(doi == '10.3389/fcimb.2020.00434' ~ '1'
                                                   ,.default = '0')
          , `beta_study.pcoa_other.` = case_when(doi == '10.3389/fmicb.2021.711134' ~ '0'
                                                ,doi == '10.1186/s40168-017-0248-8' ~ '0' 
                                                ,grepl('pcoa', tolower(beta_study_other)) ~ '1'
                                                ,doi == '10.1186/s13059-022-02637-7' ~ '1'
                                                ,.default = '0')
          ) %>%
  mutate_at(., .vars=vars(starts_with('beta_study.'))
             , .funs=as.numeric) %>%
  filter(., (`beta_study.other.` == 1) | !is.na(beta_study_other)) %>%
  rename_with(., .cols=starts_with('beta_study.')
               , .fn=~str_remove(str_remove(., 'beta_study.'), '\\.')
               ) %>%
  filter(., doi == '10.1038/s41531-021-00156-z') %>%
  mutate_all(., as.character) %>%
  pivot_longer(cols=everything()) %>%
  reactable(.,  defaultPageSize = 15)
```

So, we have dbRDA models, which is esesntially a regression against the PCoA
space instead of an adonis-type model which is a regression against the 
distance matrix. dbRDA allows more metrics (e.g. not just euclidean distance)
for the ecological data.

Going back to the paper, 

>To assess data heterogeneity, we first performed 99% close-reference 
clustering for ASVs for each dataset so they can be compared with each other. 
Principal coordinate analysis on all samples based on 99% OTUs showed a clear
separation of samples by study (PERMANOVA R2=0.45, P<0.001, Fig. S1), 
indicating study-specific batch effects. Among all confounding factors, 
hypervariable region showed the strongest association with variation of 
microbial composition (Canonical correspondence analysis (CCA), F=5.629, 
P=0.0035, Fig. S2), followed by country (F=3.243, P=0.019) and sequencing
platform (F=2.781, P=0.026). 

They then collapsed the data to genus level, where they found:

> Examining distribution of each genus in each dataset revealed a clustering 
of datasets largely by hypervariable regions (Fig. S3), indicating different 
regions of 16S rRNA gene surveyed may contribute to the divergence of taxa 
observed between studies. The cross-dataset heterogeneity was alleviated when 
analyzed at the genus-level, as indicated both in PERMANOVA and CCA results 
(Figs. S1, 2). Hypervariable region remained of borderline significance in the 
genus-level CCA (F=2.318, P=0.047).

We don't see an association with the outcome (COPD) in the statistical testing
in either Figure S1 and Figure S2.  They then go on to meta-analyze taxa
but they don't do any more work wiht the effect of COPD in beta diversity
as far as I can find in either the main text fo the supplement.

So, I think we can code this as no inference and I think for analytical
purposes, we're going to skip the "other" category.

### 10.3389/fcimb.2022.816526

```{r, echo=FALSE}
data %>%
  mutate(., multi_envo = 1 * (`envo.other.` %in% c('1', 1))) %>%
  select(doi, multi_envo, starts_with('beta_study')) %>%
  mutate(., beta_study_other = ifelse(`beta_study.other.` == '0', NA, beta_study_other)
          , `beta_study.adonis_adj.` = case_when(doi == '10.1111/bjd.20626'  ~ '0'
                                               ,.default = `beta_study.adonis_adj.`)
          , `beta_study.no_adj.` = case_when(doi == '10.1111/bjd.20626' ~ '1'
                                            ,doi == '10.1136/gutjnl-2015-310376' ~ '1'
                                            ,.default = `beta_study.no_adj.`)
          , `beta_study.other.` = case_when(doi == '10.1093/femsec/fix153' ~ '0'
                                           ,doi == '10.1038/s41531-021-00156-z'  ~ '1'
                                           ,.default = `beta_study.other.`)
          ,`beta_study.not_described.` = case_when(doi == '10.1186/s13059-022-02637-7' ~ '0'
                                                   ,.default = `beta_study.not_described.`)
          , `beta_study.no_inference_test.` = case_when(doi == '10.1038/s41396-020-0727-y' ~ '1'
                                                       ,doi == '10.1038/s41531-021-00156-z'  ~ '1'
                                                       ,.default = '0')
          , `beta_study.no_study_pcoa.` = case_when(doi == '10.1111/bjd.20626' ~ '1'
                                                   ,doi == '10.1016/j.chom.2013.08.006' ~ '0'
                                                   ,doi == '10.1136/gutjnl-2015-310376' ~ '0'
                                                   ,doi == '10.1186/s40168-017-0248-8' ~ '0'
                                                   ,.default = `beta_study.no_study_pcoa.`)
          , `beta_study.pcoa_study_effects.` = case_when(doi == '10.1111/bjd.20626' ~ '0'
                                                        ,.default = `beta_study.pcoa_study_effects.`
                                                         )
          , `beta_study.pcoa_study_avg.` = case_when(doi == '10.1186/s13059-022-02637-7' ~ '1'
                                                    ,.default = '0')
          , `beta_study.parallel_pcoa` = case_when(doi == '10.3389/fcimb.2020.00434' ~ '1'
                                                   ,.default = '0')
          , `beta_study.pcoa_other.` = case_when(doi == '10.3389/fmicb.2021.711134' ~ '0'
                                                ,doi == '10.1186/s40168-017-0248-8' ~ '0' 
                                                ,grepl('pcoa', tolower(beta_study_other)) ~ '1'
                                                ,doi == '10.1186/s13059-022-02637-7' ~ '1'
                                                ,.default = '0')
          ) %>%
  mutate_at(., .vars=vars(starts_with('beta_study.'))
             , .funs=as.numeric) %>%
  filter(., (`beta_study.other.` == 1) | !is.na(beta_study_other)) %>%
  rename_with(., .cols=starts_with('beta_study.')
               , .fn=~str_remove(str_remove(., 'beta_study.'), '\\.')
               ) %>%
  filter(., doi == '10.3389/fcimb.2022.816526') %>%
  mutate_all(., as.character) %>%
  pivot_longer(cols=everything()) %>%
  reactable(.,  defaultPageSize = 15)
```

The "other" category lists the data as "ANOSIM" which is a parallel test to 
adonis (permanova). 


Looking at the methods:

> Alpha and beta diversity metrics were evaluated by QIIME2 (Fung et al., 2021).
In identifying T2DM versus healthy controls, the Wilcoxon rank sum test was 
used to determine statistical differences between groups, considering that there
were only two groups which did not follow a normal distribution. Additionally, 
in identifying study heterogeneity among five groups, Anosim analysis was used.

From the results section describing study effect:

> In the principal coordinate analysis (PCoA), there was no corresponding 
statistical test to conclude whether the differences between the different 
groups were significant or not. Therefore, the significance of the differences 
was calculated using the Anosim analysis (Figure 1C), in which R = 0.473 
indicated significant differences between groups in the five studies (p = 0.001).

And then from later in the results on the association between the microbiome
and the diabetes status:

> In the analysis of the beta diversity, the PCoA revealed that the saliva 
samples from T2DM and control groups could not be separated, suggesting 
insignificantly different salivary microorganisms. R = 0.027 from the Anosim 
analysis further verified insignificant difference between groups (p = 0.004) 
(Figures 2A, B).

So, this is **not** a permanova, it's another test and account for study 
effect? 

### 10.1128/mbio.01018-16

```{r, echo=FALSE}
data %>%
  select(doi,starts_with('beta_study')) %>%
  mutate(., beta_study_other = ifelse(`beta_study.other.` == '0', NA, beta_study_other)
          , `beta_study.adonis_adj.` = case_when(doi == '10.1111/bjd.20626'  ~ '0'
                                                ,doi == '10.3389/fcimb.2022.816526' ~ '0'
                                               ,.default = `beta_study.adonis_adj.`)
          , `beta_study.no_adj.` = case_when(doi == '10.1111/bjd.20626' ~ '1'
                                            ,doi == '10.1136/gutjnl-2015-310376' ~ '1'
                                            ,.default = `beta_study.no_adj.`)
          , `beta_study.other_test_adj.` = case_when(doi == '10.3389/fcimb.2022.816526' ~ '1'
                                        ,.default = `beta_study.other_test_adj.`)
          , `beta_study.other.` = case_when(doi == '10.1093/femsec/fix153' ~ '0'
                                           ,doi == '10.1038/s41531-021-00156-z'  ~ '1'
                                           ,doi == '10.3389/fcimb.2022.816526' ~ '0'
                                           ,.default = `beta_study.other.`)
          ,`beta_study.not_described.` = case_when(doi == '10.1186/s13059-022-02637-7' ~ '0'
                                                   ,.default = `beta_study.not_described.`)
          , `beta_study.no_inference_test.` = case_when(doi == '10.1038/s41396-020-0727-y' ~ '1'
                                                       ,doi == '10.1038/s41531-021-00156-z'  ~ '1'
                                                       ,.default = '0')
          , `beta_study.no_study_pcoa.` = case_when(doi == '10.1111/bjd.20626' ~ '1'
                                                   ,doi == '10.1016/j.chom.2013.08.006' ~ '0'
                                                   ,doi == '10.1136/gutjnl-2015-310376' ~ '0'
                                                   ,doi == '10.1186/s40168-017-0248-8' ~ '0'
                                                   ,.default = `beta_study.no_study_pcoa.`)
          , `beta_study.pcoa_study_effects.` = case_when(doi == '10.1111/bjd.20626' ~ '0'
                                                        ,.default = `beta_study.pcoa_study_effects.`
                                                         )
          , `beta_study.pcoa_study_avg.` = case_when(doi == '10.1186/s13059-022-02637-7' ~ '1'
                                                    ,.default = '0')
          , `beta_study.parallel_pcoa` = case_when(doi == '10.3389/fcimb.2020.00434' ~ '1'
                                                   ,.default = '0')
          , `beta_study.pcoa_other.` = case_when(doi == '10.3389/fmicb.2021.711134' ~ '0'
                                                ,doi == '10.1186/s40168-017-0248-8' ~ '0' 
                                                ,grepl('pcoa', tolower(beta_study_other)) ~ '1'
                                                ,doi == '10.1186/s13059-022-02637-7' ~ '1'
                                                ,.default = '0')
          ) %>%
  mutate_at(., .vars=vars(starts_with('beta_study.'))
             , .funs=as.numeric) %>%
  filter(., (`beta_study.other.` == 1) | !is.na(beta_study_other)) %>%
  rename_with(., .cols=starts_with('beta_study.')
               , .fn=~str_remove(str_remove(., 'beta_study.'), '\\.')
               ) %>%
  filter(., doi == '10.1128/mbio.01018-16') %>%
  mutate_all(., as.character) %>%
  pivot_longer(cols=everything()) %>%
  reactable(.,  defaultPageSize = 12)
```

Looking at the AMOVA paper 
([Anderson, 2008](https://onlinelibrary.wiley.com/doi/epdf/10.1111/j.1442-9993.2001.01070.pp.x)), 
and the [vegan documentation](https://rdrr.io/rforge/vegan/man/adonis.html), 
it looks like PERMANOVA (adonis in R) wraps the permutative AMOVA function 
relatively well. I *think* the AMOVA is based on wtihin-strata permutations 
([Anderson, 2008](https://onlinelibrary.wiley.com/doi/epdf/10.1111/j.1442-9993.2001.01070.pp.x)).
So, going back to the paper (Sze & Schloss, 2016), and the methods section,

From the results, 

> We then used analysis of molecular variance (AMOVA) to test for significant 
differences between the structures of nonobese and obese individuals (Table 1).
The data sets of Escobar et al., Goodrich et al., and Turnbaugh et al. 
indicated a significant difference in community structure (all P < 0.05). 
Because it was not possible to ascertain the directionality of the difference
in community structure because the samples are arrayed in a nondimensional 
space or perform a pooled analysis using studies that had nonoverlapping 16S 
rRNA gene sequence regions, it is unclear whether these differences reflect a 
broader, but perhaps small, shift in community structure between nonobese and 
obese individuals.

From the methods,

> Next, we compared the community structure from nonobese and obese individuals
by using AMOVA with Bray-Curtis distance matrices (36). This analysis was 
performed with the vegan (version 2.3-5) R package. 

...I cant find a specifcic AMOVA in the [vegan 2.3-5 documentation]() nor can 
I find the Anderson 2008 paper in the documentation. But, going into the code,
they ran a data set-by-data set diveristy analysis, rather than combining
the data set 
([Sze_Obesity_mBio_2016/run_beta_diversity.R](https://github.com/SchlossLab/Sze_Obesity_mBio_2016/blob/master/code/run_beta_diversity.R)).

So, I think at the end of the day, they did a study-by-study parallel analysis
and didn't acutally combine or compare beta diversity. This is... *umm*.

So, think we code this as "parallel analysis" or something similar, and we'll
flag this as not other.

### 10.1186/s40168-017-0368-1

```{r, echo=FALSE}
data %>%
  select(doi,starts_with('beta_study')) %>%
  mutate(., beta_study_other = ifelse(`beta_study.other.` == '0', NA, beta_study_other)
          , `beta_study.adonis_adj.` = case_when(doi == '10.1111/bjd.20626'  ~ '0'
                                                ,doi == '10.3389/fcimb.2022.816526' ~ '0'
                                               ,.default = `beta_study.adonis_adj.`)
          , `beta_study.no_adj.` = case_when(doi == '10.1111/bjd.20626' ~ '1'
                                            ,doi == '10.1136/gutjnl-2015-310376' ~ '1'
                                            ,.default = `beta_study.no_adj.`)
          , `beta_study.other_test_adj.` = case_when(doi == '10.3389/fcimb.2022.816526' ~ '1'
                                        ,.default = `beta_study.other_test_adj.`)
          , `beta_study.other.` = case_when(doi == '10.1093/femsec/fix153' ~ '0'
                                           ,doi == '10.1038/s41531-021-00156-z'  ~ '1'
                                           ,doi == '10.3389/fcimb.2022.816526' ~ '0'
                                           ,doi == '10.1128/mbio.01018-16' ~ '0'
                                           ,.default = `beta_study.other.`)
          ,`beta_study.not_described.` = case_when(doi == '10.1186/s13059-022-02637-7' ~ '0'
                                                   ,.default = `beta_study.not_described.`)
          , `beta_study.no_inference_test.` = case_when(doi == '10.1038/s41396-020-0727-y' ~ '1'
                                                       ,doi == '10.1038/s41531-021-00156-z'  ~ '1'
                                                       ,.default = '0')
          , `beta_study.parallel.` = case_when(doi == '10.1128/mbio.01018-16' ~ '1'
                                              ,.default = '0')
         , `beta_study.no_study_pcoa.` = case_when(doi == '10.1111/bjd.20626' ~ '1'
                                                   ,doi == '10.1016/j.chom.2013.08.006' ~ '0'
                                                   ,doi == '10.1136/gutjnl-2015-310376' ~ '0'
                                                   ,doi == '10.1186/s40168-017-0248-8' ~ '0'
                                                   ,.default = `beta_study.no_study_pcoa.`)
          , `beta_study.pcoa_study_effects.` = case_when(doi == '10.1111/bjd.20626' ~ '0'
                                                        ,.default = `beta_study.pcoa_study_effects.`
                                                         )
          , `beta_study.pcoa_study_avg.` = case_when(doi == '10.1186/s13059-022-02637-7' ~ '1'
                                                    ,.default = '0')
          , `beta_study.parallel_pcoa` = case_when(doi == '10.3389/fcimb.2020.00434' ~ '1'
                                                   ,.default = '0')
          , `beta_study.pcoa_other.` = case_when(doi == '10.3389/fmicb.2021.711134' ~ '0'
                                                ,doi == '10.1186/s40168-017-0248-8' ~ '0' 
                                                ,grepl('pcoa', tolower(beta_study_other)) ~ '1'
                                                ,doi == '10.1186/s13059-022-02637-7' ~ '1'
                                                ,.default = '0')
          ) %>%
  mutate_at(., .vars=vars(starts_with('beta_study.'))
             , .funs=as.numeric) %>%
  filter(., (`beta_study.other.` == 1) | !is.na(beta_study_other)) %>%
  rename_with(., .cols=starts_with('beta_study.')
               , .fn=~str_remove(str_remove(., 'beta_study.'), '\\.')
               ) %>%
  filter(., doi == '10.1186/s40168-017-0368-1') %>%
  mutate_all(., as.character) %>%
  pivot_longer(cols=everything()) %>%
  reactable(.,  defaultPageSize = 15)
```

We see "other" as "PERMANOVA", which is a standard statistical test (*adonis* 
is the vegan implementation of PERMANVOA form 
[Anderson, 2001](https://doi.org/10.1002/9781118445112.stat07841)). 

This is a study where we have two analyses, a primary analysis from the 
atuhors, and a secondary "meta" analysis with combined data.

So, from the methods section,

> **Statistical analysis**
> ... Differences in community structure across samples (β-diversity) were 
calculated using the weighted UniFrac distance metric and visualized by 
Principal Coordinates Analysis (PCoA) plots using custom R scripts. Significant
differences in β-diversity across patient groups were evaluated using 
Permutational Multivariate Analysis of Variance (PERMANOVA) with 10$^{4}$ 
permutations. 

And then, later in the methods section, in the "meta-analysis sub section
> **Meta-analysis**
> To further check the validity of the prediction results, a meta-analysis was 
performed using recent data published by Khanna et al. [22], which also aimed 
to find microbial fingerprints predicting the risk of recurrence after 
successful treatment in patients with primary CDI (more information on both
studies can be found in Table 1).
> ...
> β-Diversity was calculated using the weighted UniFrac distance metric, and 
significant differences across patient groups were evaluated using PERMANOVA. 
For predictive models, we trained a random forest model on each individual 
dataset as well as the combined (meta) dataset. We also built the model by 
training on one dataset and using the other for cross-validation. The 
discriminatory power of OTUs, genera, and families were calculated as the area 
under the ROC curve (AUC) in each case. 

From the results and methods section:

> We combined our high-throughput 16S rRNA gene sequence data with a recent 
study by Khanna et al. [22]. Although both studies shared a common experimental 
approach, results revealed a strong study effect as the most clearly discernible
signal in the data (PERMANOVA, p value = 0.002). 

Looking in the supplement, I can't find a PCoA by study (they do a PCoA by 
timepoint but not a PCoA highlighting the main text vs Khanna et al. 

So, we're going to code this as a case where we check for a study effect, 
and no PCoA (at least no meta-analysis PCoA.)


### 10.1186/s13059-022-02637-7

```{r, echo=FALSE}
data %>%
  select(doi,starts_with('beta_study')) %>%
  mutate(., beta_study_other = ifelse(`beta_study.other.` == '0', NA, beta_study_other)
          , `beta_study.adonis_adj.` = case_when(doi == '10.1111/bjd.20626'  ~ '0'
                                                ,doi == '10.3389/fcimb.2022.816526' ~ '0'
                                               ,.default = `beta_study.adonis_adj.`)
          , `beta_study.no_adj.` = case_when(doi == '10.1111/bjd.20626' ~ '1'
                                            ,doi == '10.1136/gutjnl-2015-310376' ~ '1'
                                            ,.default = `beta_study.no_adj.`)
          , `beta_study.other_test_adj.` = case_when(doi == '10.3389/fcimb.2022.816526' ~ '1'
                                        ,.default = `beta_study.other_test_adj.`)
          , `beta_study.other.` = case_when(doi == '10.1093/femsec/fix153' ~ '0'
                                           ,doi == '10.1038/s41531-021-00156-z'  ~ '1'
                                           ,doi == '10.3389/fcimb.2022.816526' ~ '0'
                                           ,doi == '10.1128/mbio.01018-16' ~ '0'
                                           ,.default = `beta_study.other.`)
          ,`beta_study.not_described.` = case_when(doi == '10.1186/s13059-022-02637-7' ~ '0'
                                                   ,.default = `beta_study.not_described.`)
          , `beta_study.no_inference_test.` = case_when(doi == '10.1038/s41396-020-0727-y' ~ '1'
                                                       ,doi == '10.1038/s41531-021-00156-z'  ~ '1'
                                                       ,doi == '10.1186/s40168-017-0368-1' ~ '1'
                                                       ,.default = '0')
          , `beta_study.parallel.` = case_when(doi == '10.1128/mbio.01018-16' ~ '1'
                                              ,.default = '0')
         , `beta_study.no_study_pcoa.` = case_when(doi == '10.1111/bjd.20626' ~ '1'
                                                   ,doi == '10.1016/j.chom.2013.08.006' ~ '0'
                                                   ,doi == '10.1136/gutjnl-2015-310376' ~ '0'
                                                   ,doi == '10.1186/s40168-017-0248-8' ~ '0'
                                                   ,.default = `beta_study.no_study_pcoa.`)
          , `beta_study.pcoa_study_effects.` = case_when(doi == '10.1111/bjd.20626' ~ '0'
                                                        ,.default = `beta_study.pcoa_study_effects.`
                                                         )
          , `beta_study.pcoa_study_avg.` = case_when(doi == '10.1186/s13059-022-02637-7' ~ '1'
                                                    ,.default = '0')
          , `beta_study.parallel_pcoa` = case_when(doi == '10.3389/fcimb.2020.00434' ~ '1'
                                                   ,.default = '0')
          , `beta_study.pcoa_other.` = case_when(doi == '10.3389/fmicb.2021.711134' ~ '0'
                                                ,doi == '10.1186/s40168-017-0248-8' ~ '0' 
                                                ,grepl('pcoa', tolower(beta_study_other)) ~ '1'
                                                ,doi == '10.1186/s13059-022-02637-7' ~ '1'
                                                ,.default = '0')
          ) %>%
  mutate_at(., .vars=vars(starts_with('beta_study.'))
             , .funs=as.numeric) %>%
  filter(., (`beta_study.other.` == 1) | !is.na(beta_study_other)) %>%
  rename_with(., .cols=starts_with('beta_study.')
               , .fn=~str_remove(str_remove(., 'beta_study.'), '\\.')
               ) %>%
  filter(., doi == '10.1186/s13059-022-02637-7') %>%
  mutate_all(., as.character) %>%
  pivot_longer(cols=everything()) %>%
  reactable(.,  defaultPageSize = 16)
```

I think we've already addressed this. We have a study-pooled Bray Curtis measure
and I think we may just need to discuss this explicitly in the text?

### 10.1186/s40168-017-0248-8

```{r, echo=FALSE}
data %>%
  select(doi,starts_with('beta_study')) %>%
  mutate(., beta_study_other = ifelse(`beta_study.other.` == '0', NA, beta_study_other)
          , `beta_study.adonis_adj.` = case_when(doi == '10.1111/bjd.20626'  ~ '0'
                                                ,doi == '10.3389/fcimb.2022.816526' ~ '0'
                                               ,.default = `beta_study.adonis_adj.`)
          , `beta_study.no_adj.` = case_when(doi == '10.1111/bjd.20626' ~ '1'
                                            ,doi == '10.1136/gutjnl-2015-310376' ~ '1'
                                            ,.default = `beta_study.no_adj.`)
          , `beta_study.other_test_adj.` = case_when(doi == '10.3389/fcimb.2022.816526' ~ '1'
                                        ,.default = `beta_study.other_test_adj.`)
          , `beta_study.other.` = case_when(doi == '10.1093/femsec/fix153' ~ '0'
                                           ,doi == '10.1038/s41531-021-00156-z'  ~ '1'
                                           ,doi == '10.3389/fcimb.2022.816526' ~ '0'
                                           ,doi == '10.1128/mbio.01018-16' ~ '0'
                                           ,.default = `beta_study.other.`)
          ,`beta_study.not_described.` = case_when(doi == '10.1186/s13059-022-02637-7' ~ '0'
                                                   ,.default = `beta_study.not_described.`)
          , `beta_study.no_inference_test.` = case_when(doi == '10.1038/s41396-020-0727-y' ~ '1'
                                                       ,doi == '10.1038/s41531-021-00156-z'  ~ '1'
                                                       ,doi == '10.1186/s40168-017-0368-1' ~ '1'
                                                       ,.default = '0')
          , `beta_study.parallel.` = case_when(doi == '10.1128/mbio.01018-16' ~ '1'
                                              ,.default = '0')
         , `beta_study.no_study_pcoa.` = case_when(doi == '10.1111/bjd.20626' ~ '1'
                                                   ,doi == '10.1016/j.chom.2013.08.006' ~ '0'
                                                   ,doi == '10.1136/gutjnl-2015-310376' ~ '0'
                                                   ,doi == '10.1186/s40168-017-0248-8' ~ '0'
                                                   ,.default = `beta_study.no_study_pcoa.`)
          , `beta_study.pcoa_study_effects.` = case_when(doi == '10.1111/bjd.20626' ~ '0'
                                                        ,.default = `beta_study.pcoa_study_effects.`
                                                         )
          , `beta_study.pcoa_study_avg.` = case_when(doi == '10.1186/s13059-022-02637-7' ~ '1'
                                                    ,.default = '0')
          , `beta_study.parallel_pcoa` = case_when(doi == '10.3389/fcimb.2020.00434' ~ '1'
                                                   ,.default = '0')
          , `beta_study.pcoa_other.` = case_when(doi == '10.3389/fmicb.2021.711134' ~ '0'
                                                ,doi == '10.1186/s40168-017-0248-8' ~ '0' 
                                                ,grepl('pcoa', tolower(beta_study_other)) ~ '1'
                                                ,doi == '10.1186/s13059-022-02637-7' ~ '1'
                                                ,.default = '0')
          ) %>%
  mutate_at(., .vars=vars(starts_with('beta_study.'))
             , .funs=as.numeric) %>%
  filter(., (`beta_study.other.` == 1) | !is.na(beta_study_other)) %>%
  rename_with(., .cols=starts_with('beta_study.')
               , .fn=~str_remove(str_remove(., 'beta_study.'), '\\.')
               ) %>%
  filter(., doi == '10.1186/s40168-017-0248-8') %>%
  mutate_all(., as.character) %>%
  pivot_longer(cols=everything()) %>%
  reactable(.,  defaultPageSize = 15)
```

So, we have the PCoA by primer pair effect here, but they dont appear to have 
an accompanying statistical test. So, just visualization. I think we should 
flag this as poca_other, but not PCoA.

### 10.3389/fmicb.2021.711134
```{r, echo=FALSE}
data %>%
  select(doi,starts_with('beta_study')) %>%
  mutate(., beta_study_other = ifelse(`beta_study.other.` == '0', NA, beta_study_other)
          , `beta_study.adonis_adj.` = case_when(doi == '10.1111/bjd.20626'  ~ '0'
                                                ,doi == '10.3389/fcimb.2022.816526' ~ '0'
                                               ,.default = `beta_study.adonis_adj.`)
          , `beta_study.no_adj.` = case_when(doi == '10.1111/bjd.20626' ~ '1'
                                            ,doi == '10.1136/gutjnl-2015-310376' ~ '1'
                                            ,.default = `beta_study.no_adj.`)
          , `beta_study.other_test_adj.` = case_when(doi == '10.3389/fcimb.2022.816526' ~ '1'
                                        ,.default = `beta_study.other_test_adj.`)
          , `beta_study.other.` = case_when(doi == '10.1093/femsec/fix153' ~ '0'
                                           ,doi == '10.1038/s41531-021-00156-z'  ~ '1'
                                           ,doi == '10.3389/fcimb.2022.816526' ~ '0'
                                           ,doi == '10.1128/mbio.01018-16' ~ '0'
                                           ,doi == '10.1186/s40168-017-0248-8' ~ '0'
                                           ,.default = `beta_study.other.`)
          ,`beta_study.not_described.` = case_when(doi == '10.1186/s13059-022-02637-7' ~ '0'
                                                   ,.default = `beta_study.not_described.`)
          , `beta_study.no_inference_test.` = case_when(doi == '10.1038/s41396-020-0727-y' ~ '1'
                                                       ,doi == '10.1038/s41531-021-00156-z'  ~ '1'
                                                       ,doi == '10.1186/s40168-017-0368-1' ~ '1'
                                                       ,.default = '0')
          , `beta_study.parallel.` = case_when(doi == '10.1128/mbio.01018-16' ~ '1'
                                              ,.default = '0')
          , `beta_study.no_study_pcoa.` = case_when(doi == '10.1111/bjd.20626' ~ '1'
                                                   ,doi == '10.1016/j.chom.2013.08.006' ~ '0'
                                                   ,doi == '10.1136/gutjnl-2015-310376' ~ '0'
                                                   ,doi == '10.1186/s40168-017-0248-8' ~ '0'
                                                   ,.default = `beta_study.no_study_pcoa.`)
          , `beta_study.pcoa_study_effects.` = case_when(doi == '10.1111/bjd.20626' ~ '0'
                                                        ,.default = `beta_study.pcoa_study_effects.`
                                                         )
          , `beta_study.pcoa_study_avg.` = case_when(doi == '10.1186/s13059-022-02637-7' ~ '1'
                                                    ,.default = '0')
          , `beta_study.parallel_pcoa` = case_when(doi == '10.3389/fcimb.2020.00434' ~ '1'
                                                   ,.default = '0')
          , `beta_study.pcoa_other.` = case_when(doi == '10.3389/fmicb.2021.711134' ~ '0'
                                                ,doi == '10.1186/s40168-017-0248-8' ~ '0' 
                                                ,grepl('pcoa', tolower(beta_study_other)) ~ '1'
                                                ,doi == '10.1186/s13059-022-02637-7' ~ '1'
                                                ,.default = '0')
          ) %>%
  mutate_at(., .vars=vars(starts_with('beta_study.'))
             , .funs=as.numeric) %>%
  filter(., (`beta_study.other.` == 1) | !is.na(beta_study_other)) %>%
  rename_with(., .cols=starts_with('beta_study.')
               , .fn=~str_remove(str_remove(., 'beta_study.'), '\\.')
               ) %>%
  filter(., doi == '10.3389/fmicb.2021.711134') %>%
  mutate_all(., as.character) %>%
  pivot_longer(cols=everything()) %>%
  reactable(.,  defaultPageSize = 15)
```


### 10.3389/fcimb.2020.00434
```{r, echo=FALSE}
data %>%
  select(doi,starts_with('beta_study')) %>%
  mutate(., beta_study_other = ifelse(`beta_study.other.` == '0', NA, beta_study_other)
          , `beta_study.adonis_adj.` = case_when(doi == '10.1111/bjd.20626'  ~ '0'
                                                ,doi == '10.3389/fcimb.2022.816526' ~ '0'
                                               ,.default = `beta_study.adonis_adj.`)
          , `beta_study.no_adj.` = case_when(doi == '10.1111/bjd.20626' ~ '1'
                                            ,doi == '10.1136/gutjnl-2015-310376' ~ '1'
                                            ,.default = `beta_study.no_adj.`)
          , `beta_study.other_test_adj.` = case_when(doi == '10.3389/fcimb.2022.816526' ~ '1'
                                        ,.default = `beta_study.other_test_adj.`)
          , `beta_study.other.` = case_when(doi == '10.1093/femsec/fix153' ~ '0'
                                           ,doi == '10.1038/s41531-021-00156-z'  ~ '1'
                                           ,doi == '10.3389/fcimb.2022.816526' ~ '0'
                                           ,doi == '10.1128/mbio.01018-16' ~ '0'
                                           ,.default = `beta_study.other.`)
          ,`beta_study.not_described.` = case_when(doi == '10.1186/s13059-022-02637-7' ~ '0'
                                                   ,.default = `beta_study.not_described.`)
          , `beta_study.no_inference_test.` = case_when(doi == '10.1038/s41396-020-0727-y' ~ '1'
                                                       ,doi == '10.1038/s41531-021-00156-z'  ~ '1'
                                                       ,doi == '10.1186/s40168-017-0368-1' ~ '1'
                                                       ,.default = '0')
          , `beta_study.parallel.` = case_when(doi == '10.1128/mbio.01018-16' ~ '1'
                                              ,.default = '0')
         , `beta_study.no_study_pcoa.` = case_when(doi == '10.1111/bjd.20626' ~ '1'
                                                   ,doi == '10.1016/j.chom.2013.08.006' ~ '0'
                                                   ,doi == '10.1136/gutjnl-2015-310376' ~ '0'
                                                   ,doi == '10.1186/s40168-017-0248-8' ~ '0'
                                                   ,.default = `beta_study.no_study_pcoa.`)
          , `beta_study.pcoa_study_effects.` = case_when(doi == '10.1111/bjd.20626' ~ '0'
                                                        ,.default = `beta_study.pcoa_study_effects.`
                                                         )
          , `beta_study.pcoa_study_avg.` = case_when(doi == '10.1186/s13059-022-02637-7' ~ '1'
                                                    ,.default = '0')
          , `beta_study.parallel_pcoa` = case_when(doi == '10.3389/fcimb.2020.00434' ~ '1'
                                                   ,.default = '0')
          , `beta_study.pcoa_other.` = case_when(doi == '10.3389/fmicb.2021.711134' ~ '0'
                                                ,doi == '10.1186/s40168-017-0248-8' ~ '0' 
                                                ,grepl('pcoa', tolower(beta_study_other)) ~ '1'
                                                ,doi == '10.1186/s13059-022-02637-7' ~ '1'
                                                ,.default = '0')
          ) %>%
  mutate_at(., .vars=vars(starts_with('beta_study.'))
             , .funs=as.numeric) %>%
  filter(., (`beta_study.other.` == 1) | !is.na(beta_study_other)) %>%
  rename_with(., .cols=starts_with('beta_study.')
               , .fn=~str_remove(str_remove(., 'beta_study.'), '\\.')
               ) %>%
  filter(., doi == '10.3389/fcimb.2020.00434') %>%
  mutate_all(., as.character) %>%
  pivot_longer(cols=everything()) %>%
  reactable(.,  defaultPageSize = 15)
```

They're essentially doing a parallel analysis here. From results:

> Composition of the Gut Microbiome Associated With HIV+ and MSM Status
> 
> We explored the potential influence of HIV and MSM status on the composition 
of the gut microbiome, according to the PERMANOVA test of ecological distances. 
PCoA ordination plots of Bray–Curtis showed that samples from works by 
Lozupone et al. (2013) (R2 = 0.154, FDR p < 0.001), Dubourg et al. (2016) 
(R2 = 0.080, FDR p < 0.001), and Armstrong et al. (2018) (R2 = 0.059, FDR 
p < 0.001) were significantly clustered according to HIV status (Figure 6, 
Table 4). The samples from works by Noguera-Julian et al. (2016) (R2 = 0.122, 
FDR p < 0.001), Vesterbacka et al. (2017) (R2 = 0.081, FDR p < 0.001), 
Armstrong et al. (2018) (R2 = 0.090, FDR p < 0.001), and Li et al. (2019) 
(R2 = 0.115, FDR p < 0.001) showed better clustering according to MSM status 
rather than HIV status (Figure 7, Table 5).

Checking their methods, they used methods

> For principal coordinates analysis (PCoA), distance matrices were calculated 
using the Bray–Curtis, Jensen–Shannon divergence, and Jaccard ecological 
dissimilarity indexes. The permutational multivariate analysis of variance 
(PERMANOVA) test was performed on this distance matrix.

So, I think i'd code this as a "parallel" analysis. Which is not great when
it comes to beta diversity.

### 10.1038/ismej.2013.54
```{r, echo=FALSE}
data %>%
  select(doi,starts_with('beta_study')) %>%
  mutate(., beta_study_other = ifelse(`beta_study.other.` == '0', NA, beta_study_other)
          , `beta_study.adonis_adj.` = case_when(doi == '10.1111/bjd.20626'  ~ '0'
                                                ,doi == '10.3389/fcimb.2022.816526' ~ '0'
                                               ,.default = `beta_study.adonis_adj.`)
          , `beta_study.no_adj.` = case_when(doi == '10.1111/bjd.20626' ~ '1'
                                            ,doi == '10.1136/gutjnl-2015-310376' ~ '1'
                                            ,.default = `beta_study.no_adj.`)
          , `beta_study.other_test_adj.` = case_when(doi == '10.3389/fcimb.2022.816526' ~ '1'
                                        ,.default = `beta_study.other_test_adj.`)
          , `beta_study.other.` = case_when(doi == '10.1093/femsec/fix153' ~ '0'
                                           ,doi == '10.1038/s41531-021-00156-z'  ~ '1'
                                           ,doi == '10.3389/fcimb.2022.816526' ~ '0'
                                           ,doi == '10.1128/mbio.01018-16' ~ '0'
                                           ,.default = `beta_study.other.`)
          ,`beta_study.not_described.` = case_when(doi == '10.1186/s13059-022-02637-7' ~ '0'
                                                   ,.default = `beta_study.not_described.`)
          , `beta_study.no_inference_test.` = case_when(doi == '10.1038/s41396-020-0727-y' ~ '1'
                                                       ,doi == '10.1038/s41531-021-00156-z'  ~ '1'
                                                       ,doi == '10.1186/s40168-017-0368-1' ~ '1'
                                                       ,.default = '0')
          , `beta_study.parallel.` = case_when(doi == '10.1128/mbio.01018-16' ~ '1'
                                              ,doi == '10.3389/fcimb.2020.00434' ~ '1'
                                              ,.default = '0')
          , `beta_study.no_study_pcoa.` = case_when(doi == '10.1111/bjd.20626' ~ '1'
                                                   ,doi == '10.1016/j.chom.2013.08.006' ~ '0'
                                                   ,doi == '10.1136/gutjnl-2015-310376' ~ '0'
                                                   ,doi == '10.1186/s40168-017-0248-8' ~ '0'
                                                   ,.default = `beta_study.no_study_pcoa.`)
          , `beta_study.pcoa_study_effects.` = case_when(doi == '10.1111/bjd.20626' ~ '0'
                                                        ,.default = `beta_study.pcoa_study_effects.`
                                                         )
          , `beta_study.pcoa_study_avg.` = case_when(doi == '10.1186/s13059-022-02637-7' ~ '1'
                                                    ,.default = '0')
          , `beta_study.parallel_pcoa` = case_when(doi == '10.3389/fcimb.2020.00434' ~ '1'
                                                   ,.default = '0')
          , `beta_study.pcoa_other.` = case_when(doi == '10.3389/fmicb.2021.711134' ~ '0'
                                                ,doi == '10.1186/s40168-017-0248-8' ~ '0' 
                                                ,grepl('pcoa', tolower(beta_study_other)) ~ '1'
                                                ,doi == '10.1186/s13059-022-02637-7' ~ '1'
                                                ,.default = '0')
          ) %>%
  mutate_at(., .vars=vars(starts_with('beta_study.'))
             , .funs=as.numeric) %>%
  filter(., (`beta_study.other.` == 1) | !is.na(beta_study_other)) %>%
  rename_with(., .cols=starts_with('beta_study.')
               , .fn=~str_remove(str_remove(., 'beta_study.'), '\\.')
               ) %>%
  filter(., doi == '10.1038/ismej.2013.54') %>%
  mutate_all(., as.character) %>%
  pivot_longer(cols=everything()) %>%
  reactable(.,  defaultPageSize = 15)
```

This is one of those multi-enviromental studies, whcih I think I'm going to code
as multi-enviromnet. From the abstract:

> Ecologists have long studied the temporal dynamics of plant and animal 
communities with much less attention paid to the temporal dynamics exhibited by
microbial communities. As a result, we do not know if overarching temporal 
trends exist for microbial communities or if changes in microbial communities
are generally predictable with time. Using microbial time series assessed via 
high-throughput sequencing, we conducted a meta-analysis of temporal dynamics 
in microbial communities, including 76 sites representing air, aquatic, soil,
brewery wastewater treatment, human- and plant-associated microbial biomes.
We found that temporal variability in both within- and between-community 
diversity was consistent among microbial communities from similar environments.
Community structure changed systematically with time in less than half of the 
cases, and the highest rates of change were observed within ranges of 1 day to 
1 month for all communities examined. Microbial communities exhibited species–
time relationships (STRs), which describe the accumulation of new taxa to a 
community, similar to those observed previously for plant and animal 
communities, suggesting that STRs are remarkably consistent across a broad 
range of taxa. These results highlight that a continued integration of microbial
ecology into the broader field of ecology will provide new insight into the 
temporal patterns of microbial and ‘macro’-bial communities alike.

So, I think I'm going to leave this uncoded, knowing that we're not actually
going to work iwth the summary data at the end of the project and will just
code it as "pan-enviromental". 

### 10.1186/s13059-019-1908-8

```{r, echo=FALSE}
data %>%
  select(doi,starts_with('beta_study')) %>%
  mutate(., beta_study_other = ifelse(`beta_study.other.` == '0', NA, beta_study_other)
          , `beta_study.adonis_adj.` = case_when(doi == '10.1111/bjd.20626'  ~ '0'
                                                ,doi == '10.3389/fcimb.2022.816526' ~ '0'
                                               ,.default = `beta_study.adonis_adj.`)
          , `beta_study.no_adj.` = case_when(doi == '10.1111/bjd.20626' ~ '1'
                                            ,doi == '10.1136/gutjnl-2015-310376' ~ '1'
                                            ,.default = `beta_study.no_adj.`)
          , `beta_study.other_test_adj.` = case_when(doi == '10.3389/fcimb.2022.816526' ~ '1'
                                        ,.default = `beta_study.other_test_adj.`)
          , `beta_study.other.` = case_when(doi == '10.1093/femsec/fix153' ~ '0'
                                           ,doi == '10.1038/s41531-021-00156-z'  ~ '1'
                                           ,doi == '10.3389/fcimb.2022.816526' ~ '0'
                                           ,doi == '10.1128/mbio.01018-16' ~ '0'
                                           ,.default = `beta_study.other.`)
          ,`beta_study.not_described.` = case_when(doi == '10.1186/s13059-022-02637-7' ~ '0'
                                                   ,.default = `beta_study.not_described.`)
          , `beta_study.no_inference_test.` = case_when(doi == '10.1038/s41396-020-0727-y' ~ '1'
                                                       ,doi == '10.1038/s41531-021-00156-z'  ~ '1'
                                                       ,doi == '10.1186/s40168-017-0368-1' ~ '1'
                                                       ,.default = '0')
          , `beta_study.parallel.` = case_when(doi == '10.1128/mbio.01018-16' ~ '1'
                                              ,doi == '10.3389/fcimb.2020.00434' ~ '1'
                                              ,.default = '0')
          , `beta_study.no_study_pcoa.` = case_when(doi == '10.1111/bjd.20626' ~ '1'
                                                   ,doi == '10.1016/j.chom.2013.08.006' ~ '0'
                                                   ,doi == '10.1136/gutjnl-2015-310376' ~ '0'
                                                   ,doi == '10.1186/s40168-017-0248-8' ~ '0'
                                                   ,.default = `beta_study.no_study_pcoa.`)
          , `beta_study.pcoa_study_effects.` = case_when(doi == '10.1111/bjd.20626' ~ '0'
                                                        ,.default = `beta_study.pcoa_study_effects.`
                                                         )
          , `beta_study.pcoa_study_avg.` = case_when(doi == '10.1186/s13059-022-02637-7' ~ '1'
                                                    ,.default = '0')
          , `beta_study.parallel_pcoa.` = case_when(doi == '10.3389/fcimb.2020.00434' ~ '1'
                                                   ,.default = '0')
          , `beta_study.pcoa_other.` = case_when(doi == '10.3389/fmicb.2021.711134' ~ '0'
                                                ,doi == '10.1186/s40168-017-0248-8' ~ '0' 
                                                ,grepl('pcoa', tolower(beta_study_other)) ~ '1'
                                                ,doi == '10.1186/s13059-022-02637-7' ~ '1'
                                                ,.default = '0')
          ) %>%
  mutate_at(., .vars=vars(starts_with('beta_study.'))
             , .funs=as.numeric) %>%
  filter(., (`beta_study.other.` == 1) | !is.na(beta_study_other)) %>%
  rename_with(., .cols=starts_with('beta_study.')
               , .fn=~str_remove(str_remove(., 'beta_study.'), '\\.')
               ) %>%
  filter(., doi == '10.1186/s13059-019-1908-8') %>%
  mutate_all(., as.character) %>%
  pivot_longer(cols=everything()) %>%
  reactable(.,  defaultPageSize = 15)
```

This is another pan environmental paper. I think I'd tag it as not other,
if we're already coding as "not adjusted" because adonis is one fo the many 
statistical tests we could apply? So, I'll code it as other is false?

## Tidy up

I want to look at what kind of PCoAs we're seeing. 

```{r, results='hide'}
cleaned[['beta_analysis']] <- 
  data %>%
  select(doi, `analyses_perf.beta.`, starts_with('beta_study'), `envo.other.`) %>%
  mutate(., beta_study_other = ifelse(`beta_study.other.` == '0', NA, beta_study_other)
          , `beta_study.adonis_adj.` = case_when(doi == '10.1111/bjd.20626'  ~ '0'
                                                ,doi == '10.3389/fcimb.2022.816526' ~ '0'
                                                ,doi == '10.1186/s13059-022-02637-7' ~ '1'
                                               ,.default = `beta_study.adonis_adj.`)
          , `beta_study.no_adj.` = case_when(doi == '10.1111/bjd.20626' ~ '1'
                                            ,doi == '10.1136/gutjnl-2015-310376' ~ '1'
                                            ,.default = `beta_study.no_adj.`)
          , `beta_study.other_test_adj.` = case_when(doi == '10.3389/fcimb.2022.816526' ~ '1'
                                        ,.default = `beta_study.other_test_adj.`)
          , `beta_study.other.` = case_when(doi == '10.1093/femsec/fix153' ~ '0'
                                           ,doi == '10.1038/s41531-021-00156-z'  ~ '1'
                                           ,doi == '10.3389/fcimb.2022.816526' ~ '0'
                                           ,doi == '10.1128/mbio.01018-16' ~ '0'
                                           ,doi == '10.1186/s13059-019-1908-8' ~ '0'
                                           ,.default = `beta_study.other.`)
          ,`beta_study.not_described.` = case_when(doi == '10.1186/s13059-022-02637-7' ~ '0'
                                                   ,.default = `beta_study.not_described.`)
          , `beta_study.no_inference_test.` = case_when(doi == '10.1038/s41396-020-0727-y' ~ '1'
                                                       ,doi == '10.1038/s41531-021-00156-z'  ~ '1'
                                                       ,doi == '10.1186/s40168-017-0368-1' ~ '1'
                                                       ,.default = '0')
          , `beta_study.parallel.` = case_when(doi == '10.1128/mbio.01018-16' ~ '1'
                                              ,doi == '10.3389/fcimb.2020.00434' ~ '1'
                                              ,.default = '0')
         , `beta_study.no_study_pcoa.` = case_when(doi == '10.1111/bjd.20626' ~ '1'
                                                   ,doi == '10.1016/j.chom.2013.08.006' ~ '0'
                                                   ,doi == '10.1136/gutjnl-2015-310376' ~ '0'
                                                   ,doi == '10.1186/s40168-017-0248-8' ~ '0'
                                                   ,.default = `beta_study.no_study_pcoa.`)
          , `beta_study.pcoa_study_effects.` = case_when(doi == '10.1111/bjd.20626' ~ '0'
                                                        ,.default = `beta_study.pcoa_study_effects.`
                                                         )
          , `beta_study.pcoa_study_avg.` = case_when(doi == '10.1186/s13059-022-02637-7' ~ '1'
                                                    ,.default = '0')
          , `beta_study.parallel_pcoa.` = case_when(doi == '10.3389/fcimb.2020.00434' ~ '1'
                                                   ,.default = '0')
          , `beta_study.pcoa_other.` = case_when(doi == '10.3389/fmicb.2021.711134' ~ '0'
                                                ,doi == '10.1186/s40168-017-0248-8' ~ '0' 
                                                ,grepl('pcoa', tolower(beta_study_other)) ~ '1'
                                                ,doi == '10.1186/s13059-022-02637-7' ~ '1'
                                                ,.default = '0')
          ) %>%
  mutate_at(., .vars=vars(`analyses_perf.beta.`, starts_with('beta_study.'), `envo.other.`)
             , .funs=as.numeric) %>%
  mutate_at(., .vars=vars(starts_with('beta_study.'))
             , .funs=~ifelse(`analyses_perf.beta.` == 0, 0, .)
             ) %>%
  mutate(., beta_pcoa_code = ((`beta_study.no_study_pcoa.` * 2) +
                             (`beta_study.pcoa_study_effects.` * 4) + 
                             (`beta_study.pcoa_study_avg.` * 8) + 
                             (`beta_study.parallel_pcoa.` * 16) + 
                             (`beta_study.not_described.` * 64))
          , beta_stat_code =  ((`beta_study.no_adj.` * 2) + 
                               (`beta_study.adonis_adj.` * 4) + 
                               (`beta_study.other_test_adj.` * 8) +
                               (`beta_study.parallel.` * 16) + 
                               (`beta_study.no_inference_test.` * 32) +
                               (`beta_study.not_described.` * 64))
          ) %>%
  mutate_at(., .vars=vars(ends_with('code'))
             , .funs=~ifelse(. > 0, ., `analyses_perf.beta.`)) %>%
  mutate(., beta_poca_label = case_when(`analyses_perf.beta.` == 0 ~ 'no beta'
                                       ,beta_pcoa_code == 1 ~ 'no ordination'
                                       ,beta_pcoa_code == 2 ~ 'no ordination by dataset'
                                       ,beta_pcoa_code == 4 ~ 'ordination showing dataset effects'
                                       ,beta_pcoa_code == 6 ~ 'both - Walters'
                                       ,beta_pcoa_code == 12 ~ 'ordination of dataset average'
                                       ,beta_pcoa_code == 16 ~ 'parallel ordination by dataset'
                                       ,beta_pcoa_code == 64 ~ 'not described'
                                       ,.default = as.character(beta_pcoa_code))
          , beta_stat_label = case_when(`analyses_perf.beta.` == 0 ~ 'no beta'
                                       ,beta_stat_code == 1 ~ 'no statistical test'
                                       ,beta_stat_code == 2 ~ 'no dataset effect acknoweldged'
                                       ,beta_stat_code == 4 ~ 'adonis accounting for dataset effect'
                                       ,beta_stat_code == 8 ~ 'other test accounting for dataset effect'
                                       ,beta_stat_code == 16 ~ 'parallel test by dataset'
                                       ,beta_stat_code == 32 ~ 'no inference test; test by study'
                                       ,beta_stat_code == 64 ~ 'not described')
              ) %>%
  select(c(doi, starts_with('beta_')))
```

# Differential Abundance: taxonomy

With differential abundance, we had a *whole* series of questions, including
the level where the analysis was conducted, whether the analysis was targeted
or untargeted (targeted analyses may be easier to compare because you're 
looking for approximately the same thing), how the data was filtered, and
how study effects were handled. We also got a list of the tools which I may
cross reference with the study coding to make sure that the modeling makes
sense and potentially matches the capability of the cited tool (e.g. we
can't adjust for a study effect in a univariate technique like LefSe; ANCOM
doesn't let you pool p-values because ANCOM is based on a W statistic which 
isn't actually a p-value, etc.)

I'm going to start by looking at the taxonomic levels.


For the differential abundance analysis, we asked

* At which taxonomic levels was differential abundance analyzed? 

With possible "select all" levels of

| Column | Response Text |
|:---|:---|
| `diff_abund_level[otu_asv]` | OTU/ASV |
| `diff_abund_level[species]` | Species |
| `diff_abund_level[genus]` | Genus |
| `diff_abund_level[family]` | Family |
| `diff_abund_level[order]` | Family |
| `diff_abund_level[class]` | Class |
| `diff_abund_level[phylum]` | Phylum |
| `diff_abund_level[not_described]` | Not described |

We'll start by checking missing data, and then making sure that if our flag
our taxonomic level as not described, we're not missing another taxonomic
level.

#### Missing data

```{r, echo=FALSE}
data %>% 
  column_to_rownames('doi') %>%
  select(., starts_with('diff_abund_level.')) %>%
  mutate_at(., .vars=vars(starts_with('diff_abund_level.'))
             , .funs=~!(. %in% c('0', '1'))) %>%
  summarise(across(everything(), sum)) %>%
  reactable()
```
So, all the taxonomic levels are defined here. Yay!

#### Described/not described

And then, we'll look at whether the data is not described with a level defined
or else missing a taxonomic level.

```{r, echo=FALSE}
data %>% 
  column_to_rownames('doi') %>%
  select(., `analyses_perf.differenital_abundance.`, starts_with('diff_abund_level.')) %>%
  mutate_at(., .vars=vars(starts_with('diff_abund_level.'))
             , .funs=~ifelse(`analyses_perf.differenital_abundance.` %in% c('0', 0), 0, as.numeric(.))
             ) %>%
  mutate(., defined_level = (`diff_abund_level.otu_asv.` +
                             `diff_abund_level.species.` +
                             `diff_abund_level.genus.` +
                             `diff_abund_level.family.` +
                             `diff_abund_level.order.` +
                             `diff_abund_level.class.` +
                             `diff_abund_level.phylum.`)
          , mismatch = ((`diff_abund_level.not_described.` == 1) & (defined_level > 0))
          , should_have = (defined_level == 0) & !((`diff_abund_level.not_described.` == 1) | 
                                                 (`analyses_perf.differenital_abundance.` == '0'))
         ) %>%
  filter(., mismatch | should_have) %>%
  reactable()
```

So, everything is both defined and we have level information for. So, we're good
on differential abundance levels.

## Tidy data

```{r, result='hide'}
cleaned[['da_level']] <-
  data %>% 
  select(c(doi, `analyses_perf.differenital_abundance.`, starts_with('diff_abund_level'))) %>%
  mutate_at(., .vars=vars(`analyses_perf.differenital_abundance.`, starts_with('diff_abund_level'))
             , .funs=as.numeric) %>%
  mutate_at(., .vars=vars(starts_with('diff_abund_level'))
             , .funs=~ifelse(`analyses_perf.differenital_abundance.` == 0, 0, .)
             ) %>%
  rowwise() %>%
  mutate(., diff_level_min = max(`diff_abund_level.otu_asv.` * 8
                                 ,`diff_abund_level.species.` * 7
                                 ,`diff_abund_level.genus.` * 6 
                                 ,`diff_abund_level.family.` * 5
                                 ,`diff_abund_level.order.` * 4
                                 ,`diff_abund_level.class.` * 3
                                 ,`diff_abund_level.phylum.` * 2          
                                 ,`diff_abund_level.not_described.` * 9)
          ) %>%
  ungroup() %>%
  mutate(., da_level_min = case_when(`analyses_perf.differenital_abundance.` == 0 ~ 'No DA'
                                      ,diff_level_min == 8 ~ 'feature'
                                      ,diff_level_min == 7 ~ 'species'
                                      ,diff_level_min == 6 ~ 'genus'
                                      ,diff_level_min == 5 ~ 'family'
                                      ,diff_level_min == 4 ~ 'order'
                                      ,diff_level_min == 3 ~ 'class'
                                      ,diff_level_min == 2 ~ 'phylum'
                                      ,diff_level_min == 9 ~ 'not described')
         , da_level2 = case_when(da_level_min == 'feature' ~ 1
                                  ,da_level_min %in% c('not described', 'No DA') ~ NA
                                  ,.default = 0)
          ) %>%
  rename_with(., .cols=starts_with('diff_abund_level')
               , .fn=~str_replace(., 'diff_abund_', 'da_')) %>%
  select(c(doi, starts_with('da_level')))
```

# Differential abundance: Target

We asked whether targeted or untargeted differential abundance was performed:

* What types of microbiome differential abundance testing was performed? 

It was a "select all" question, with two possibilities:

| Column | Response Text |
|:---|:---|
| `diff_target[targeted]` | Differential abundance of a targeted taxa (IE is F. nucleatum higher in all the studies) |
| `diff_target[untargeted]` | Untargeted differential abundance |

## Missing data

```{r, echo=FALSE}
data %>% 
  column_to_rownames('doi') %>%
  select(., starts_with('diff_target.')) %>%
  mutate_at(., .vars=vars(starts_with('diff_target.'))
             , .funs=~!(. %in% c('0', '1'))
             ) %>%
  summarise(across(everything(), sum)) %>%
  reactable()
```

We're not missing any data for the differential abundance target, so we can
move forward.

## Tidy Data

```{r, result='hide'}
cleaned[['da_target']] <-
  data %>% 
  select(c(doi, `analyses_perf.differenital_abundance.`, starts_with('diff_target.'))) %>%
  mutate_at(., .vars=vars(`analyses_perf.differenital_abundance.`, starts_with('diff_target.'))
             , .funs=as.numeric) %>%
  mutate_at(., .vars=vars(starts_with('diff_target.'))
             , .funs=~ifelse(`analyses_perf.differenital_abundance.` == 0, 0, .)
             ) %>%
  rename_with(., .cols=starts_with('diff_target')
               , .fn=~str_replace(., 'diff_', 'da_')) %>%
  mutate(., da_target_code = ((`da_target.targeted.` * 2) + 
                              (`da_target.untargeted.` * 4))
          , da_target_label = case_when(`analyses_perf.differenital_abundance.` == 0 ~ 'no DA'
                                       ,da_target_code == 2 ~ 'targeted DA only'
                                       ,da_target_code == 4 ~ 'untargeted DA only'
                                       ,da_target_code == 6 ~ 'both targeted and untargeted DA'
                                       ,da_target_code == 0 ~ 'not described'
                                       ,.default = as.character(da_target_code))
          ) %>%
  select(c(doi, starts_with('da_target')))
```

# Differential Abundance: Data Filtering


We're looking at the differentail abundance filter, where we ask how the
data was filtered to do the differential abundance analysis. 

The question was:

* How was the data filtered before differential abundance?

and then it was a select all question with multiple answers:

| Column | Response Text |
|:---|:---|
| `diff_filter[no_filtered]` | The features were not filtered |
| `diff_filter[per_study]` | The filter was applied individually in each study |
| `diff_filter[consistent]` | The filter was applied across all studies |
| `diff_filter[prevelance]` | the features were filtered based on prevalence |
| `diff_filter[abundance]` | The features were filtered based on abundance |
| `diff_filter[taxonomy_meta]` | The features were filtered based on taxonomic assignment or other information |

## Missing data

We'll start by checking how many rows have missing values for the column.

```{r, echo=FALSE}
data %>% 
  column_to_rownames('doi') %>%
  select(.,  `analyses_perf.differenital_abundance.`, starts_with('diff_filter.')) %>%
  mutate_at(., .vars=vars(starts_with('diff_filter.'))
             , .funs=~ifelse( `analyses_perf.differenital_abundance.` == '0', '0', .)
             ) %>%
  mutate_at(., .vars=vars(starts_with('diff_filter.'))
             , .funs=~!(. %in% c('0', '1'))
             ) %>%
  select(c(starts_with('diff_filter.'))) %>%
  summarise(across(everything(), sum)) %>%
  pivot_longer(everything()) %>%
  reactable()
```

```{r, echo=FALSE}
data %>% 
  column_to_rownames('doi') %>%
  select(.,  `analyses_perf.differenital_abundance.`, starts_with('diff_filter.')) %>%
  mutate_at(., .vars=vars(starts_with('diff_filter.'))
             , .funs=~ifelse( `analyses_perf.differenital_abundance.` == '0', '0', .)
             ) %>%
  filter(., (!(`diff_filter.per_study.` %in% c('0', '1')) | 
             !(`diff_filter.consistent.` %in% c('0', '1')) | 
             !(`diff_filter.taxonomy_meta.` %in% c('0', '1')))
             ) %>%
  select(starts_with('diff_filter.')) %>%
  rename_with(., .cols=starts_with('diff_filter.')
               , .fn=~str_remove(str_remove(., 'diff_filter.'), '\\.')) %>%
  mutate_at(., .vars=vars(everything())
             , .funs=~ifelse(is.na(.), 'NA', as.character(.))) %>%
  reactable()
```


So, we have two studies that need to be reviewed.

### 10.1111/bjd.20626

The differential abundance for this paper was only performed for the main study,
it wasn't performed for the combined analysis protion. So, I think this study
should be coded as all 0s, becuase I don't think differential abundance in this
study *qualifies* for the analysis. So, I'm going to code this all as 0, and
maybe code the analysis as not performed.

### 10.1128/msystems.00797-20

Looking at the methods, there are two data sets included in the methods, the
author's Japanesse data set and an additional German data set.

> Our Japanese data set was comprised of 26 iRBD patients and 137 healthy 
controls, whereas the German data set was comprised of 20 iRBD patients and 
38 healthy controls (43). We first collated the experimental methods and 
demographic features (see Table S7 in the supplemental material), as well 
as statistical measures of sequencing depths (see Table S8 in the supplemental
material) of the two data sets. The read count of each sample was all more than
10,000 in the two data sets, and no sample was excluded from our meta-analysis.
For each taxon, we counted the number of samples in which the relative 
abundance of the taxon was more than 1E−4. We then filtered 39 families and 132
genera, in which the number of such samples was more than 10% (17/163 and 6/58)
in both data sets.
 
So, I think based on the methods, they did consistent filtering across the 
studies and filtered based on prevelance and abundance. (I'd describe this)
as joint filtering, but this is a combined filter?

## Tidy data

```{r, results='hide'}
cleaned[['analyses_performed']] <- 
  cleaned[['analyses_performed']] %>%
  mutate(., `analyses_perf.differenital_abundance.` = case_when(doi == '10.1111/bjd.20626' ~ 0
                                                               ,.default=`analyses_perf.differenital_abundance.`)
          )
```
```{r, results='hide'}
cleaned[['da_filter']] <-
  data %>%
  select(c(doi, `analyses_perf.differenital_abundance.`, contains('diff_filter.'))) %>%
  mutate(., `analyses_perf.differenital_abundance.` = case_when(doi == '10.1111/bjd.20626' ~ '0'
                                                               ,.default=`analyses_perf.differenital_abundance.`)
          , `diff_filter.per_study.` = case_when(doi == '10.1128/msystems.00797-20' ~ '0'
                                                ,.default = `diff_filter.per_study.`)
          , `diff_filter.prevelance.` = case_when(doi == '10.1128/msystems.00797-20' ~ '1'
                                                 ,.default = `diff_filter.prevelance.`)
          , `diff_filter.abundance.` = case_when(doi == '10.1128/msystems.00797-20' ~ '1' 
                                                 ,.default = `diff_filter.abundance.`)
          ) %>%
  mutate_at(., .vars=vars(c(`analyses_perf.differenital_abundance.`, starts_with('diff_filter.')))
             , .funs=as.numeric
             ) %>%
  mutate_at(., .vars=vars(starts_with('diff_filter.'))
             , .funs=~ifelse(`analyses_perf.differenital_abundance.` == 0, 0, .)
             ) %>%
  select(c(doi, starts_with('diff'))) %>%
  rename_with(., .cols=starts_with('diff'), .fn=~str_replace(., 'diff_', 'da_'))
```

# Differential Abundance: Analyses Performed
We asked

* ow were multiple studies handled in differential abundance testing?

Which was a select all with possible responses:

| Column  | Response Text |
| :--- | :--- |
| `diff_type[pool_no_adj]`  | The studies were pooled with no affect adjustment |
| `diff_type[fixed_effect]` | Pooled analysis adjusted for study or technical effect (fixed effect) |
| `diff_type[random_effect]` | Pooled analysis with study as a random effect |
| `diff_type[meta_analysis]` | A comparison of results across individual cohorts (i.e. forest plot, effect pooling) |
| `diff_type[not_described]` | Not described |
| `diff_type[other]`  | Other |
| `diff_type_other` | *other text* |

Some of these are mutually exclusive, so I want to see how they're coded?

I think we can also use the differential abundance test method to re-enforce
some of these?

## Missing data

### Method

```{r, echo=FALSE}
data %>% 
  select(c(doi, `analyses_perf.differenital_abundance.`, starts_with('diff_type.'))) %>%
  mutate(., `analyses_perf.differenital_abundance.` = ifelse(doi == '10.1111/bjd.20626', '0',
                                                             `analyses_perf.differenital_abundance.`)) %>%
  mutate_at(., .vars=vars(starts_with('diff_type'))
            , .funs=~ifelse(`analyses_perf.differenital_abundance.` %in% c(0, '0'), '0', .)
            ) %>%
  mutate_at(., .vars=vars(starts_with('diff_type'))
             , .funs=~!(. %in% c('0', '1'))) %>%
  summarise(across(.cols=starts_with('diff'), sum)) %>%
  reactable()
```

So, there is no missing data in this coding.

### Algorithm

```{r, echo=FALSE}
data %>% 
  select(c(doi, `analyses_perf.differenital_abundance.`, starts_with('diff_algorith.'))) %>%
  mutate(., `analyses_perf.differenital_abundance.` = ifelse(doi == '10.1111/bjd.20626', '0',
                                                             `analyses_perf.differenital_abundance.`)) %>%
  column_to_rownames('doi') %>%
  mutate_at(., .vars=vars(starts_with('diff_algorith.'))
             , .funs=~ifelse(`analyses_perf.differenital_abundance.` %in% c(0, '0'), '0', .)
            ) %>%
  select(-c(`analyses_perf.differenital_abundance.`)) %>%
  mutate_at(., .vars=vars(starts_with('diff_algorith.'))
             , .funs=~(!(. %in% c('0', '1'))) * 1) %>%
  summarise(across(starts_with('diff_algorith.'), sum)) %>%
  pivot_longer(cols=starts_with('diff_al')) %>%
  reactable(.,  defaultPageSize = 15)
```

So, I want to check where the methods are missing

```{r, echo=FALSE}
data %>%
  select(c(doi, `analyses_perf.differenital_abundance.`, starts_with('diff_algorith'))) %>%
  mutate(., `analyses_perf.differenital_abundance.` = ifelse(doi == '10.1111/bjd.20626', '0',
                                                             `analyses_perf.differenital_abundance.`)) %>%
  mutate_at(., .vars=vars(starts_with('diff_algorith.'))
             , .funs=~ifelse(`analyses_perf.differenital_abundance.` %in% c(0, '0'), '0', .)
             ) %>%
  select(c(doi, `diff_algorith.LefSe.`, `diff_algorith.other.`, `diff_algorith_other`)) %>%
  filter(.,!(`diff_algorith.LefSe.` %in% c('0', '1')) | !(`diff_algorith.other.` %in% c('0', '1'))) %>%
  reactable()
  #colnames()
```



So, we have two articles to review?

#### 10.1111/1462-2920.13632

```{r, echo=FALSE}
data %>%
  mutate(., `analyses_perf.differenital_abundance.` = ifelse(doi == '10.1111/bjd.20626', '0',
                                                             `analyses_perf.differenital_abundance.`)) %>%
  mutate_at(., .vars=vars(starts_with('diff_algorith.'), starts_with('diff_type'))
             , .funs=~ifelse(`analyses_perf.differenital_abundance.` %in% c(0, '0'), '0', .)
             ) %>%
  mutate_at(., .vars=vars(starts_with('diff_type'), starts_with('diff_alg'))
             , .funs=~ifelse(is.na(.), 'NA', .)) %>%
  select(doi, starts_with('diff_alg')) %>%
  filter(., doi == '10.1111/1462-2920.13632') %>%
  pivot_longer(cols=-c(doi)) %>%
  pivot_wider(id_cols = name, names_from = doi, values_from = value) %>%
  reactable(.,  defaultPageSize = 15)
```

So, let's see if there is LefSe, and whether the LEfSe correlates with the 
Wilcoxon rank sum test. (The kruskal-wallis test is an expanded case of the 
Wilcoxon rank-sum test for multiple groups). LefSe is also a special case of 
the Wilcoxon test, where the test is applied, and then threshholded based 
on an effect size.

From the methods section,

> Linear Discriminant Analysis (LDA) Effect Size (LEfSe) was used to determine 
which phylotype(s) differentiated CRS from healthy subjects (Segata et al., 
2011). LEfSe analysis was performed using a non-parametric factorial Kruskal–
Wallis (KW) sum-rank test with an alpha value of ≤0.05, followed by the 
(unpaired) Wilcoxon rank-sum with an alpha score of ≤0.05, and a one-against-all
strategy for multi-class analysis. LEfSe analysis was performed in mothur using 
1000 independent subsampling iterations of the collapsed phylotype table. 
The results of individual LEfSe iterations were joined using a custom script, 
and only those phylotypes with an LDA threshold above 2.0 and consistently 
significant across CRS or healthy subjects were plotted.

So, I think interpeting this, we have LefSe, and I wouldn't code this as an
"other" method?

#### 10.1128/msystems.00138-20

```{r, echo=FALSE}
data %>%
  mutate(., `analyses_perf.differenital_abundance.` = ifelse(doi == '10.1111/bjd.20626', '0',
                                                             `analyses_perf.differenital_abundance.`)) %>%
  mutate_at(., .vars=vars(starts_with('diff_algorith.'), starts_with('diff_type'))
             , .funs=~ifelse(`analyses_perf.differenital_abundance.` %in% c(0, '0'), '0', .)
             ) %>%
  mutate(., `diff_algorith.LefSe.` = case_when(doi == '10.1111/1462-2920.13632' ~ '1'
                                              ,.default=`diff_algorith.LefSe.`)
          , `diff_algorith.rare_kw.` = case_when(doi == '10.1111/1462-2920.13632' ~ '0'
                                                ,.default = `diff_algorith.rare_kw.`) 
          , `diff_algorith.other.` = case_when(doi == '10.1111/1462-2920.13632' ~ '0'
                                              ,.default = `diff_algorith.other.`)
    
           ) %>%
  mutate_at(., .vars=vars(starts_with('diff_type'), starts_with('diff_alg'))
             , .funs=~ifelse(is.na(.), 'NA', .)) %>%
  select(doi, starts_with('diff_alg')) %>%
  filter(., doi == '10.1128/msystems.00138-20') %>%
  pivot_longer(cols=-c(doi)) %>%
  pivot_wider(id_cols = name, names_from = doi, values_from = value) %>%
  reactable(.,  defaultPageSize = 15)
```
So, we're missing coding on "other" but there's also not anything listed for
other, so I think that it makes sense to have "other" as 0.

## Not described and describe data

I want to look at whether the data or algorithm were described but the method
is listed as "not described". I want to look at the places where we have a 
differential abundance algorithm that's defined and we don't have a defination
for either the appraoch or the algorith.

```{r, echo=FALSE}
data %>%
  mutate(., `analyses_perf.differenital_abundance.` = ifelse(doi == '10.1111/bjd.20626', '0',
                                                             `analyses_perf.differenital_abundance.`)) %>%
  mutate_at(., .vars=vars(starts_with('diff_algorith.'), starts_with('diff_type'))
             , .funs=~ifelse(`analyses_perf.differenital_abundance.` %in% c(0, '0'), '0', .)
             ) %>%
  mutate(., `diff_algorith.LefSe.` = case_when(doi == '10.1111/1462-2920.13632' ~ '1'
                                              ,.default=`diff_algorith.LefSe.`)
          , `diff_algorith.rare_kw.` = case_when(doi == '10.1111/1462-2920.13632' ~ '0'
                                                ,.default = `diff_algorith.rare_kw.`) 
          , `diff_algorith.other.` = case_when(doi == '10.1111/1462-2920.13632' ~ '0'
                                              ,doi == '10.1128/msystems.00138-20' ~ '0'
                                              ,.default = `diff_algorith.other.`)
    
           ) %>%
  mutate_at(., .vars=vars(starts_with('diff_algorith.'), starts_with('diff_type.'))
             , .funs=as.numeric) %>%
  select(c(doi, starts_with('diff_type.'),  starts_with('diff_algorith.'))) %>%
    mutate(., `diff_type.described.` = ((`diff_type.pool_no_adj.` +
                                       `diff_type.fixed_effect.` + 
                                       `diff_type.random_effect.` + 
                                       `diff_type.meta_analysis.` + 
                                       `diff_type.other.`) > 0) * 1
            , `diff_algorith.described` = ((`diff_algorith.parametric_rarefied.` + 
                                            `diff_algorith.rare_kw.` + 
                                            `diff_algorith.LefSe.` +
                                            `diff_algorith.DeSeq2.` +
                                            `diff_algorith.MaAslin.`  +
                                            `diff_algorith.metagenomSeq.` +
                                            `diff_algorith.limma_voom.`  +
                                            `diff_algorith.parametric_clr.` + 
                                            `diff_algorith.aldex2.` +
                                            `diff_algorith.ancom.` +
                                            `diff_algorith.ancom_bc.` +
                                            `diff_algorith.dr.` +
                                            `diff_algorith.other.`) > 0) * 1
            ) %>%
  filter(., ((`diff_type.not_described.` == 1)  & ((`diff_type.described.` == 1) | 
                                                   (`diff_algorith.described` == 1))) | 
            ((`diff_algorith.not_described.` == 1) & (`diff_algorith.described` == 1))
            ) %>%
  select(c(doi, `diff_type.not_described.`, `diff_type.described.`, 
           `diff_algorith.not_described.`, `diff_algorith.described`)) %>%
  reactable()
```

### 10.1007/s00248-018-1176-2

```{r, echo=FALSE}
data %>%
  mutate(., `analyses_perf.differenital_abundance.` = ifelse(doi == '10.1111/bjd.20626', '0',
                                                             `analyses_perf.differenital_abundance.`)) %>%
  mutate_at(., .vars=vars(starts_with('diff_algorith.'), starts_with('diff_type'))
             , .funs=~ifelse(`analyses_perf.differenital_abundance.` %in% c(0, '0'), '0', .)
             ) %>%
  mutate(., `diff_algorith.LefSe.` = case_when(doi == '10.1111/1462-2920.13632' ~ '1'
                                              ,.default=`diff_algorith.LefSe.`)
          , `diff_algorith.rare_kw.` = case_when(doi == '10.1111/1462-2920.13632' ~ '0'
                                                ,.default = `diff_algorith.rare_kw.`) 
          , `diff_algorith.other.` = case_when(doi == '10.1111/1462-2920.13632' ~ '0'
                                              ,doi == '10.1128/msystems.00138-20' ~ '0'
                                              ,.default = `diff_algorith.other.`)
    
           ) %>%
  filter(., doi=='10.1007/s00248-018-1176-2') %>%
  select(c(doi, starts_with('diff_type'), starts_with('diff_al'))) %>%
  pivot_longer(cols=starts_with('diff')) %>%
  mutate(., value = ifelse(is.na(value), 'N/A', value)) %>%
  filter(., !(value %in% c('0', 'N/A'))) %>%
  pivot_wider(id_cols = name, names_from = doi, values_from = value) %>%
  reactable()
```

This was a paper where they did an analysis in a primary data set, and then
went in and did targeted differential abundance in their population and 
a follow up. From the description of their primary analysis:

>  All the reads were used for normalization, and the relative abundances were 
calculated by dividing the taxa abundance with the total number of reads in each
sample. The genus abundance tables were analyzed for the identification of 
discriminating genera using LEfSe and Boruta. The identification of 
significantly different phylum, families, and genus between ASD and healthy 
children was performed using Wilcoxon rank sum test. The P values were corrected
for multiple comparisons using false discovery rates (FDRs). Differentially 
abundant taxa were highlighted using GraPhlan [35]. 

The results section highlights the use of the secondary population 
("meta-analysis") on **targeted** differential abundance:

> **Comparative Meta-Analysis of Two Different Populations Revealed *Lactobacillus* to Be Significantly Associated with Autism**
> 
> To identify autism-specific marker taxa (irrespective of diet and geographical
location), a meta-analysis approach was carried out by comparing our dataset 
with a similar dataset of ASD children from the US population (SRA053656 data). 
A total of 6156 OTUs (≥10 abundance) were obtained from SRA053656 dataset [25].
The OTUs of ASD and healthy samples from the Indian (this study) and US datasets
were combined to construct a combined OTU table using the closed-reference OTU
picking protocol of QIIME. The combined dataset from the Indian and US 
population contained 9031 OTUs. The abundance of different genera was calculated
from these OTUs. The bias arising due to the differences in sequencing depths 
was nullified by using normalized abundance from each sample. Among the 
commonly present genera in both (USA and India) the populations, Wilcoxon rank
sum test revealed genus Lactobacillus to be significantly higher (FDR adjusted 
P<0.01) in ASD children as compared to healthy children in both the 
populations (Fig. 5).

I think it's worth checking what we said about filtering for this data set, as
well, although I think its important to disentangle the **meta-analysis** from
the main analysis.


```{r, echo=FALSE}
data %>%
  mutate(., `analyses_perf.differenital_abundance.` = ifelse(doi == '10.1111/bjd.20626', '0',
                                                             `analyses_perf.differenital_abundance.`)) %>%
  mutate_at(., .vars=vars(starts_with('diff_algorith.'), starts_with('diff_type'))
             , .funs=~ifelse(`analyses_perf.differenital_abundance.` %in% c(0, '0'), '0', .)
             ) %>%
  mutate(., `diff_algorith.LefSe.` = case_when(doi == '10.1111/1462-2920.13632' ~ '1'
                                              ,.default=`diff_algorith.LefSe.`)
          , `diff_algorith.rare_kw.` = case_when(doi == '10.1111/1462-2920.13632' ~ '0'
                                                ,.default = `diff_algorith.rare_kw.`) 
          , `diff_algorith.other.` = case_when(doi == '10.1111/1462-2920.13632' ~ '0'
                                              ,doi == '10.1128/msystems.00138-20' ~ '0'
                                              ,.default = `diff_algorith.other.`)
    
           ) %>%
  filter(., doi=='10.1007/s00248-018-1176-2') %>%
  select(c(doi, starts_with('diff_tar'))) %>%
  pivot_longer(cols=starts_with('diff_tar')) %>%
  pivot_wider(id_cols = name, names_from = doi, values_from = value) %>%
  reactable(.,  defaultPageSize = 15)
```

Again, looking at the manuscript, we have a targeted analysis for the 
**meta-analysis** with kruskal-wallis. And our methodology appears to be a
parallel analysis?

```{r, results='hide'}
cleaned[['da_target']] <- 
  cleaned[['da_target']] %>%
  mutate(., `da_target.targeted.` = case_when(doi == '10.1111/bjd.20626' ~ 0
                                               ,doi == '10.1007/s00248-018-1176-2' ~ 1
                                               ,.default = `da_target.targeted.`)
          , `da_target.untargeted.` = case_when(doi == '10.1111/bjd.20626' ~ 0
                                                 ,doi == '10.1007/s00248-018-1176-2' ~ 0
                                                 ,.default = `da_target.untargeted.`)
          )
```

### 10.1111/1462-2920.13632

```{r, echo=FALSE}
data %>%
  mutate(., `analyses_perf.differenital_abundance.` = ifelse(doi == '10.1111/bjd.20626', '0',
                                                             `analyses_perf.differenital_abundance.`)) %>%
  mutate_at(., .vars=vars(starts_with('diff_algorith.'), starts_with('diff_type'))
             , .funs=~ifelse(`analyses_perf.differenital_abundance.` %in% c(0, '0'), '0', .)
             ) %>%
  mutate(., `diff_algorith.LefSe.` = case_when(doi == '10.1111/1462-2920.13632' ~ '1'
                                              ,doi == '10.1007/s00248-018-1176-2' ~ '1'
                                              ,.default=`diff_algorith.LefSe.`)
          , `diff_algorith.rare_kw.` = case_when(doi == '10.1111/1462-2920.13632' ~ '0'
                                                ,doi == '10.1007/s00248-018-1176-2' ~ '1'
                                                ,.default = `diff_algorith.rare_kw.`) 
          , `diff_algorith.other.` = case_when(doi == '10.1111/1462-2920.13632' ~ '0'
                                              ,doi == '10.1128/msystems.00138-20' ~ '0'
                                              ,doi == '10.1007/s00248-018-1176-2' ~ '0'
                                              ,.default = `diff_algorith.other.`)
          , `diff_type.parallel.` = case_when(doi == '10.1007/s00248-018-1176-2' ~ '1'
                                             ,.default = '0')
          , `diff_type.not_described.` = case_when(doi == '10.1007/s00248-018-1176-2' ~ '0'
                                                 ,.default = `diff_type.not_described.`)
    
           ) %>%
  filter(., doi=='10.1111/1462-2920.13632') %>%
  select(c(doi, starts_with('diff_type'), starts_with('diff_al'))) %>%
  pivot_longer(cols=starts_with('diff')) %>%
  mutate(., value = ifelse(is.na(value), 'N/A', value)) %>%
  filter(., !(value %in% c('0', 'N/A'))) %>%
  pivot_wider(id_cols = name, names_from = doi, values_from = value) %>%
  reactable()
```

So, the method is not described, but the algorithm is given as LefSe, so 
let's check the paper?

The data set here refers to the pooled data across all their studies, which were
then processed together. From their methods section, we get that LefSe 
was perfromed?

> Linear Discriminant Analysis (LDA) Effect Size (LEfSe) was used to determine
which phylotype(s) differentiated CRS from healthy subjects (Segata et al., 
2011). LEfSe analysis was performed using a non-parametric factorial 
Kruskal–Wallis (KW) sum-rank test with an alpha value of ≤0.05, followed by the
(unpaired) Wilcoxon rank-sum with an alpha score of ≤0.05, and a one-against-all
strategy for multi-class analysis. LEfSe analysis was performed in mothur using 
1000 independent subsampling iterations of the collapsed phylotype table. 
The results of individual LEfSe iterations were joined using a custom script, 
and only those phylotypes with an LDA threshold above 2.0 and consistently 
significant across CRS or healthy subjects were plotted.

And then, from the results:

> Linear discriminant analysis effect size (LDA-LEfSe) revealed three phylotypes
with an LDA score of at least 2.0 that were significantly more abundant in 
healthy samples than in CRS patients throughout 1000 subsampling iterations 
(Fig. 3). All of the phylotypes differentiating healthy bacterial communities 
from those with CRS were members of the phylum Actinobacteria: namely members 
of the genus *Propionibacterium*, members of an unclassified phylotype of 
*Actinobacteria* and unclassified *Corynebacteriaceae*. Only one genus level 
phylotype, *Corynebacterium*, was identified as a potential biomarker of 
CRS-associated sinonasal microbiota.

When I look through the rest of the paper and the supplemental material,
there is no discussion of study effects anywhere.

So, I think we should code this as a differential abundance method does not 
consider the study effect, and is very much described.


### 10.1371/journal.pone.0062578

This is a study comparing the microbiome in mouse strains and the microbiome in
human studies. They focused on their mouse samples, and then added an 
independent human data set for validation(?)

```{r, echo=FALSE}
data %>%
  mutate(., `analyses_perf.differenital_abundance.` = ifelse(doi == '10.1111/bjd.20626', '0',
                                                             `analyses_perf.differenital_abundance.`)) %>%
  mutate_at(., .vars=vars(starts_with('diff_algorith.'), starts_with('diff_type'))
             , .funs=~ifelse(`analyses_perf.differenital_abundance.` %in% c(0, '0'), '0', .)
             ) %>%
  mutate(., `diff_algorith.LefSe.` = case_when(doi == '10.1111/1462-2920.13632' ~ '1'
                                              ,doi == '10.1007/s00248-018-1176-2' ~ '1'
                                              ,.default=`diff_algorith.LefSe.`)
          , `diff_algorith.rare_kw.` = case_when(doi == '10.1111/1462-2920.13632' ~ '0'
                                                ,doi == '10.1007/s00248-018-1176-2' ~ '1'
                                                ,.default = `diff_algorith.rare_kw.`) 
          , `diff_algorith.other.` = case_when(doi == '10.1111/1462-2920.13632' ~ '0'
                                              ,doi == '10.1128/msystems.00138-20' ~ '0'
                                              ,doi == '10.1007/s00248-018-1176-2' ~ '0'
                                              ,.default = `diff_algorith.other.`)
          , `diff_type.parallel.` = case_when(doi == '10.1007/s00248-018-1176-2' ~ '1'
                                             ,.default = '0')
          , `diff_type.not_described.` = case_when(doi == '10.1111/1462-2920.13632' ~ '0'
                                                  ,doi == '10.1007/s00248-018-1176-2' ~ '0'
                                                  ,.default = `diff_type.not_described.`)
          , `diff_type.pool_no_adj.` = case_when(doi == '10.1111/1462-2920.13632' ~ '1'
                                                ,.default = `diff_type.pool_no_adj.`)
    
           ) %>%
  filter(., doi=='10.1371/journal.pone.0062578') %>%
  select(c(doi, starts_with('diff_type'), starts_with('diff_al'))) %>%
  pivot_longer(cols=starts_with('diff')) %>%
  mutate(., value = ifelse(is.na(value), 'N/A', value)) %>%
  filter(., !(value %in% c('0', 'N/A'))) %>%
  reactable(.,  defaultPageSize = 15)
```

From the methods section, the samples were collected as:

> **Dataset**
> 
A total of 88 mice and 128 human (16 individuals with each individual sampled 
twice within 6 weeks and each sample sequenced 4 times) GM profiles determined 
using tag-encoded 16S rRNA gene 454/FLX Titanium (Roche) pyro-sequencing were 
included in the study (Table 1). All samples enrolled in the present 
meta-analysis have been treated according to the same protocols concerning 
DNA extraction, library preparation and sequencing [6], [14]. 
> ...
> All animal experiments were carried out in accordance with the Council of 
Europe Convention European Treaty Series (ETS) 123 on the Protection of 
Vertebrate Animals used for Experimental and Other Scientific Purposes, and the 
Danish Animal Experimentation Act (LBK 1306 from 23/11/2007). The study was
approved by the Animal Experiments Inspectorate, Ministry of Justice, Denmark.
> 
> Human specimens used in this meta-analysis come from the independent study 
that was approved by The Scientific Ethics Committee of Capital Region, Denmark
(reference H-4-2010-137). Written informed consent was obtained from volunteers
prior to recruitment.

And then we have the statistical analysis: 

> **Statistics**
> 
> ... 
> 
> Differences in taxa abundances at phylum and genus level between categories 
were verified with Metastats (http://metastats.cbcb.umd.edu). From each group 
14 samples (corresponding to the smallest category) were randomly chosen and 
combinations of group pairs were tested using 1000 permutations (p value 
threshold=0.05; false discovery rate threshold=0.5). The relationship between 
sequencing depth and shared GM, classified into phylum and genus level, 
between mice and humans was plotted based on multiple subsampled OTU tables 
composed of two categories collecting 794988 human and 714440 mouse GM 16S 
rRNA gene reads. Simulation of each sequencing depth was repeated 100 times 
and an average proportion of shared taxonomic groups between the two categories
were calculated (abundance threshold for unshared taxa=0.19%).

I'm not sure if I would have coded this as a "meta-analysis" since the data
was collected and processed all together, but it's microbiome (\*jazz hands\*).
Table S2 and Table S3 give pairwise p-values across groups. We're not seeing
a discussion of "dataset" here, although I feel like this is a case where the 
"data set effect" is confounded with an exposure group (similar to 
`10.1111/bjd.20626`). So, I think if we describe it this way, we can say we're
ignoring the effects. 

As a note, I found the metastat article by 
[White, Nagarajan, and Pop, 2009](10.1371/journal.pcbi.1000352). This
approaches differential abundance as (i) transform into relative abundance 
space and (ii) a t-test on the normalized data. 

So, I think we can say that differential abundance was not adjusted, and
described? (Although not done well.)


### 10.1016/j.gpb.2018.03.009

The paper looks at the association between cystic fibrosis associated pathogens
and the airway microbiome structure in participants with ADHD.

For our coding,

```{r, echo=FALSE}
data %>%
  mutate(., `analyses_perf.differenital_abundance.` = ifelse(doi == '10.1111/bjd.20626', '0',
                                                             `analyses_perf.differenital_abundance.`)) %>%
  mutate_at(., .vars=vars(starts_with('diff_algorith.'), starts_with('diff_type'))
             , .funs=~ifelse(`analyses_perf.differenital_abundance.` %in% c(0, '0'), '0', .)
             ) %>%
  mutate(., `diff_algorith.LefSe.` = case_when(doi == '10.1111/1462-2920.13632' ~ '1'
                                              ,doi == '10.1007/s00248-018-1176-2' ~ '1'
                                              ,.default=`diff_algorith.LefSe.`)
          , `diff_algorith.rare_kw.` = case_when(doi == '10.1111/1462-2920.13632' ~ '0'
                                                ,doi == '10.1007/s00248-018-1176-2' ~ '1'
                                                ,.default = `diff_algorith.rare_kw.`) 
          , `diff_algorith.other.` = case_when(doi == '10.1111/1462-2920.13632' ~ '0'
                                              ,doi == '10.1128/msystems.00138-20' ~ '0'
                                              ,doi == '10.1007/s00248-018-1176-2' ~ '0'
                                              ,.default = `diff_algorith.other.`)
          , `diff_type.parallel.` = case_when(doi == '10.1007/s00248-018-1176-2' ~ '1'
                                             ,.default = '0')
          , `diff_type.not_described.` = case_when(doi == '10.1111/1462-2920.13632' ~ '0'
                                                  ,doi == '10.1371/journal.pone.0062578' ~ '0'
                                                  ,doi == '10.1007/s00248-018-1176-2' ~ '0'
                                                  ,.default = `diff_type.not_described.`)
          , `diff_type.pool_no_adj.` = case_when(doi == '10.1111/1462-2920.13632' ~ '1'
                                                ,doi == '10.1371/journal.pone.0062578' ~ '1'
                                                ,.default = `diff_type.pool_no_adj.`)
    
           ) %>%
  filter(., doi=='10.1016/j.gpb.2018.03.009') %>%
  select(c(doi, starts_with('diff_type'), starts_with('diff_al'))) %>%
  pivot_longer(cols=starts_with('diff')) %>%
  mutate(., value = ifelse(is.na(value), 'N/A', value)) %>%
  filter(., !(value %in% c('0', 'N/A'))) %>%
  pivot_wider(id_cols = name, names_from = doi, values_from = value) %>%
  reactable(.,  defaultPageSize = 15)
```

The authors identify a study effect in Figure 2A. They then used a functional
prediction mechanism to predict the metabolic function (redox capacity) of the
CF lung microbiome. They then looked at the association between those predicted
redox pathways and metabolic networks. I think it could be argued that some of
the work with redox potential coudl be differential abundance, but that feels
more like differential abundance of a pathway than of taxa. So, either 
describe this as "targeted" or its not DA, and I'm leaning toward not DA?


```{r, results='hide'}
cleaned[['da_target']] <- 
  cleaned[['da_target']] %>%
  mutate(., `da_target.targeted.` = case_when(doi == '10.1111/bjd.20626' ~ 0
                                             ,doi == '10.1016/j.gpb.2018.03.009' ~ 0
                                             ,doi == '10.1007/s00248-018-1176-2' ~ 1
                                             ,.default = `da_target.targeted.`)
          , `da_target.untargeted.` = case_when(doi == '10.1111/bjd.20626' ~ 0
                                               ,doi == '10.1016/j.gpb.2018.03.009' ~ 0
                                               ,doi == '10.1007/s00248-018-1176-2' ~ 0
                                               ,.default = `da_target.untargeted.`)
          )
```
```{r, results='hide'}
cleaned[['analyses_performed']] <-
  cleaned[['analyses_performed']] %>%
  mutate(., `analyses_perf.differenital_abundance.` = case_when(doi == '10.1016/j.gpb.2018.03.009' ~ 0
                                                               ,.default = `analyses_perf.differenital_abundance.`)
          )
```

### 10.1038/s41598-018-32413-2

```{r, echo=FALSE}
data %>%
  mutate(., `analyses_perf.differenital_abundance.` = case_when(doi == '10.1111/bjd.20626' ~ '0'
                                                               ,doi == '10.1016/j.gpb.2018.03.009' ~ '0'
                                                               ,.default=`analyses_perf.differenital_abundance.`)) %>%
  mutate_at(., .vars=vars(starts_with('diff_algorith.'), starts_with('diff_type'))
             , .funs=~ifelse(`analyses_perf.differenital_abundance.` %in% c(0, '0'), '0', .)
             ) %>%
  mutate(., `diff_algorith.LefSe.` = case_when(doi == '10.1111/1462-2920.13632' ~ '1'
                                              ,doi == '10.1007/s00248-018-1176-2' ~ '1'
                                              ,.default=`diff_algorith.LefSe.`)
          , `diff_algorith.rare_kw.` = case_when(doi == '10.1111/1462-2920.13632' ~ '0'
                                                ,doi == '10.1007/s00248-018-1176-2' ~ '1'
                                                ,.default = `diff_algorith.rare_kw.`) 
          , `diff_algorith.other.` = case_when(doi == '10.1111/1462-2920.13632' ~ '0'
                                              ,doi == '10.1128/msystems.00138-20' ~ '0'
                                              ,doi == '10.1007/s00248-018-1176-2' ~ '0'
                                              ,.default = `diff_algorith.other.`)
          , `diff_type.parallel.` = case_when(doi == '10.1007/s00248-018-1176-2' ~ '1'
                                             ,.default = '0')
          , `diff_type.not_described.` = case_when(doi == '10.1111/1462-2920.13632' ~ '0'
                                                  ,doi == '10.1007/s00248-018-1176-2' ~ '0'
                                                  ,doi == '10.1371/journal.pone.0062578' ~ '0'
                                                  ,.default = `diff_type.not_described.`)
          , `diff_type.pool_no_adj.` = case_when(doi == '10.1111/1462-2920.13632' ~ '1'
                                                ,doi == '10.1371/journal.pone.0062578' ~ '1'
                                                ,.default = `diff_type.pool_no_adj.`)
    
           ) %>%
  filter(., doi=='10.1038/s41598-018-32413-2') %>%
  select(c(doi, starts_with('diff_type'), starts_with('diff_al'))) %>%
  pivot_longer(cols=starts_with('diff')) %>%
  mutate(., value = ifelse(is.na(value), 'N/A', value)) %>%
  filter(., !(value %in% c('0', 'N/A'))) %>%
  pivot_wider(id_cols = name, names_from = doi, values_from = value) %>%
  reactable(.,  defaultPageSize = 15)
```

So, we code the differential abundance algorithm as "other", but it doesn't
actually list the "other" category.

Going back to the original article, we're looking for markers of polyps in
colorectal cancer. This is an article with a primary data set that was
analyzed and then, at least according to the abstract,

> Moreover, we performed a meta-analysis involving the reconstructed microbiota
composition of adenomatous polyps and publicly available metagenomics datasets 
of colorectal cancer. These analyses allowed the identification of microbial 
taxa such as Faecalibacterium, Bacteroides and Romboutsia, which appear to be 
depleted in cancerogenic mucosa as well as in adenomatous polyps, thus 
representing novel microbial biomarkers associated with early tumor formation. 
Furthermore, an absolute quantification of Fusubacterium nucleatum in polyps 
further compounded the important role of this microorganism as a valuable 
putative microbial biomarker for early diagnosis of colorectal cancer.

The methods section _really_ doesn't describe the sources of the other 
data sets, but we see differential abundance in Figure S3. I'd argue the 
differential abundance algorithm is a parametric test on relative abundance
data, which I guess is an "other" category?

And then based on Figure 3, it looks like its a parallel analysis but not
pooling fo the data?

### 10.1186/s40168-017-0248-8

```{r, echo=FALSE}
data %>%
  mutate(., `analyses_perf.differenital_abundance.` = case_when(doi == '10.1111/bjd.20626' ~ '0'
                                                               ,doi == '10.1016/j.gpb.2018.03.009' ~ '0'
                                                               ,.default=`analyses_perf.differenital_abundance.`)) %>%
  mutate_at(., .vars=vars(starts_with('diff_algorith.'), starts_with('diff_type'))
             , .funs=~ifelse(`analyses_perf.differenital_abundance.` %in% c(0, '0'), '0', .)
             ) %>%
  mutate(., `diff_algorith.LefSe.` = case_when(doi == '10.1111/1462-2920.13632' ~ '1'
                                              ,doi == '10.1007/s00248-018-1176-2' ~ '1'
                                              ,.default=`diff_algorith.LefSe.`)
          , `diff_algorith.rare_kw.` = case_when(doi == '10.1111/1462-2920.13632' ~ '0'
                                                ,doi == '10.1007/s00248-018-1176-2' ~ '1'
                                                ,.default = `diff_algorith.rare_kw.`) 
          , `diff_algorith.other.` = case_when(doi == '10.1111/1462-2920.13632' ~ '0'
                                              ,doi == '10.1128/msystems.00138-20' ~ '0'
                                              ,doi == '10.1007/s00248-018-1176-2' ~ '0'
                                              ,.default = `diff_algorith.other.`)
          , `diff_type.parallel.` = case_when(doi == '10.1007/s00248-018-1176-2' ~ '1'
                                             ,doi == '10.1038/s41598-018-32413-2' ~ '1'
                                             ,.default = '0')
          , `diff_type.not_described.` = case_when(doi == '10.1111/1462-2920.13632' ~ '0'
                                                  ,doi == '10.1007/s00248-018-1176-2' ~ '0'
                                                  ,doi == '10.1371/journal.pone.0062578' ~ '0'
                                                  ,doi == '10.1038/s41598-018-32413-2' ~ '0'
                                                  ,.default = `diff_type.not_described.`)
          , `diff_type.pool_no_adj.` = case_when(doi == '10.1111/1462-2920.13632' ~ '1'
                                                ,doi == '10.1371/journal.pone.0062578' ~ '1'
                                                ,.default = `diff_type.pool_no_adj.`)
          ,diff_algorith_other = case_when(doi == '10.1038/s41598-018-32413-2' ~ 'parametric on relative abundance'
                                          ,.default = diff_algorith_other)
    
           ) %>%
  filter(., doi=='10.1186/s40168-017-0248-8') %>%
  select(c(doi, starts_with('diff_type'), starts_with('diff_al'))) %>%
  pivot_longer(cols=starts_with('diff')) %>%
  mutate(., value = ifelse(is.na(value), 'N/A', value)) %>%
  filter(., !(value %in% c('0', 'N/A'))) %>%
  pivot_wider(id_cols = name, names_from = doi, values_from = value) %>%
  reactable(.,  defaultPageSize = 15)
```

From the methods section, 

> We also utilized a negative binomial regression model to evaluate the degree 
to which number of species (our dependent variable) could be explained by 
gestational age, group (NEC vs. control), and a gestational age by group 
interaction term.
> ...
> Differences in taxonomic relative abundance were evaluated with Mann-Whitney 
or Kruskal-Wallis tests and Benjamini-Hochberg false discovery rate correction.

So, I think the differential abundance type is relatively well described? We
do see a parallel test looking at the difference for the untargeted levels.

But, also, its _phylum_ level. So, I think we can tag this as described,
and keep it as is?

### 10.3389/fcimb.2022.816526

```{r, echo=FALSE}
data %>%
  mutate(., `analyses_perf.differenital_abundance.` = case_when(doi == '10.1111/bjd.20626' ~ '0'
                                                               ,doi == '10.1016/j.gpb.2018.03.009' ~ '0'
                                                               ,.default=`analyses_perf.differenital_abundance.`)) %>%
  mutate_at(., .vars=vars(starts_with('diff_algorith.'), starts_with('diff_type'))
             , .funs=~ifelse(`analyses_perf.differenital_abundance.` %in% c(0, '0'), '0', .)
             ) %>%
  mutate(., `diff_algorith.LefSe.` = case_when(doi == '10.1111/1462-2920.13632' ~ '1'
                                              ,doi == '10.1007/s00248-018-1176-2' ~ '1'
                                              ,.default=`diff_algorith.LefSe.`)
          , `diff_algorith.rare_kw.` = case_when(doi == '10.1111/1462-2920.13632' ~ '0'
                                                ,doi == '10.1007/s00248-018-1176-2' ~ '1'
                                                ,.default = `diff_algorith.rare_kw.`) 
          , `diff_algorith.other.` = case_when(doi == '10.1111/1462-2920.13632' ~ '0'
                                              ,doi == '10.1128/msystems.00138-20' ~ '0'
                                              ,doi == '10.1007/s00248-018-1176-2' ~ '0'
                                              ,.default = `diff_algorith.other.`)
          , `diff_type.parallel.` = case_when(doi == '10.1007/s00248-018-1176-2' ~ '1'
                                             ,doi == '10.1038/s41598-018-32413-2' ~ '1'
                                             ,.default = '0')
          , `diff_type.not_described.` = case_when(doi == '10.1111/1462-2920.13632' ~ '0'
                                                  ,doi == '10.1007/s00248-018-1176-2' ~ '0'
                                                  ,doi == '10.1371/journal.pone.0062578' ~ '0'
                                                  ,doi == '10.1038/s41598-018-32413-2' ~ '0'
                                                  ,doi == '10.1186/s40168-017-0248-8' ~ '0'
                                                  ,.default = `diff_type.not_described.`)
          , `diff_type.pool_no_adj.` = case_when(doi == '10.1111/1462-2920.13632' ~ '1'
                                                ,doi == '10.1371/journal.pone.0062578' ~ '1'
                                                ,.default = `diff_type.pool_no_adj.`)
          ,diff_algorith_other = case_when(doi == '10.1038/s41598-018-32413-2' ~ 'parametric on relative abundance'
                                          ,.default = diff_algorith_other)
    
           ) %>%
  filter(., doi=='10.3389/fcimb.2022.816526') %>%
  select(c(doi, starts_with('diff_type'), starts_with('diff_al'))) %>%
  pivot_longer(cols=starts_with('diff')) %>%
  mutate(., value = ifelse(is.na(value), 'N/A', value)) %>%
  filter(., !(value %in% c('0', 'N/A'))) %>%
  pivot_wider(id_cols = name, names_from = doi, values_from = value) %>%
  reactable(.,  defaultPageSize = 15)
```

The paper compares "biomarkers" in type 2 diabetes. Again, the differential
abundance method is described here (we see if as LefSe). From the methods, 
we see:

>  In identifying T2DM versus healthy controls, the Wilcoxon rank sum test was 
used to determine statistical differences between groups, considering that 
there were only two groups which did not follow a normal distribution. 

And then, from the results:

> To further dissect the presence of significantly different bacteria 
between T2DM patients and control individuals, a linear discriminant analysis 
(LDA) Effect Size (LEfSe) analysis was performed from the phylum to the OTU 
level. The Kruskal-Wallis rank-sum test was conducted for OTUs with LDA 
scores > 2, which depicted a significant difference (p < 0.05) in some OTUs 
between T2DM patients and healthy controls (Figure 3C).

So, I think the method is described

### 10.1016/j.csbj.2020.08.028

```{r, echo=FALSE}
data %>%
  mutate(., `analyses_perf.differenital_abundance.` = case_when(doi == '10.1111/bjd.20626' ~ '0'
                                                               ,doi == '10.1016/j.gpb.2018.03.009' ~ '0'
                                                               ,.default=`analyses_perf.differenital_abundance.`)) %>%
  mutate_at(., .vars=vars(starts_with('diff_algorith.'), starts_with('diff_type'))
             , .funs=~ifelse(`analyses_perf.differenital_abundance.` %in% c(0, '0'), '0', .)
             ) %>%
  mutate(., `diff_algorith.LefSe.` = case_when(doi == '10.1111/1462-2920.13632' ~ '1'
                                              ,doi == '10.1007/s00248-018-1176-2' ~ '1'
                                              ,.default=`diff_algorith.LefSe.`)
          , `diff_algorith.rare_kw.` = case_when(doi == '10.1111/1462-2920.13632' ~ '0'
                                                ,doi == '10.1007/s00248-018-1176-2' ~ '1'
                                                ,.default = `diff_algorith.rare_kw.`) 
          , `diff_algorith.other.` = case_when(doi == '10.1111/1462-2920.13632' ~ '0'
                                              ,doi == '10.1128/msystems.00138-20' ~ '0'
                                              ,doi == '10.1007/s00248-018-1176-2' ~ '0'
                                              ,.default = `diff_algorith.other.`)
          , `diff_type.parallel.` = case_when(doi == '10.1007/s00248-018-1176-2' ~ '1'
                                             ,doi == '10.1038/s41598-018-32413-2' ~ '1'
                                             ,.default = '0')
          , `diff_type.not_described.` = case_when(doi == '10.1111/1462-2920.13632' ~ '0'
                                                  ,doi == '10.1007/s00248-018-1176-2' ~ '0'
                                                  ,doi == '10.1371/journal.pone.0062578' ~ '0'
                                                  ,doi == '10.1038/s41598-018-32413-2' ~ '0'
                                                  ,doi == '10.1186/s40168-017-0248-8' ~ '0'
                                                  ,doi == '10.3389/fcimb.2022.816526' ~ '0'
                                                  ,.default = `diff_type.not_described.`)
          , `diff_type.pool_no_adj.` = case_when(doi == '10.1111/1462-2920.13632' ~ '1'
                                                ,doi == '10.1371/journal.pone.0062578' ~ '1'
                                                ,.default = `diff_type.pool_no_adj.`)
          ,diff_algorith_other = case_when(doi == '10.1038/s41598-018-32413-2' ~ 'parametric on relative abundance'
                                          ,.default = diff_algorith_other)
    
           ) %>%
  filter(., doi=='10.1016/j.csbj.2020.08.028') %>%
  select(c(doi, starts_with('diff_type'), starts_with('diff_al'))) %>%
  pivot_longer(cols=starts_with('diff')) %>%
  mutate(., value = ifelse(is.na(value), 'N/A', value)) %>%
  filter(., !(value %in% c('0', 'N/A'))) %>%
  pivot_wider(id_cols = name, names_from = doi, values_from = value) %>%
  reactable(.,  defaultPageSize = 15)
```

This paper is primarily focused on establishing community state types (CSTs) 
across the infant microbiome. I wouldn't classify CSTs as differential 
abundance. From the methods section,

> Furthermore, differential abundance of bacterial genera and alpha-diversity 
was tested by ANOVA analysis. Moreover, we also calculated the post hoc analysis
LSD (least significant difference) for multiple comparison.

We continue to see sumary heat maps in Figure 1 by age, Figure 2 by birth mode
and age, and figure 3 by feeding status. But, we don't see an explicit analysis
by study as far as I can tell. So, I'd say `diff_type.pool_no_adj.`.


### 10.1128/mbio.01018-16

```{r, echo=FALSE}
data %>%
  mutate(., `analyses_perf.differenital_abundance.` = case_when(doi == '10.1111/bjd.20626' ~ '0'
                                                               ,doi == '10.1016/j.gpb.2018.03.009' ~ '0'
                                                               ,.default=`analyses_perf.differenital_abundance.`)) %>%
  mutate_at(., .vars=vars(starts_with('diff_algorith.'), starts_with('diff_type'))
             , .funs=~ifelse(`analyses_perf.differenital_abundance.` %in% c(0, '0'), '0', .)
             ) %>%
  mutate(., `diff_algorith.LefSe.` = case_when(doi == '10.1111/1462-2920.13632' ~ '1'
                                              ,doi == '10.1007/s00248-018-1176-2' ~ '1'
                                              ,.default=`diff_algorith.LefSe.`)
          , `diff_algorith.rare_kw.` = case_when(doi == '10.1111/1462-2920.13632' ~ '0'
                                                ,doi == '10.1007/s00248-018-1176-2' ~ '1'
                                                ,.default = `diff_algorith.rare_kw.`) 
          , `diff_algorith.other.` = case_when(doi == '10.1111/1462-2920.13632' ~ '0'
                                              ,doi == '10.1128/msystems.00138-20' ~ '0'
                                              ,doi == '10.1007/s00248-018-1176-2' ~ '0'
                                              ,.default = `diff_algorith.other.`)
          , `diff_type.parallel.` = case_when(doi == '10.1007/s00248-018-1176-2' ~ '1'
                                             ,doi == '10.1038/s41598-018-32413-2' ~ '1'
                                             ,.default = '0')
          , `diff_type.not_described.` = case_when(doi == '10.1111/1462-2920.13632' ~ '0'
                                                  ,doi == '10.1007/s00248-018-1176-2' ~ '0'
                                                  ,doi == '10.1371/journal.pone.0062578' ~ '0'
                                                  ,doi == '10.1038/s41598-018-32413-2' ~ '0'
                                                  ,doi == '10.1186/s40168-017-0248-8' ~ '0'
                                                  ,doi == '10.3389/fcimb.2022.816526' ~ '0'
                                                  ,doi == '10.1016/j.csbj.2020.08.028' ~ '0'
                                                  ,.default = `diff_type.not_described.`)
          , `diff_type.pool_no_adj.` = case_when(doi == '10.1111/1462-2920.13632' ~ '1'
                                                ,doi == '10.1016/j.csbj.2020.08.028' ~ '1'
                                                ,doi == '10.1371/journal.pone.0062578' ~ '1'
                                                ,.default = `diff_type.pool_no_adj.`)
          ,diff_algorith_other = case_when(doi == '10.1038/s41598-018-32413-2' ~ 'parametric on relative abundance'
                                          ,.default = diff_algorith_other)
    
           ) %>%
  filter(., doi=='10.1128/mbio.01018-16') %>%
  select(c(doi, starts_with('diff_type'), starts_with('diff_al'))) %>%
  pivot_longer(cols=starts_with('diff')) %>%
  mutate(., value = ifelse(is.na(value), 'N/A', value)) %>%
  filter(., !(value %in% c('0', 'N/A'))) %>%
  pivot_wider(id_cols = name, names_from = doi, values_from = value) %>%
  reactable(.,  defaultPageSize = 15)
```

And, we're back to the Sze and Schloss paper. We see the Bacteriodetes:Firmicutes
ratio, whcih might be a targeted analysis, so I'd like to check that?

```{r, echo = FALSE}
data %>%
  filter(., doi == '10.1128/mbio.01018-16') %>%
  select(doi, starts_with('diff_tar')) %>%
  reactable()
```

And then looking at Figure 2, we see parallel analyses for this, looking at the 
relative risk association with the ratio. So, I'd code this as parallel, but 
described.

## Conflicts no adjustment vs adjustment

I want to make sure that there's not a case wher we see an analysis which 
doesn't adjust for study effect and then does some sort of adjustment for study
effect. So, if we have a crude model but a fixed model, we _should_ have coded 
as a fixed effect adjustment.

```{r, echo=FALSE}
  data %>%
  mutate(., `analyses_perf.differenital_abundance.` = case_when(doi == '10.1111/bjd.20626' ~ '0'
                                                               ,doi == '10.1016/j.gpb.2018.03.009' ~ '0'
                                                               ,.default=`analyses_perf.differenital_abundance.`)
          ) %>%
  mutate_at(., .vars=vars(starts_with('diff_algorith.'), starts_with('diff_type'))
             , .funs=~ifelse(`analyses_perf.differenital_abundance.` %in% c(0, '0'), '0', .)
             ) %>%
  mutate(., `diff_algorith.LefSe.` = case_when(doi == '10.1111/1462-2920.13632' ~ '1'
                                              ,doi == '10.1007/s00248-018-1176-2' ~ '1'
                                              ,.default=`diff_algorith.LefSe.`)
          , `diff_algorith.rare_kw.` = case_when(doi == '10.1111/1462-2920.13632' ~ '0'
                                                ,doi == '10.1007/s00248-018-1176-2' ~ '1'
                                                ,.default = `diff_algorith.rare_kw.`) 
          , `diff_algorith.other.` = case_when(doi == '10.1111/1462-2920.13632' ~ '0'
                                              ,doi == '10.1128/msystems.00138-20' ~ '0'
                                              ,doi == '10.1007/s00248-018-1176-2' ~ '0'
                                              ,.default = `diff_algorith.other.`)
          , `diff_type.parallel.` = case_when(doi == '10.1007/s00248-018-1176-2' ~ '1'
                                             ,doi == '10.1038/s41598-018-32413-2' ~ '1'
                                             ,doi == '10.1128/mbio.01018-16' ~ '1'
                                             ,.default = '0')
          , `diff_type.not_described.` = case_when(doi == '10.1111/1462-2920.13632' ~ '0'
                                                  ,doi == '10.1007/s00248-018-1176-2' ~ '0'
                                                  ,doi == '10.1371/journal.pone.0062578' ~ '0'
                                                  ,doi == '10.1038/s41598-018-32413-2' ~ '0'
                                                  ,doi == '10.1186/s40168-017-0248-8' ~ '0'
                                                  ,doi == '10.3389/fcimb.2022.816526' ~ '0'
                                                  ,doi == '10.1016/j.csbj.2020.08.028' ~ '0'
                                                  ,doi == '10.1128/mbio.01018-16' ~ '0'
                                                  ,.default = `diff_type.not_described.`)
          , `diff_type.pool_no_adj.` = case_when(doi == '10.1111/1462-2920.13632' ~ '1'
                                                ,doi == '10.1016/j.csbj.2020.08.028' ~ '1'
                                                ,doi == '10.1371/journal.pone.0062578' ~ '1'
                                                ,.default = `diff_type.pool_no_adj.`)
          ,diff_algorith_other = case_when(doi == '10.1038/s41598-018-32413-2' ~ 'parametric on relative abundance'
                                          ,.default = diff_algorith_other)
    
           ) %>%
  select(doi, starts_with('diff_typ'), starts_with('diff_al')) %>%
  mutate_at(., .vars=vars(starts_with('diff_type.'))
             , .funs=as.numeric) %>%
  mutate(., da_unadj = `diff_type.pool_no_adj.`
          , da_adj = ((`diff_type.fixed_effect.` + `diff_type.random_effect.` + 
                       `diff_type.meta_analysis.`) > 0) * 1
          , da_other = diff_type.other.
         ) %>%
  filter(., (da_unadj == 1) & (da_adj == 1)) %>%
  select(c(doi, starts_with('diff_type'))) %>%
  rename_with(., .cols=starts_with('diff_type.')
               , .fn=~str_remove(str_remove(., 'diff_type.'), '\\.')) %>%
  reactable()
```

### 10.1007/s12275-022-1526-0 

This paper did a primary analysis pooling three Korean data sets, which are 
presented as esesntially a single data set, and then there are additional data
sets brought in to predict health/disease.

In the first part of the paper, they focused on their Korean cohort, including
Maaslin to do multivariate analysis on those Korean data sets. From 
the methods section, 

> To determine significant associations between genus-level taxa and covariates,
we used the MaAsLin R package (Morgan et al., 2012). This method involves 
multiple steps of analysis: the arcsine transformation of microbial abundance,
removing the outliers of the relative abundance of taxa, identifying confounding
variables by constructing boosted additive generalized linear models, and 
calculating associations using classical generalized linear models. We 
restricted this method to genera with an over 5% occurrence and used an outlier 
cut-off of 0.005 for covariate pre-processing. P-value were corrected for 
multiple testing by calculating the false discovery rate (FDR). An FDR < 0.1 
represented a significant relationship between individual microbes and 
covariates.

This is shown in the results, from the section entitled, "Investigating 
microbiome covariates in Korean population", we have Figure 1, which describes
the tests on the Korean data.


> **Fig. 1. Significant associations between microbiome and covariates in**
**Korean populations.**  ... (B) Significant association between covariates and
individual microbial taxa. The associations were calculated with MaAsLin at the 
genus level and were visualized at the phylum level to which each genus 
belongs. A cut-off value of FDR less than 0.1 was considered to be significant.
Based on 0, the bar graph on the right represents the number of markers with a 
significant positive association. The bar graph on the left represents the 
number of markers with a significant negative association. The color of the 
bar graph indicates the phylum to which the markers belong. A detailed list 
of markers can be found in Supplementary data Table S1.

There's a second set of testing, from "Clustering beneficial and pathogenic 
microbial taxa" in the results section, which involves multiple datasets.
From the text: 

> To identify common microbial markers for healthy and nonhealthy individuals 
in multiple datasets, we conducted the analysis as follows. First, we performed
linear discriminant effect size (LEfSe) analysis on genus-level relative 
abundances for each dataset independently (Segata et al., 2011). Since this 
method considers differences between class means when calculating effect size, 
it has the advantage of preferring abundant markers. Kruskal-Wallis test 
(alpha value of 0.05) and LDA scores greater than 1.0 were regarded as 
significance thresholds in the analysis. Bacterial genera detected in over a 
quarter of the datasets were selected as potential candidates for common 
microbial markers. The distance matrix between  the microbes was then derived 
from the linear discriminant analysis (LDA) scores using cosine similarity, 
and the distance matrices were clustered using Ward’s hierarchical clustering 
method. The uncertainty in hierarchical cluster analysis was assessed by the 
multiscale bootstrap resampling method with 10,000 iterations, and approximately
unbiased (AU) P-values were used to determine the clustering significance. 
Clustering analysis was computed using the “pvclust” package in R (Suzuki 
and Shimodaira, 2006). The clusters were considered as pathogenic and beneficial
taxa in cases showing positive association with non-healthy groups and healthy
controls, respectively.

So, we see a parallel analysis for differential abundance using LefSe, and then
they pooled those based on a cosine score and classification. So, I think
we can say that this is a pooling technique, but not a meta-analysis and not a 
fixed effect or random effect.

## Has DA but no methods

I'd also like to check if we have cases where we have differential abundance
flagged but no method is identified. So, let's look there?

```{r, echo=FALSE}
  data %>%
  mutate(., `analyses_perf.differenital_abundance.` = case_when(doi == '10.1111/bjd.20626' ~ '0'
                                                               ,doi == '10.1016/j.gpb.2018.03.009' ~ '0'
                                                               ,.default=`analyses_perf.differenital_abundance.`)
          ) %>%
  mutate_at(., .vars=vars(starts_with('diff_algorith.'), starts_with('diff_type'))
             , .funs=~ifelse(`analyses_perf.differenital_abundance.` %in% c(0, '0'), '0', .)
             ) %>%
  mutate(., `diff_algorith.LefSe.` = case_when(doi == '10.1111/1462-2920.13632' ~ '1'
                                              ,doi == '10.1007/s00248-018-1176-2' ~ '1'
                                              ,.default=`diff_algorith.LefSe.`)
          , `diff_algorith.rare_kw.` = case_when(doi == '10.1111/1462-2920.13632' ~ '0'
                                                ,doi == '10.1007/s00248-018-1176-2' ~ '1'
                                                ,.default = `diff_algorith.rare_kw.`) 
          , `diff_algorith.other.` = case_when(doi == '10.1111/1462-2920.13632' ~ '0'
                                              ,doi == '10.1128/msystems.00138-20' ~ '0'
                                              ,doi == '10.1007/s00248-018-1176-2' ~ '0'
                                              ,.default = `diff_algorith.other.`)
          , `diff_type.parallel.` = case_when(doi == '10.1007/s00248-018-1176-2' ~ '1'
                                             ,doi == '10.1038/s41598-018-32413-2' ~ '1'
                                             ,doi == '10.1128/mbio.01018-16' ~ '1'
                                             ,.default = '0')
          , `diff_type.not_described.` = case_when(doi == '10.1111/1462-2920.13632' ~ '0'
                                                  ,doi == '10.1007/s00248-018-1176-2' ~ '0'
                                                  ,doi == '10.1371/journal.pone.0062578' ~ '0'
                                                  ,doi == '10.1038/s41598-018-32413-2' ~ '0'
                                                  ,doi == '10.1186/s40168-017-0248-8' ~ '0'
                                                  ,doi == '10.3389/fcimb.2022.816526' ~ '0'
                                                  ,doi == '10.1016/j.csbj.2020.08.028' ~ '0'
                                                  ,doi == '10.1128/mbio.01018-16' ~ '0'
                                                  ,.default = `diff_type.not_described.`)
          , `diff_type.pool_no_adj.` = case_when(doi == '10.1111/1462-2920.13632' ~ '1'
                                                ,doi == '10.1016/j.csbj.2020.08.028' ~ '1'
                                                ,doi == '10.1371/journal.pone.0062578' ~ '1'
                                                ,doi == '10.1007/s12275-022-1526-0' ~ '0'
                                                ,.default = `diff_type.pool_no_adj.`)
          , `diff_type.random_effect.` = case_when(doi == '10.1007/s12275-022-1526-0' ~ '0'
                                                 ,.default = `diff_type.random_effect.`)
          , `diff_type.meta_analysis.` = case_when(doi == '10.1007/s12275-022-1526-0' ~ '0'
                                                 ,.default = `diff_type.meta_analysis.`)
          , `diff_type.combine_no_meta.` = case_when(doi == '10.1007/s12275-022-1526-0' ~ '1'
                                                    ,.default = '0')
          , diff_algorith_other = case_when(doi == '10.1038/s41598-018-32413-2' ~ 'parametric on relative abundance'
                                          ,.default = diff_algorith_other)
    
           ) %>%
  mutate_at(., .vars=vars(starts_with('diff_type.'), `analyses_perf.differenital_abundance.`)
             , .funs=as.numeric) %>%
  mutate(., has_type = rowSums(select(., starts_with('diff_type.')))
          ) %>%
  filter(., has_type == 0) %>%
  filter(., `analyses_perf.differenital_abundance.` == 1) %>%
  select(c(doi, `analyses_perf.differenital_abundance.`, starts_with('diff_type.'), starts_with('diff_al'))) %>%
  mutate_at(., .vars=vars(everything()), .funs=as.character) %>%
  pivot_longer(cols=everything()) %>%
  filter(., !(value %in% c('0', NA)))
  
```

So, we have a case where we have an algorithm but we don't actually have a
method? 

### 10.1186/s40168-017-0368-1	

This is a comparison of C. diff in a reference data set, and then with a 
validation set.

From the methods (under th eheading meta-analysis):

> β-Diversity was calculated using the weighted UniFrac distance metric, and 
significant differences across patient groups were evaluated using PERMANOVA. 
For predictive models, we trained a random forest model on each individual 
\dataset as well as the combined (meta) dataset. We also built the model by 
training on one dataset and using the other for cross-validation. The 
discriminatory power of OTUs, genera, and families were calculated as the 
area under the ROC curve (AUC) in each case.

From their result,

> **Meta-analysis**
> 
> We combined our high-throughput 16S rRNA gene sequence data with a recent 
study by Khanna et al. [22]. Although both studies shared a common experimental 
approach, results revealed a strong study effect as the most clearly 
discernible signal in the data (PERMANOVA, p value = 0.002). The random forest 
trained to classify which samples come from which study had an error rate of 
about 2% with AUC of 0.98 (Additional file 3). This resilient study-level effect
was consistent, even when we included only shared OTUs between two studies. We
then generated predictive models using our dataset with truncated sequences 
(200 bp, leave-one-out cross-validation), and the results showed a performance 
reduction at all taxonomic levels compared to our original dataset with sequence
read lengths of 250 bp (Table 2). We also constructed three separate random 
forest (RF) classifiers of CDI recurrence using the Khanna et al. [22] dataset.
Members of Veillonellaceae family were ranked first for all constructed models,
albeit with no statistically significant discriminatory powers (Table 2). 
Finally, when we trained on our data and used the Khanna et al. [22] for 
cross-validation, the error rate was 0.29, and vice versa, the error rate was 
0.32; none of these RF models were significant.

So, I dont think they performed differential abundance on their combined data.


## Tidy data

```{r, results = 'hide'}
cleaned[['da_study']] <- 
  data %>%
  mutate(., `analyses_perf.differenital_abundance.` = case_when(doi == '10.1111/bjd.20626' ~ '0'
                                                               ,doi == '10.1016/j.gpb.2018.03.009' ~ '0'
                                                               ,doi == '10.1186/s40168-017-0368-1' ~ '0'
                                                               ,.default=`analyses_perf.differenital_abundance.`)
          ) %>%
  mutate_at(., .vars=vars(starts_with('diff_algorith.'), starts_with('diff_type'))
             , .funs=~ifelse(`analyses_perf.differenital_abundance.` %in% c(0, '0'), '0', .)
             ) %>%
  mutate(., `diff_algorith.LefSe.` = case_when(doi == '10.1111/1462-2920.13632' ~ '1'
                                              ,doi == '10.1007/s00248-018-1176-2' ~ '1'
                                              ,.default=`diff_algorith.LefSe.`)
          , `diff_algorith.rare_kw.` = case_when(doi == '10.1111/1462-2920.13632' ~ '0'
                                                ,doi == '10.1007/s00248-018-1176-2' ~ '1'
                                                ,.default = `diff_algorith.rare_kw.`) 
          , `diff_algorith.other.` = case_when(doi == '10.1111/1462-2920.13632' ~ '0'
                                              ,doi == '10.1128/msystems.00138-20' ~ '0'
                                              ,doi == '10.1007/s00248-018-1176-2' ~ '0'
                                              ,.default = `diff_algorith.other.`)
          , `diff_type.parallel.` = case_when(doi == '10.1007/s00248-018-1176-2' ~ '1'
                                             ,doi == '10.1038/s41598-018-32413-2' ~ '1'
                                             ,doi == '10.1128/mbio.01018-16' ~ '1'
                                             ,.default = '0')
          , `diff_type.not_described.` = case_when(doi == '10.1111/1462-2920.13632' ~ '0'
                                                  ,doi == '10.1007/s00248-018-1176-2' ~ '0'
                                                  ,doi == '10.1371/journal.pone.0062578' ~ '0'
                                                  ,doi == '10.1038/s41598-018-32413-2' ~ '0'
                                                  ,doi == '10.1186/s40168-017-0248-8' ~ '0'
                                                  ,doi == '10.3389/fcimb.2022.816526' ~ '0'
                                                  ,doi == '10.1016/j.csbj.2020.08.028' ~ '0'
                                                  ,doi == '10.1128/mbio.01018-16' ~ '0'
                                                  ,.default = `diff_type.not_described.`)
          , `diff_type.pool_no_adj.` = case_when(doi == '10.1111/1462-2920.13632' ~ '1'
                                                ,doi == '10.1016/j.csbj.2020.08.028' ~ '1'
                                                ,doi == '10.1371/journal.pone.0062578' ~ '1'
                                                ,doi == '10.1007/s12275-022-1526-0' ~ '0'
                                                ,.default = `diff_type.pool_no_adj.`)
          , `diff_type.random_effect.` = case_when(doi == '10.1007/s12275-022-1526-0' ~ '0'
                                                 ,.default = `diff_type.random_effect.`)
          , `diff_type.meta_analysis.` = case_when(doi == '10.1007/s12275-022-1526-0' ~ '0'
                                                 ,.default = `diff_type.meta_analysis.`)
          , `diff_type.combine_no_meta.` = case_when(doi == '10.1007/s12275-022-1526-0' ~ '1'
                                                    ,.default = '0')
          , diff_algorith_other = case_when(doi == '10.1038/s41598-018-32413-2' ~ 'parametric on relative abundance'
                                          ,.default = diff_algorith_other)
    
           ) %>%
  mutate_at(., .vars=vars(starts_with('diff_type.'), starts_with('diff_algorith.'))
             , .funs=as.numeric) %>%
  rename_with(., .cols=starts_with('diff'), .fn=~str_replace(., 'diff', 'da')) %>%
  mutate(., da_study_code = ((`da_type.pool_no_adj.` * 2) +
                             (`da_type.fixed_effect.` * 4) + 
                             (`da_type.random_effect.` * 8) +
                             (`da_type.meta_analysis.` * 16) + 
                             (`da_type.combine_no_meta.` * 32) + 
                             (`da_type.parallel.` * 64) + 
                             (`da_type.not_described.` * 128) + 
                             (`da_type.other.` * 256))
         , da_study_label = case_when(`analyses_perf.differenital_abundance.` == 0 ~ 'no da'
                                      ,da_study_code == 2 ~ 'no dataset effect acknoweldged'
                                      ,da_study_code == 4 ~ 'fixed effect adjustment for dataset'
                                      ,da_study_code == 8 ~ 'random effects adjustment for dataset'
                                      ,da_study_code == 16 ~ 'meta analysis of datasets'
                                      ,da_study_code == 20 ~ 'meta analysis of datasets'
                                      ,da_study_code == 32 ~ 'other estimate pooling method'
                                      ,da_study_code == 64 ~ 'parallel analyses'
                                      ,da_study_code == 128 ~ 'not described'
                                      ,da_study_code == 256 ~ 'dataset adjustment before analysis'
                                      ,.default = as.character(da_study_code))
              ) %>%
    select(doi, starts_with('da_study'), starts_with('da_type'), starts_with('da_algorith'))

```

```{r}
cleaned[['analyses_performed']] <-
  cleaned[['analyses_performed']] %>%
  mutate(., `analyses_perf.differenital_abundance.` = case_when(doi == '10.1111/bjd.20626' ~ 0
                                                               ,doi == '10.1016/j.gpb.2018.03.009' ~ 0
                                                               ,doi == '10.1186/s40168-017-0368-1' ~ 0
                                                               ,.default=`analyses_perf.differenital_abundance.`)
          )
```
# Sample classifier - taxonomic levels

Our last major technique was looking at the sample classifier. We asked three
major questions about the sample classifier:

 * What taxonomic level(s) were used to build a sample classifier? [`class_level`]
 
 with select all options of 
 
 | Column | Response Text |
|:---|:---|
| `class_level[otu_asv]` | OTU/ASV |
| `class_level[species]` | Species |
| `class_level[genus]` | Genus |
| `class_level[family]` | Family |
| `class_level[order]` | Family |
| `class_level[class]` | Class |
| `class_level[phylum]` | Phylum |
| `class_level[not_described]` | Not described |

## Missing Data


We'll start by checking if any of the taxonomic levels are missing... 

```{r, echo=FALSE}
data %>% 
  select(c(doi, `analyses_perf.sample_classification.`, starts_with('class_level.'))) %>%
  mutate_at(., .vars=vars(starts_with('class_level.'))
             , .funs=~ifelse(`analyses_perf.sample_classification.` == '0', '0', .)
             ) %>%
  mutate_at(., .vars=vars(starts_with('class_level.'))
             , .funs=~!(. %in% c('0', '1'))
  ) %>%
  summarise(across(starts_with('class_level.'), sum)) %>%
  rename_with(., .fn=~str_remove(str_remove(., 'class_level.'), '\\.')) %>%
  reactable()
```

We're not missing any taxonomic data here. 

## Tidy data

```{r, results='hide'}
cleaned[['classifier_taxa']] <- 
  data %>%
  select(c(doi, `analyses_perf.sample_classification.`, starts_with('class_level.'))) %>%
  mutate_at(., .vars=vars(starts_with('class_level'))
             , .funs=~ifelse(`analyses_perf.sample_classification.` %in% c('0', 0), 0, as.numeric(.))
             ) %>%
  mutate(., `analyses_perf.sample_classification.` = as.numeric(`analyses_perf.sample_classification.`)
           ) %>%
  rowwise() %>%
  mutate(., class_level_min = max(`class_level.otu_asv.` * 8
                                 ,`class_level.species.` * 7
                                 ,`class_level.genus.` * 6 
                                 ,`class_level.family.` * 5
                                 ,`class_level.order.` * 4
                                 ,`class_level.class.` * 3
                                 ,`class_level.phylum.` * 2          
                                 ,`class_level.not_described.` * 9)
          ) %>%
  ungroup() %>%
  mutate(., class_level_min = case_when(`analyses_perf.sample_classification.` == 0 ~ 'No classifier'
                                      ,class_level_min == 8 ~ 'feature'
                                      ,class_level_min == 7 ~ 'species'
                                      ,class_level_min == 6 ~ 'genus'
                                      ,class_level_min == 5 ~ 'family'
                                      ,class_level_min == 4 ~ 'order'
                                      ,class_level_min == 3 ~ 'class'
                                      ,class_level_min == 2 ~ 'phylum'
                                      ,class_level_min == 9 ~ 'not described')
         , class_level2 = case_when(class_level_min == 'feature' ~ 1
                                  ,class_level_min %in% c('not described', 'No DA') ~ NA
                                  ,.default = 0)
          ) %>%
  select(c(doi, starts_with('class')))
```

# Sample Classifier: Training

## Classifier Training

We asked:
* How was the classifier trained? [`class_train`]

This was a multiple choice question, with options

1. It was trained on a single study
2. It was trained on multiple studies with no consideration of study effects
3. Leave a study out or cross validation training
4. Other (free text)
5. Not described

```{r, echo=FALSE}
data %>% 
  select(c(doi, `analyses_perf.sample_classification.`, starts_with('class_t'))) %>%
  mutate(., class_train = ifelse(`analyses_perf.sample_classification.` %in% c('0', 0), 'No classifier', class_train)) %>%
  group_by(class_train) %>% count() %>%
  reactable()
```


I want to check the "other" category which apparently didn't make it into the
final data set.

```{r, echo=FALSE}
data %>% 
  select(c(doi, `analyses_perf.sample_classification.`, starts_with('class_t'))) %>%
  mutate(., class_train = ifelse(`analyses_perf.sample_classification.` %in% c('0', 0), 'No classifier', class_train)) %>%
  filter(., tolower(class_train) %in% c('other')) %>%
  select(doi) %>%
  reactable()
```


### 10.1016/j.chom.2013.08.006

This is one of the _early_ papers by Cathy Lozupone et al. They built a random
forest classifier to help identify OTUs to distingish between chronic HIV
infection from HIV-negative controls. The paper is *old* (received in 2013, 
relatively few revisions) but the work was likely done earlier as both Cathy 
Lozupone (who lead the projects) and Dan Knights (who would have done the RF 
models) had moved on from Rob's lab by spring of January 2013 and was settled 
in faculty positions. So, the techniques are somewhat outdated now.

According to the paper's supplemental methods,

> Random Forests analysis was performed for each comparison on 100 rarefied 
versions of the data, and the average cross-validation error estimates and OTU
importance estimates were calculated using custom code. Random Forests assigns
an importance score to each OTU by estimating the increase in error caused by
removing that OTU from the set of predictors. We considered an OTU to be highly
predictive if its average importance score was at least 0.001, as previously 
done in (Yatsunenko et al., 2012). Additionally, we determined the direction of
change with HIV status as the difference in the mean relative abundance in 
samples from healthy individuals and individuals chronically infected with HIV.
OTUs for which the direction of change was not supported by an ANOVA p-value of
less than 0.1 were excluded from further consideration.

We find random forest twice in the results. The first time it appears is in a
section focused on the HIV cohort section:

> To further determine the degree to which gut microbiota with chronic untreated
infection differed from HIV-negative controls, we used a supervised learning 
technique called Random Forests (Knights et al., 2011a). The purpose of the 
Random Forests classifier is to learn a function that maps a set of predictive
features to a discrete state, in this case healthy versus chronically infected
with HIV and without ART. We used as features the relative abundance in each 
sample of 97% identity (ID) Operational Taxonomic Units (OTUs; clusters in
which sequences have ≥97% identity over their aligned 16S rRNA genes, which 
approximates assignment to the same species; Stackebrandt and Goebal, 1994). 
The measure of the method’s success is its ability to classify new samples as 
coming from an HIV-positive or -negative individual. We only used one sample 
per individual when multiple time points were collected. The model could 
classify unknown samples according to HIV status with a 4.33% ± 0.821% error
rate, which is 10.6 times higher than the baseline error rate for random 
guessing of 45.83%, indicating that HIV-associated microbiota exhibit highly
characteristic differences.

This was based on the HIV cohort *only*. 

There's a later section in the paper which compares people with HIV and 
uninfected individuals to western/non western participants (Global Gut, 
Yatsunenko et al, 2012) and a long term diet study (Wu et al, 2011). The 
authors were specifically interested in the *Bacteriodetes*:*Prevotella* ratio, 
although they also performed a random forest model to distinguish taxa. 
We see a mention of random forest here:

> Additionally, the discriminative 97% ID OTUs that differentiated adults from 
Western and agrarian cultures using Random Forests analysis (Yatsunenko et al., 
2012) were highly related and sometimes identical to those that discriminated
between HIV-negative and -positive individuals (Figure S2). This was 
particularly evident for the Bacteroidales order, where Western and HIV-
negative associated OTUs clustered together in the Bacteroides and Alistipes 
genera and agrarian and HIV-positive associated OTUs clustered together in the
Prevotella genus (Figure S2A). However, consistent with the notion that 
Bacteroides and Prevotella are indicator taxa for more complex community 
assemblages, parallels between agrarian cultures and HIV-positive individuals
were also observed in other taxonomic groups (see Figure S2 for more details). 

However, random forests were also used in Yatsunenko et al, and I think this is
referencing *those* random forests.

So, I don't think the study actually did sample classification, and we should
this as a study without a classifier.

### 10.1016/j.febslet.2014.09.039

This is also a relatively early paper (2014) which looked at the associaton
between the gut microbiome and two different conditions (lean vs obese and 
healthy vs IBD). The authors observed the study effects were larger in than
the biological effects of lean vs obese individuals (Figure 6), and so they
conducted analyeses by study, looking for overlap. 

This applied to the random forest classifier they used for the obesity
comparison, where a classier was applied to each data set, as shown in 
Table 4 (reproduced below.)

> **Table 4. Receiver operator characteristic curve values for 97% closed-reference OTUs by study. Values listed are average values of 5× repeated ROC analyses, using random forest method, with 10-fold cross validation**
> 
> | Study  | ROC AUC values  | Standard deviation |
> | --- | --- | --- |
> | Turnbaugh  | 0.7250379  | 0.1488683 |
> | Amish  | 0.6077041  | 0.1103585 |
> | HMP  | 0.6656818  | 0.1295004 |
> | Wu  | 0.8623333  | 0.1638126 |
> | Yatsunenko  | 0.6259477  | 0.1033857 |

So, we're seeing classification study-by-study. 

There was not a visable study effect in PCoA space for the IBD results, so they
pooled the IBD results.

There's a later description of sample classification for the IBD data in the 
paper. From the results:

> The question of whether the combined IBD microbiota data can provide an 
accurate training dataset for predicting disease states was tested using 
supervised learning (with the same ROC based approach described above for BMI 
samples). All levels of taxonomy (phylum to species) were tested, with 10-fold
cross validation for error rate prediction. The species level table performed 
best for almost all comparisons between healthy controls and individual disease
states and all disease states combined, which are shown in Table 7 , with one 
exception-the family level had a slightly higher AUC ROC value (only 0.00326 
higher) for healthy controls versus ICD samples, but this is much smaller than 
the accuracy that would be lost for each of the other categories when using the
family level versus the species level. When all four classes of samples are 
combined, the supervised learning accuracy drops (68.6%). The confusion matrix
for the combined four class data is shown in Table S2.

Reproducing Table 7 below:

> **Table 7. Predicting IBD state via supervised learning. Values listed are average values of 5× repeated ROC analyses, using random forest method, with 10-fold cross validation. Healthy controls versus each individual category of disease are shown as well as healthy controls versus all IBD categories combined. A ROC AUC value of 0.5 is no better than random guess, whereas 1.0 indicates perfect specificity and sensitivity**
> 
> | Categories compared | ROC AUC value | ROC AUC stdev |
> | --- | --- | --- |
> | Healthy controls versus ulcerative colitis | 0.92258803 | 0.04239816 |
> | Healthy controls versus colonic Crohn's disease | 0.87879176 | 0.07375804 |
> | Healthy controls versus ileal Crohn's disease | 0.96996245 | 0.0378 |
> | Healthy controls versus all IBD categories | 0.92404109 | 0.04297096 |

So, looking at the text, we don't actually see the data trained by study. I
suspect this is because there isn't a strong study effect in PCoA space, so 
essentially, they disregarded the study effect.

So, our "other" here is two models: one where they considered study effect and
trained separately on each study and one where they didn't.

## Classifier validation

And then we also have the question about whether or not the classifier had a 
validation set. I specifically want to know if there are cases where we 
have a validation cohort 

```{r, echo=FALSE}
data %>% 
  mutate(., `analyses_perf.sample_classification.` = case_when(doi == '10.1016/j.chom.2013.08.006' ~ '0'
                                                              ,.default = `analyses_perf.sample_classification.`)
          , class_train = case_when(doi == '10.1016/j.chom.2013.08.006' ~ 'No classifier'
                                   ,doi == '10.1016/j.febslet.2014.09.039' ~ 'Walters: pooled and by study'
                                   ,.default = class_train)
          ) %>%
  mutate_at(., .vars=vars(class_train, class_validate)
             , .funs=~ case_when(`analyses_perf.sample_classification.` %in% c('0', 0) ~ 'No classifier'
                               ,.default=.)
             ) %>%
  select(c(doi, validation, `analyses_perf.sample_classification.`, class_train, class_validate)) %>%
  mutate(., validation = tolower(validation)) %>%
  group_by(validation, class_validate) %>% 
  count() %>%
  ungroup() %>%
  pivot_wider(id_cols = class_validate, names_from = validation, values_from = n, values_fill = 0) %>%
  reactable()
```

I'd like to check the cases where it's noted the classifier was valdiated on a 
secondary data set by there's no validation listed. 


```{r, echo=FALSE}
data %>%
  mutate(., `analyses_perf.sample_classification.` = case_when(doi == '10.1016/j.chom.2013.08.006' ~ '0'
                                                              ,.default = `analyses_perf.sample_classification.`)
         , class_train = case_when(doi == '10.1016/j.chom.2013.08.006' ~ 'No classifier'
                                   ,doi == '10.1016/j.febslet.2014.09.039' ~ 'Walters: pooled and by study'
                                   ,.default = class_train)
          ) %>%
  select(c(doi, validation, `analyses_perf.sample_classification.`, class_validate)) %>%
  mutate(., class_validate = ifelse(`analyses_perf.sample_classification.` %in% c('0', 0), 'No classifier', class_validate)
          , validation = tolower(validation)
          ) %>%
  filter(., (validation == 'no') & (class_validate == 'Yes')) %>%
  select(doi) %>%
  reactable()
```
### 10.1186/s13059-022-02637-7

This is a paper where they essentially tried to build a classifier to distingish
between healthy and unhealthy individuals with each disease. From the methods:

> For the classification, each disease cohort was randomly subsampled to a 
maximum of 23 cases and 23 control samples. For each pair of disease cohorts 
(train, predict), ASVs were filtered, keeping only ASVs present in both 
cohorts. A random forest classifier (implemented in scikit-learn version 
0.23.1, using default parameters, 100 trees per forest) was trained on the 
case/control samples in the training cohort. The trained classifier was then 
used to predict the case/control status of the prediction cohort, and false 
and true positive rates and AUC were calculated using scikit-learn. In the 
cases where the train and predict disease cohorts were the same 
(i.e., assessing the classifier predictions on the same disease cohort), the 
disease cohort samples were randomly split to 2/3 of the samples to be used as 
the training cohort, and the remaining 1/3 of the samples used as the
prediction cohort.

So, I see cross validation approach, but there's not an independent validation
cohort here. 

The methods continue:

> Three random null-models were used: in the random-prediction model, labels 
of the prediction cohort (i.e., case/control) were randomly permuted prior 
to the classifier prediction. In the random-training model, labels of the 
training cohort (again case/control) were randomly permuted prior to the 
training (thus leaving intact the case/control differences in the prediction 
cohort). In the third model, labels of the prediction cohort were randomly 
permuted prior to the classifier prediction, and the labels of the training 
cohort were randomly permuted prior to the training.
> 
> AUC values shown represent the mean AUC results of 50 repeats of the entire 
process described (for both real data and randomizations).

I also want to check how we describe the article for sample classification.

```{r, echo=FALSE}
data %>%
  select(c(doi, class_train, class_validate)) %>%
  filter(., doi == '10.1186/s13059-022-02637-7') %>%
  reactable()
```

So, the leave-one-out is correctly coded, but there is no independent validation
cohort and `validation` is correct, but `class_validate` is not. 

### 10.1128/mbio.00630-18

This paper is looking at using the microbiome to build externaly valid tumor
biomarkers in colorectal cancer. They considered two sets of data: fecal samples
and tumor samples. They used differential abundance testing to identify taxa
which were signifcially associated (OR) with colorectal cancer in feces or 
tissue, and then consturcted a per-taxa random forest model to predit the 
colorectal cancer and then built a combined model based on both the data
from the combined ORs and full microbiome.

From the methods on Random Forest mdoeling:

> To classify individuals as having normal colons or tumors, we built Random 
Forest classification models for each data set and comparison using taxa with 
significant ORs (after multiple-comparison correction), all taxa, or OTUs. 
Because no taxa were identified as having a significant OR associated with 
adenomas using stool or tissue samples, classification models based on OR data
were not constructed to classify individuals as having normal colons or 
adenomas. For all models, the value of trees included (i.e., ntree) was set to 
500 and the number of variables that were randomly tested (i.e., mtry) was set 
to the square root of the number of taxa or OTUs within the model. Using the 
square root of the total number of features as the number of features to test 
has been found to reliably approximate the optimum value after model 
tuning (53). All fecal models were built using a 10-fold cross-validation (CV), 
while tissue models were built using a 5-fold CV due to study sample size. One 
exception to this was the models constructed using data from the Weir study, 
which were built using a 2-fold CV due to the small number of samples. For 
models constructed based on the taxa that had a significant OR or using all 
of the taxa, we trained the models using a single study and then tested on the 
remaining studies with AUCs recorded during both training and testing phases. 
For the models constructed using OTU data, 100 10-fold CVs were run to generate 
a range of AUCs that could be reasonably expected to occur. The average AUC 
from these 100 repeats was reported. The mean decrease in accuracy (MDA), 
a measure of the importance of each taxon to the overall model, was used to 
rank the taxa used in each model.

So, based on their methods, they did both study specific and pooled models. But,
there are no independent validation tests where we see what happens to that
excluded cohort if Im reading correctly. So, no independent validation.

```{r, echo=FALSE}
data %>%
  select(c(doi, class_train, class_validate)) %>%
  filter(., doi == '10.1128/mbio.00630-18') %>%
  reactable()
```

### 10.1128/mbio.01018-16

This is another Schloss article, and was written in response to Walters, Xu, 
and Knight (doi: 10.1016/j.febslet.2014.09.039). It has a similar structure to
the last article (doi: 10.1128/mbio.00630-18).

From the methods:

> Third, we used the AUCRF (version 1.1) R package to generate random forest
models (37). For each study, we developed models using either OTUs or 
genus-level phylotypes. The quality of each model was assessed by measuring the 
AUC of the receiver operating characteristic (ROC) with 10-fold cross 
validation. Because the genus-level phylotype models were developed with a 
common reference, it was possible to use one study’s model (i.e., the training 
set) to classify the samples from the other studies (i.e., the testing sets). 
The optimum threshold for the training set was set as the probability threshold
that had the highest combined sensitivity and specificity. This threshold was 
then used to calculate the accuracy of the model applied to the test studies.
To generate ROC curves and calculate the accuracy of the models, we used the 
pROC (version 1.8) R package (38). Finally, we performed power and sample 
number simulations for different effect sizes for each study using the pwr 
(version 1.1-3) R package and base R functions. We also calculated the actual 
sample size needed on the basis of the effect size of each individual study.

So, the trained a model on individual studies and then used that to classify
the rest fo the data in other studies. I think we can say that this has semi
independent validation, but I dont think there's an independent validation
cohort where *new* data was introduced to test the classifier. 

```{r, echo=FALSE}
data %>%
  select(c(doi, class_train, class_validate)) %>%
  filter(., doi == '10.1128/mbio.01018-16') %>%
  reactable()
```

### 10.1186/s40168-017-0368-1

This study initially focused on the a reference data set that the authors 
generated. They have data sets from two locations in New York. The authors then 
built a random forest model on their data, as described in the methods:

> To test whether microbial community composition can predict recurrence after 
full treatment, we trained a random forest model on pre-treatment samples, at 
OTU, genus, and family levels. We evaluated their performance using 
leave-one-out cross-validation and scored the predictive power in a receiver 
operating characteristic (ROC) analysis. The discriminatory power of OTUs, 
genera, and families were calculated as the area under the ROC curve (AUC). To
assess the random forest model constructed, study groups were shuffled randomly
and 100 random forest classifications were computed. The out-of-bag error 
estimate was compared to the un-shuffled dataset using a one-sample Wilcoxon 
signed-rank test to assess the performance of the classification model.

This was then combined with another study, and built classifiers:

> For predictive models, we trained a random forest model on each individual 
dataset as well as the combined (meta) dataset. We also built the model by 
training on one dataset and using the other for cross-validation. The 
discriminatory power of OTUs, genera, and families were calculated as the area
under the ROC curve (AUC) in each case.

So, I think we have a cross-valiation, but I dont think we can say that there
was actual validation of an independent cohort.

```{r, echo=FALSE}
data %>%
  select(c(doi, class_train, class_validate)) %>%
  filter(., doi == '10.1186/s40168-017-0368-1') %>%
  reactable()
```

## Tidy Validation

```{r, results='hide'}
cleaned[['classifier']] <- 
  data %>%
  select(c(doi, `analyses_perf.sample_classification.`, validation, class_train, class_validate)) %>%
  mutate(., `analyses_perf.sample_classification.` = case_when(doi == '10.1016/j.chom.2013.08.006' ~ '0'
                                                              ,.default = `analyses_perf.sample_classification.`)
          , class_train = case_when(doi == '10.1016/j.chom.2013.08.006' ~ 'No classifier'
                                   ,doi == '10.1016/j.febslet.2014.09.039' ~ 'Walters: pooled and by study'
                                   ,.default = class_train)
          ) %>%
  mutate_at(., .vars=vars(c(class_train, class_validate))
             , .funs=~ifelse(`analyses_perf.sample_classification.` %in% c('0', 0), 'no classifier', .)
             ) %>%
  mutate_at(., .vars=vars(c(class_validate))
             , .funs=~ifelse(tolower(validation) == 'no', 'No', .)
             ) %>%
  select(c(doi, starts_with('class')))
```
```{r, results='hide'}
cleaned[['analyses_performed']] <- 
  cleaned[['analyses_performed']] %>%
  mutate(., `analyses_perf.sample_classification.` = case_when(doi == '10.1016/j.chom.2013.08.006' ~ 0
                                                               ,.default = `analyses_perf.sample_classification.`)
        )
```

# Combine data

```{r}
names(cleaned)
```

```{r}
pooled <- 
  cleaned[['citations']] %>%
  left_join(., cleaned[['meta_purpose']], by = join_by(doi)) %>%
  left_join(., cleaned[['meta_goal']],  by = join_by(doi)) %>%
  left_join(., cleaned[['systematic_approach']],  by = join_by(doi)) %>%
  left_join(., cleaned[['num_samples_primary']],  by = join_by(doi)) %>%
  left_join(., cleaned[['sources_primary']],  by = join_by(doi)) %>%
  left_join(., cleaned[['num_samples_valid']],  by = join_by(doi)) %>%
  left_join(., cleaned[['valid_sources']],  by = join_by(doi)) %>%
  left_join(., cleaned[['envo']],  by = join_by(doi)) %>%
  left_join(., cleaned[['design_info']],  by = join_by(doi)) %>%
  left_join(., cleaned[['sequencer']],  by = join_by(doi)) %>%
  left_join(., cleaned[['hypervariable']],  by = join_by(doi)) %>%
  left_join(., cleaned[['table']],  by = join_by(doi)) %>%
  left_join(., cleaned[['analyses_performed']],  by = join_by(doi)) %>%
  rename(all_of(c(`analyses_perf.da.` =  "analyses_perf.differenital_abundance."))) %>%
  left_join(., cleaned[['describe_taxa']],  by = join_by(doi)) %>%
  mutate_at(., .vars=vars(starts_with('taxa_'))
             , .funs=~ifelse(`analyses_perf.descriptive.` %in% c(0, '0'), 0, .)
             ) %>%
  left_join(., cleaned[['alpha_level']], by = join_by(doi)) %>%
  left_join(., cleaned[['alpha_study']], by = join_by(doi)) %>%
  mutate_at(., .vars=vars(starts_with('alpha_level.'), starts_with('alpha_study.'))
             , .funs=~ifelse(`analyses_perf.alpha.` %in% c(0, '0'), 0, .)
             ) %>%
  left_join(., cleaned[['beta_level']], by = join_by(doi)) %>%
  left_join(., cleaned[['beta_analysis']], by = join_by(doi)) %>%
  mutate_at(., .vars=vars(starts_with('beta_level.'), starts_with('beta_study.'))
             , .funs=~ifelse(`analyses_perf.beta.` %in% c(0, '0'), 0, .)
             ) %>%
  left_join(., cleaned[['da_level']], by = join_by(doi)) %>%
  left_join(., cleaned[['da_target']], by = join_by(doi)) %>%
  left_join(., cleaned[['da_filter']], by = join_by(doi)) %>%
  left_join(., cleaned[['da_study']], by = join_by(doi)) %>%
  mutate_at(., .vars=vars(starts_with('da_target.'), starts_with('da_type.'), 
                          starts_with('da_algorithm.'))
             , .funs=~ifelse(`analyses_perf.da.` %in% c(0, '0'), 0, .)
             ) %>%
  left_join(., cleaned[['classifier_taxa']], by = join_by(doi)) %>%
  left_join(., cleaned[['classifier']], by = join_by(doi))
```


And then we'll save the pooled data?

```{r}
pooled %>%
  write.csv(., '../data/cleaned_data_jwd2.csv')
  #colnames()
```




















